{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports ||||Run on Restart|||\n",
    "\n",
    "from utils import get_embeddings_from_state_dict, seed_everything, prepare_tsne_data, test_model, pickle_loader\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy import stats\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "from glycowork.glycan_data.loader import lib\n",
    "from glycowork.ml.models import SweetNet\n",
    "from glycowork.ml.processing import dataset_to_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c14b4",
   "metadata": {},
   "source": [
    "# Statistics Silo\n",
    "\n",
    "This is where you pipe in the data generated by the HBBS or ABBS to compare results and get statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba010cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters to set before running the analyses below ---\n",
    "\n",
    "BASE_RANDOM_STATE = 42  # Initial seed for reproducibility of the entire sequence of experiments\n",
    "DIR = \"C:/OneDrive/New_base_1.1\" # Path to the directory containing the models in relation to this \n",
    "STAT_DIR = \"../Statistics_and_Plots\" # Path to the directory where statistical data and plots will be saved\n",
    "os.makedirs(STAT_DIR, exist_ok=True) # Create the directory if it doesn't exist\n",
    "EXPERIMENT = \"species_family\" # Name of the experiment to test (for single experiment analysis like euclidian and t-SNE)\n",
    "NUM_RUNS = 10 # Number of runs of experiment\n",
    "RUN_NUMBER = 7 # Used for t-SNE and other analyzes that take a single run\n",
    "NORMALIZE = False # Normalize the embeddings before t-SNE or Euclidean distance analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and Prepare All Summary Results from CSV files in DIR ---\n",
    "\n",
    "print(f\"Loading experiment summaries from: {DIR}\")\n",
    "\n",
    "\n",
    "all_experiment_summaries = []\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "# Check if the file name starts with \"summary_\" and ends with \".csv\"\n",
    "for item_name in os.listdir(DIR):\n",
    "    if item_name.startswith(\"summary_\") and item_name.endswith(\".csv\"):\n",
    "        summary_file_path = os.path.join(DIR, item_name)\n",
    "        experiment_name = item_name[len(\"summary_\"): -len(\".csv\")] \n",
    "\n",
    "        try:\n",
    "            df_exp_summary = pd.read_csv(summary_file_path)\n",
    "            df_exp_summary['experiment_name'] = experiment_name\n",
    "            all_experiment_summaries.append(df_exp_summary)\n",
    "            print(f\"Loaded: {item_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {item_name}: {e}\")\n",
    "\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df_combined_results = pd.concat(all_experiment_summaries, ignore_index=True)\n",
    "\n",
    "# Melt the DataFrame from wide to long format \n",
    "id_vars = ['experiment_name']\n",
    "metric_cols = [col for col in df_combined_results.columns if '_loss' in col or '_lrap' in col or '_ndcg' in col or '_time' in col]\n",
    "\n",
    "df_melted_results = df_combined_results.melt(id_vars=id_vars, \n",
    "                                             value_vars=metric_cols,\n",
    "                                             var_name='metric_full_name', \n",
    "                                             value_name='value')\n",
    "\n",
    "# Extract the metric type and model configuration from the metric_full_name\n",
    "df_melted_results[['model_config', 'metric_type']] = df_melted_results['metric_full_name'].str.rsplit('_', n=1, expand=True)\n",
    "\n",
    "print(\"You now have a bunch of experminental data ready for analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c3bad",
   "metadata": {},
   "source": [
    "## Euclidian distance analysis\n",
    "Created on a whim. \n",
    "This allows you to compare euclidian distances (that annoying copilot autocomplete thing suggested it below code I was writing for t-SNE analysis and I got intrigued and let it finish the code. after editing the code to be actually useful I did some research. seems like this analysis could actually be useful for my project)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d2a39",
   "metadata": {},
   "source": [
    "### Simple framework for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple comparison framework to compare the embeddings of different models\n",
    "\n",
    "# Construct paths to the relevant state dictionaries\n",
    "baseline_trainable_path = f\"{DIR}/saved_models_{EXPERIMENT}/baseline_trainable_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "untrained_baseline_trainable_path = f\"{DIR}/untrained_models_{EXPERIMENT}/baseline_trainable_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "baseline_fixed_path = f\"{DIR}/saved_models_{EXPERIMENT}/baseline_fixed_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "untrained_baseline_fixed_path = f\"{DIR}/untrained_models_{EXPERIMENT}/baseline_fixed_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "\n",
    "# Load the embeddings from the state dictionaries\\n\",\n",
    "baseline_emb = get_embeddings_from_state_dict(baseline_trainable_path)\n",
    "untrained_baseline_emb = get_embeddings_from_state_dict(untrained_baseline_trainable_path)\n",
    "fixed_baseline_emb = get_embeddings_from_state_dict(baseline_fixed_path)\n",
    "untrained_fixed_baseline_emb2 = get_embeddings_from_state_dict(untrained_baseline_fixed_path)\n",
    "\n",
    "# Calculate the Euclidean distance between the embeddings\\n\",\n",
    "fixed_trained_euclidean_distance = np.linalg.norm(baseline_emb - fixed_baseline_emb)\n",
    "fixed_untrained_euclidean_distance = np.linalg.norm(baseline_emb - untrained_baseline_emb)\n",
    "trainable_untrained_euclidean_distance = np.linalg.norm(fixed_baseline_emb - untrained_fixed_baseline_emb2)\n",
    "\n",
    "\n",
    "# Print the Euclidean distance values\n",
    "print(f\"Euclidean distance between baseline trainable and fixed embeddings: {fixed_trained_euclidean_distance}\")\n",
    "print(f\"Euclidean distance between baseline trainable and untrained baseline embeddings: {fixed_untrained_euclidean_distance}\")\n",
    "print(f\"Euclidean distance between fixed baseline and untrained fixed baseline embeddings: {trainable_untrained_euclidean_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a6df5",
   "metadata": {},
   "source": [
    "Reproducability\\untrained_models_kingdom_test1\\baseline_fixed_run_2_state_dict.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16230ee",
   "metadata": {},
   "source": [
    "### Proper analytical framwork running on all runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate a dictionary of lists containing the results of euclidian distance analysis of all embeddings of the experiment specified above ---\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed_everything(BASE_RANDOM_STATE, silent=True)\n",
    "\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "Euclidian_results = {\"baseline_euclidean_distance\": [],\n",
    "                    \"glylm_euclidean_distance\": [],\n",
    "                    \"baseline_glylm_euclidean_distance\": [],\n",
    "                    \"baseline_runs_euclidean_distance\": []}\n",
    "\n",
    "print(f\"Calculating the Euclidean distance of the embeddings of the experiment {EXPERIMENT} with {NUM_RUNS} runs...\")\n",
    "print()\n",
    "\n",
    "for runs in range(NUM_RUNS):\n",
    "    # --- load the embeddings ---\n",
    "    # Construct paths to the relevant state dictionaries\n",
    "    other_run = runs + 2 if runs < NUM_RUNS - 1 else 1 \n",
    "    baseline_trainable_path = f\"{DIR}/saved_models_{EXPERIMENT}/baseline_trainable_run_{runs+1}_state_dict.pth\"\n",
    "    baseline_trainable_comp_path = f\"{DIR}/saved_models_{EXPERIMENT}/baseline_trainable_run_{other_run}_state_dict.pth\"\n",
    "    baseline_fixed_path = f\"{DIR}/untrained_models_{EXPERIMENT}/baseline_trainable_run_{runs+1}_state_dict.pth\"\n",
    "    infused_trainable_path = f\"{DIR}/saved_models_{EXPERIMENT}/infused_trainable_run_{runs+1}_state_dict.pth\"\n",
    "    infused_fixed_path = f\"{DIR}/saved_models_{EXPERIMENT}/infused_fixed_run_{runs+1}_state_dict.pth\"\n",
    "\n",
    "    # Load the embeddings from the state dictionaries\n",
    "    baseline_emb = get_embeddings_from_state_dict(baseline_trainable_path)\n",
    "    baseline_comp_emb = get_embeddings_from_state_dict(baseline_trainable_comp_path)\n",
    "    fixed_baseline_emb = get_embeddings_from_state_dict(baseline_fixed_path)\n",
    "    glylm_emb = get_embeddings_from_state_dict(infused_fixed_path)\n",
    "    trained_glylm_emb = get_embeddings_from_state_dict(infused_trainable_path)\n",
    "\n",
    "    # run L2 normalization on the embeddings\n",
    "    if NORMALIZE:\n",
    "        baseline_emb = baseline_emb / np.max(np.linalg.norm(baseline_emb, axis=1, keepdims=True))\n",
    "        baseline_comp_emb = baseline_comp_emb / np.max(np.linalg.norm(baseline_comp_emb, axis=1, keepdims=True))\n",
    "        fixed_baseline_emb = fixed_baseline_emb / np.max(np.linalg.norm(fixed_baseline_emb, axis=1, keepdims=True))\n",
    "        glylm_emb = glylm_emb / np.max(np.linalg.norm(glylm_emb, axis=1, keepdims=True))\n",
    "        trained_glylm_emb = trained_glylm_emb / np.max(np.linalg.norm(trained_glylm_emb, axis=1, keepdims=True))\n",
    "\n",
    "    # Calculate the Euclidean distance between the embeddings\n",
    "    baseline_euclidean_dist = np.linalg.norm(baseline_emb - fixed_baseline_emb)\n",
    "    glylm_euclidean_dist = np.linalg.norm(glylm_emb - trained_glylm_emb)\n",
    "    baseline_glylm_euclidean_dist = np.linalg.norm(baseline_emb - glylm_emb)\n",
    "    # Calculate the Euclidean distance between the embeddings of two runs\n",
    "    baseline_runs_euclidean_dist = np.linalg.norm(baseline_emb - baseline_comp_emb)\n",
    "\n",
    "    # add the euclidean distance of the current run to the list of results\n",
    "    Euclidian_results[\"baseline_euclidean_distance\"].append(baseline_euclidean_dist)\n",
    "    Euclidian_results[\"glylm_euclidean_distance\"].append(glylm_euclidean_dist)\n",
    "    Euclidian_results[\"baseline_glylm_euclidean_distance\"].append(baseline_glylm_euclidean_dist)\n",
    "    Euclidian_results[\"baseline_runs_euclidean_distance\"].append(baseline_runs_euclidean_dist)\n",
    "\n",
    "# --- Calculate the average of the euclidean distances ---\n",
    "baseline_euclidean_dist_avg = np.mean(Euclidian_results[\"baseline_euclidean_distance\"])\n",
    "glylm_euclidean_dist_avg = np.mean(Euclidian_results[\"glylm_euclidean_distance\"])\n",
    "baseline_glylm_euclidean_dist_avg = np.mean(Euclidian_results[\"baseline_glylm_euclidean_distance\"])\n",
    "baseline_runs_euclidean_dist_avg = np.mean(Euclidian_results[\"baseline_runs_euclidean_distance\"])\n",
    "\n",
    "# Calculate the standard deviation of the euclidean distances\n",
    "baseline_euclidean_dist_std = np.std(Euclidian_results[\"baseline_euclidean_distance\"])\n",
    "glylm_euclidean_dist_std = np.std(Euclidian_results[\"glylm_euclidean_distance\"])\n",
    "baseline_glylm_euclidean_dist_std = np.std(Euclidian_results[\"baseline_glylm_euclidean_distance\"])\n",
    "baseline_runs_euclidean_dist_std = np.std(Euclidian_results[\"baseline_runs_euclidean_distance\"])\n",
    "\n",
    "# Print the average euclidean distance values with their standard deviation\n",
    "print(f\"Average Baseline (fixed to trainable) Euclidean Distance: {baseline_euclidean_dist_avg:.2f} ± {baseline_euclidean_dist_std:.2f}\")\n",
    "print(f\"Average GlyLM (fixed to trainable) Euclidean Distance: {glylm_euclidean_dist_avg:.3f} ± {glylm_euclidean_dist_std:.3f}\")\n",
    "print(f\"Average Baseline-GlyLM [trainable] Euclidean Distance: {baseline_glylm_euclidean_dist_avg:.3f} ± {baseline_glylm_euclidean_dist_std:.3f}\")\n",
    "print(f\"Average Baseline [trainable] (comparing runs) Euclidean Distance: {baseline_runs_euclidean_dist_avg:.3f} ± {baseline_runs_euclidean_dist_std:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb87be51",
   "metadata": {},
   "source": [
    "#### Before  normalization\n",
    "```\n",
    "Calculating the Euclidean distance of the embeddings of the experiment Kingdom1 with 10 runs...\n",
    "\n",
    "Average Baseline (fixed to trainable) Euclidean Distance: 1281.91 ± 0.83 <-- ignore this, that comparison was flawed\n",
    "Average GlyLM (fixed to trainable) Euclidean Distance: 30.51 ± 1.66\n",
    "Average Baseline-GlyLM [trainable] Euclidean Distance: 1217.21 ± 0.54\n",
    "Average Baseline [trainable] (comparing runs) Euclidean Distance: 1282.16 ± 1.21\n",
    "```\n",
    "#### After normalization\n",
    "```\n",
    "Calculating the Euclidean distance of the embeddings of the experiment Kingdom1 with 10 runs...\n",
    "\n",
    "Average Baseline (fixed to trainable) Euclidean Distance needs to be calculated with the untrained baseline\n",
    "Average GlyLM (fixed to trainable) Euclidean Distance: 5.23 ± 0.99\n",
    "Average Baseline-GlyLM [trainable] Euclidean Distance: 61.78 ± 1.04\n",
    "Average Baseline [trainable] (comparing runs) Euclidean Distance: 60.21 ± 1.00\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b98e4",
   "metadata": {},
   "source": [
    "## t-SNE Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loading the embeddings of the specified run number for t-SNE ---\n",
    "\n",
    "# Construct paths to the relevant state dictionaries for the specified run number\n",
    "baseline_trained_path = f\"{DIR}/saved_models_{EXPERIMENT}/baseline_trainable_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "baseline_fixed_path = f\"{DIR}/untrained_models_{EXPERIMENT}/baseline_trainable_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "infused_trained_path = f\"{DIR}/saved_models_{EXPERIMENT}/infused_trainable_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "infused_fixed_path = f\"{DIR}/saved_models_{EXPERIMENT}/infused_fixed_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "\n",
    "# comparing different runs of the same model\n",
    "other_run = RUN_NUMBER + 1 if RUN_NUMBER != 5 else 1 \n",
    "\n",
    "baseline_trained_comp_path = f\"{DIR}/saved_models_{EXPERIMENT}/baseline_trainable_run_{other_run}_state_dict.pth\"\n",
    "baseline_fixed_comp_path = f\"{DIR}/untrained_models_{EXPERIMENT}/baseline_trainable_run_{other_run}_state_dict.pth\"\n",
    "infused_trained_comp_path = f\"{DIR}/saved_models_{EXPERIMENT}/infused_trainable_run_{other_run}_state_dict.pth\"\n",
    "\n",
    "# Load the embeddings from the state dictionaries\n",
    "baseline_trained_emb = get_embeddings_from_state_dict(baseline_trained_path)\n",
    "baseline_fixed_emb = get_embeddings_from_state_dict(baseline_fixed_path)\n",
    "infused_trained_emb = get_embeddings_from_state_dict(infused_trained_path)\n",
    "infused_fixed_emb = get_embeddings_from_state_dict(infused_fixed_path)\n",
    "\n",
    "baseline_trained_comp_emb = get_embeddings_from_state_dict(baseline_trained_comp_path)\n",
    "baseline_fixed_comp_emb = get_embeddings_from_state_dict(baseline_fixed_comp_path)\n",
    "infused_trained_comp_emb = get_embeddings_from_state_dict(infused_trained_comp_path)\n",
    "\n",
    "# --- Construct a dictionary of the lists of embedding arrays and labels for prepare_tsne_data ---\n",
    "all_tsne_embeddings = [\n",
    "\n",
    "# All embeddings for t-SNE\n",
    "{'name' : 'all', 'embeddings' : [baseline_trained_emb, baseline_fixed_emb, infused_trained_emb, infused_fixed_emb],\n",
    "'labels' : [\"Baseline (trained)\", \"Baseline (original)\", \"GlyLM (trained)\", \"GlyLM (original)\"]},\n",
    "# Baseline comparison before and after training\n",
    "{'name' : 'baseline', 'embeddings' : [baseline_trained_emb, baseline_fixed_emb],\n",
    "'labels' : [\"Baseline (trained)\", \"Baseline (original)\"]},\n",
    "# Infused comparison before and after training\n",
    "{'name' : 'infused', 'embeddings' : [infused_trained_emb, infused_fixed_emb],\n",
    "'labels' : [\"Infused (trained)\", \"Infused (original)\"]},\n",
    "# Baseline vs Infused comparison\n",
    "{'name' : 'baseline_infused', 'embeddings' : [baseline_trained_emb, infused_trained_emb],\n",
    "'labels' : [\"Baseline (trained)\", \"Infused (trained)\"]},\n",
    "# Just the Baseline embeddings\n",
    "{'name' : 'base', 'embeddings' : [baseline_trained_emb],\n",
    "'labels' : [\"Baseline embeddings after training\"]},\n",
    "# Just the Infused embeddings\n",
    "{'name' : 'GlyLM', 'embeddings' : [infused_trained_emb],\n",
    "'labels' : [\"GlyLM embeddings\"]},\n",
    "# Baseline comparison between two runs\n",
    "{'name' : 'baseline_comp', 'embeddings' : [baseline_trained_comp_emb, baseline_trained_emb],\n",
    "'labels' : [\"Baseline (run 1)\", \"Baseline (run 2)\"]},\n",
    "# Infused comparison between two runs\n",
    "{'name' : 'infused_comp', 'embeddings' : [infused_trained_comp_emb, infused_trained_emb],\n",
    "'labels' : [\"Infused (run 1)\", \"Infused (run 2)\"]},\n",
    "# Baseline Fixed comparison between two runs\n",
    "{'name' : 'baseline_fixed_comp', 'embeddings' : [baseline_fixed_comp_emb, baseline_fixed_emb],\n",
    "'labels' : [\"Fixed Baseline (run 1)\", \"Fixed Baseline (run 2)\"]},\n",
    "# Add more comparisons as needed\n",
    "]   \n",
    "\n",
    "\n",
    "# Prepare the data for t-SNE\n",
    "for tsne_target in all_tsne_embeddings:\n",
    "    tsne_target['embeddings'], tsne_target['labels'] = prepare_tsne_data(\n",
    "        tsne_target['embeddings'], tsne_target['labels'], normalize=NORMALIZE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c8c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick loader to just look at one set of embeddings\n",
    "\n",
    "embeddings = get_embeddings_from_state_dict(\"../Shit/Embedding_experiments_3/saved_models_kingdom_emb1/baseline_trainable_run_1_state_dict.pth\")\n",
    "test_name = \"embedding 1\"\n",
    "\n",
    "\n",
    "tsne_input_array, tsne_labels = prepare_tsne_data(\n",
    "    embedding_arrays=[embeddings], # Pass 'embeddings' as a list\n",
    "    embedding_names=[test_name], # Pass 'test_name' as a list\n",
    "    normalize=NORMALIZE\n",
    ")\n",
    "\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components=3, \n",
    "            random_state=BASE_RANDOM_STATE, \n",
    "            perplexity=30, \n",
    "            learning_rate=200,\n",
    "            max_iter=1000)\n",
    "\n",
    "\n",
    "# Pass the directly obtained tsne_input_array to fit_transform\n",
    "tsne_coords = tsne.fit_transform(tsne_input_array)\n",
    "\n",
    "print(\"t-SNE coordinates generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39feaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- t-SNE analysis ---\n",
    "# possible tsne_target_ids: baseline_infused, infused, baseline, base, GlyLM, all \n",
    "# Comparison between runs of the same model: baseline_comp, infused_comp, baseline_fixed_comp\n",
    "\n",
    "tsne_target_id = \"infused\"\n",
    "for tsne_target in all_tsne_embeddings:\n",
    "    if tsne_target['name'] == tsne_target_id:\n",
    "        tsne_embeddings = tsne_target['embeddings']\n",
    "        tsne_labels = tsne_target['labels']\n",
    "        break\n",
    "else:\n",
    "    raise ValueError(f\"Target '{tsne_target_id}' not found in the list of embeddings.\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components=2, # change to 3 for 3D visualization\n",
    "            random_state=BASE_RANDOM_STATE, \n",
    "            perplexity=30, # experiment between 5 and 50\n",
    "            learning_rate=300, # experiment between 100 and 1000\n",
    "            max_iter=1000) # experiment between 1000 and 5000\n",
    "# Experiment with perplexity, learning_rate, and n_iter later for optimal visualization.\n",
    "# (create a function to do this automatically)\n",
    "\n",
    "# Initialize the t-SNE model and fit it to the embeddings\n",
    "tsne_coords = tsne.fit_transform(tsne_embeddings)\n",
    "\n",
    "glycoword_base_list = sorted(lib.keys()) \n",
    "glycoword_base_list.append('Ontology')\n",
    "num_concatenated_sets = tsne_coords.shape[0] // len(glycoword_base_list)\n",
    "tsne_glycowords = glycoword_base_list * num_concatenated_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9fcf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot t-SNE scatterplot or density plot (or both) ---\n",
    "# Settings\n",
    "density = False # Set to True for density plot\n",
    "scatter = True # Set to True for scatter plot\n",
    "glycoword_annotate = False # Set to True to annotate glycowords on the plot\n",
    "snfg_only = True # Set to True to only plot SNFG monosaccharides  (if glycoword_annotate is True)\n",
    "\n",
    "# Glycowords to annotate on the plot\n",
    "glycowords_to_annotate = [\n",
    "    'Gal', 'GlcNAc', 'Man', 'Fuc', 'Neu5Ac', 'Glc', \n",
    "]\n",
    "\n",
    "print(f't-SNE Visualization of {tsne_target_id.replace(\"_\", \" \").title()} Embeddings of {EXPERIMENT.replace(\"_\", \" \").title()} Experiment')\n",
    "\n",
    "# --- Plotting the t-SNE results ---\n",
    "# Settings for the plots\n",
    "plt.figure(figsize=(10, 10)) # Adjust figure size as needed\n",
    "# Set the color palette\n",
    "sns.set_palette(\"colorblind\") # Choose a color palette (e.g., 'viridis', 'tab10', 'Paired', 'colorblind', etc.)\n",
    "# Set the font size for the plot\n",
    "plt.rcParams.update({'font.size': 14}) # Adjust font size for better readability\n",
    "\n",
    "\n",
    "# Generate a DataFrame for the t-SNE coordinates and labels\n",
    "plot_df = pd.DataFrame(tsne_coords, columns=['x', 'y'])\n",
    "plot_df['Embedding Source'] = tsne_labels\n",
    "plot_df['Glycoword'] = tsne_glycowords \n",
    "\n",
    "# SNFG monosaccharides\n",
    "snfg_glycowords = [\n",
    "    '4eLeg', '6dAlt', '6dAltNAc', '6dGul', '6dTal', '6dTalNAc', 'Abe', 'Aci', 'All', 'AllA',\n",
    "    'AllN', 'AllNAc', 'Alt', 'AltA', 'AltN', 'AltNAc', 'Api', 'Ara', 'Bac', 'Col',\n",
    "    'DDmanHep', 'Dha', 'Dig', 'Fru', 'Fuc', 'FucNAc', 'Gal', 'GalA', 'GalN', 'GalNAc',\n",
    "    'Glc', 'GlcA', 'GlcN', 'GlcNAc', 'Gul', 'GulA', 'GulN', 'GulNAc', 'Ido', 'IdoA',\n",
    "    'IdoN', 'IdoNAc', 'Kdn', 'Kdo', 'Leg', 'LDmanHep', 'Lyx', 'Man', 'ManA', 'ManN',\n",
    "    'ManNAc', 'Mur', 'MurNAc', 'MurNGc', 'Neu', 'Neu5Ac', 'Neu5Gc', 'Oli', 'Par', 'Pse',\n",
    "    'Psi', 'Qui', 'QuiNAc', 'Rha', 'RhaNAc', 'Rib', 'Sia', 'Sor', 'Tag', 'Tal',\n",
    "    'TalA', 'TalN', 'TalNAc', 'Tyv', 'Xyl'\n",
    "]\n",
    "\n",
    "# If snfg_only is True, filter the data to only include SNFG monosaccharides\n",
    "if snfg_only:\n",
    "    # Filter plot_df to include only rows where 'Glycoword' is in the SNFG set\n",
    "    plot_df_filtered = plot_df[plot_df['Glycoword'].isin(snfg_glycowords)].copy()\n",
    "\n",
    "    # Update the lists for annotation/symbolization to only include filtered glycowords\n",
    "    # This ensures warnings aren't printed for non-SNFG glycowords\n",
    "    glycowords_to_annotate_filtered = [word for word in glycowords_to_annotate if word in snfg_glycowords]\n",
    "    #glycowords_to_symbolize_filtered = {word: marker for word, marker in glycowords_to_symbolize.items() if word in snfg_glycowords}\n",
    "\n",
    "    # Use the filtered DataFrame and filtered lists for plotting\n",
    "    plot_df = plot_df_filtered # Use the filtered DataFrame from here on\n",
    "    glycowords_to_annotate = glycowords_to_annotate_filtered\n",
    "    #glycowords_to_symbolize = glycowords_to_symbolize_filtered\n",
    "\n",
    "\n",
    "if density:\n",
    "    sns.kdeplot(\n",
    "        x='x', y='y',\n",
    "        hue='Embedding Source',\n",
    "        data=plot_df,\n",
    "        fill=True, # Fill the density areas\n",
    "        alpha=0.5, # Transparency for density areas\n",
    "        linewidth=0, # Remove line borders for smoother fills\n",
    "        gridsize=100, # Number of grid points for density estimation\n",
    "    )\n",
    "\n",
    "if scatter:\n",
    "    sns.scatterplot(\n",
    "        x='x', y='y',\n",
    "        hue='Embedding Source', # Color points by their source (Trained Baseline, Raw GlyLM, Trained Infused)\n",
    "        data=plot_df,\n",
    "        legend='brief', #'auto', 'brief', 'full', or False\n",
    "        alpha=0.5, # Transparency for overlapping points\n",
    "        s=20, # Size of points\n",
    "        #edgecolor='black' # edge color for better visibility\n",
    "    )\n",
    "if glycoword_annotate:\n",
    "    for glycoword in glycowords_to_annotate:\n",
    "        # Find all points corresponding to this glycoword\n",
    "        glycoword_points = plot_df[plot_df['Glycoword'] == glycoword]\n",
    "        \n",
    "        if not glycoword_points.empty:\n",
    "            # For simplicity, let's annotate the first occurrence if multiple, or average\n",
    "            # To avoid clutter, you might pick only one point per glycoword to annotate if many exist\n",
    "            # Or, plot them all with smaller text/different style\n",
    "            \n",
    "            # Example: Annotate the first instance of the glycoword found\n",
    "            x_coord = glycoword_points['x'].iloc[0]\n",
    "            y_coord = glycoword_points['y'].iloc[0]\n",
    "            \n",
    "            plt.annotate(glycoword, (x_coord, y_coord),\n",
    "                        textcoords=\"offset points\", # Offset text from point\n",
    "                        xytext=(5,5), # Offset by (5,5) pixels\n",
    "                        ha='center', # Horizontal alignment\n",
    "                        arrowprops=dict(arrowstyle='-', connectionstyle='arc3,rad=.2', linewidth=0.5))\n",
    "        else:\n",
    "            print(f\"Warning: Glycoword '{glycoword}' not found in the t-SNE data to annotate.\")\n",
    "\n",
    "\n",
    "\n",
    "#plt.title('t-SNE Visualization of Glycan Embeddings')\n",
    "plt.title(f't-SNE Visualization of {tsne_target_id.replace(\"_\", \" \").title()} Embeddings of {EXPERIMENT.replace(\"_\", \" \").title()} Experiment', fontsize=16)\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "# Fine-tune legend position\n",
    "plt.legend(title='Embedding Source', loc='upper right', bbox_to_anchor=(1, 1), \n",
    "           ncol=1, fontsize = 14, edgecolor='black', framealpha=0.8, markerscale = 2,\n",
    "           markerfirst = True, fancybox=True, draggable = True)\n",
    "# move the legend outside the plot\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "#plt.grid(True, linestyle='--', alpha=0.6) # Add a subtle grid\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47707555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3D t-SNE scatterplot\n",
    "\n",
    "# --- Imports needed for 3D plotting (ensure these are at the top of your notebook) ---\n",
    "import matplotlib.pyplot as plt # <--- ADD THIS IMPORT\n",
    "from mpl_toolkits.mplot3d import Axes3D # For 3D plotting\n",
    "import pandas as pd\n",
    "import numpy as np # <--- ADD THIS IMPORT (often used for plot utilities like colormaps)\n",
    "\n",
    "# --- Plotting Code ---\n",
    "fig = plt.figure(figsize=(12, 10)) # Adjust figure size as needed\n",
    "ax = fig.add_subplot(111, projection='3d') # Create a 3D subplot\n",
    "\n",
    "# Create a DataFrame for easier handling of colors/labels with matplotlib\n",
    "plot_df_3d = pd.DataFrame(tsne_coords, columns=['x', 'y', 'z'])\n",
    "plot_df_3d['Embedding Source'] = tsne_labels\n",
    "\n",
    "# Get unique labels for coloring\n",
    "unique_labels = plot_df_3d['Embedding Source'].unique()\n",
    "colors = plt.cm.get_cmap('viridis', len(unique_labels)) # Use a colormap for distinct colors\n",
    "\n",
    "# Plot each point\n",
    "for i, label in enumerate(unique_labels):\n",
    "    subset = plot_df_3d[plot_df_3d['Embedding Source'] == label]\n",
    "    ax.scatter(subset['x'], subset['y'], subset['z'],\n",
    "               color=colors(i),\n",
    "               label=label,\n",
    "               alpha=0.6,\n",
    "               s=20) # Size of points\n",
    "\n",
    "ax.set_title(f'3D t-SNE Visualization of {test_name} Embeddings') # Use test_name for title\n",
    "ax.set_xlabel('t-SNE Component 1')\n",
    "ax.set_ylabel('t-SNE Component 2')\n",
    "ax.set_zlabel('t-SNE Component 3')\n",
    "\n",
    "ax.legend(title='Embedding Source', bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "plt.show() # <--- ADD THIS LINE TO DISPLAY THE PLOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0020606d",
   "metadata": {},
   "source": [
    "## Statistical Analysis of Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf216074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Statistical Comparisons ---\n",
    "\n",
    "# This will define the statistical comparisons you want to perform.\n",
    "# Each entry specifies a 'metric_type' to analyze and the 'groups' to compare.\n",
    "# Possible metric types: 'lrap', 'time', 'loss', \n",
    "# possible configs: baseline_trainable, baseline_fixed, infused_trainable, infused_fixed\n",
    "\n",
    "statistical_comparisons = [\n",
    "    {\n",
    "        'name': 'Infused_vs_Baseline_LRAP',\n",
    "        'metric_type': 'lrap',\n",
    "        'group1_config': 'infused_trainable',\n",
    "        'group2_config': 'baseline_trainable',\n",
    "        'description': 'Comparison of Infused (trainable) vs. Baseline (trainable) LRAP.'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Baseline_Fixed_vs_Trainable_LRAP',\n",
    "        'metric_type': 'lrap',\n",
    "        'group1_config': 'baseline_trainable',\n",
    "        'group2_config': 'baseline_fixed',\n",
    "        'description': 'Comparison of Baseline (trainable) vs. Baseline (fixed) LRAP.'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Infused_Fixed_vs_Trainable_LRAP',\n",
    "        'metric_type': 'lrap',\n",
    "        'group1_config': 'infused_trainable',\n",
    "        'group2_config': 'infused_fixed',\n",
    "        'description': 'Comparison of Infused (trainable) vs. Infused (fixed) LRAP.'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Infused_vs_Baseline_Time', \n",
    "        'metric_type': 'time',\n",
    "        'group1_config': 'infused_trainable',\n",
    "        'group2_config': 'baseline_trainable',\n",
    "        'description': 'Comparison of Infused (trainable) vs. Baseline (trainable) training time.'\n",
    "    },\n",
    "    # Add more comparisons here as needed, \n",
    "]\n",
    "\n",
    "print(f\"Defined {len(statistical_comparisons)} statistical comparisons to run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a3127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quick comparison of the results of the different Experiments using just baseline trainable and one metric ---\n",
    "metric_quick = 'lrap' # Choose the metric to analyze (e.g., 'lrap', 'loss', 'ndcg' 'time')\n",
    "configuration = 'infused_trainable' # baseline_trainable, infused_trainable, etc.\n",
    "# Filter the melted DataFrame for LRAP results\n",
    "df_lrap_results = df_melted_results[df_melted_results['metric_type'] == metric_quick]\n",
    "df_lrap_results = df_lrap_results[df_lrap_results['model_config'] == configuration]\n",
    "df_lrap_results = df_lrap_results.groupby(['experiment_name'])['value'].agg(['mean', 'std', 'count']).reset_index()\n",
    "\n",
    "print(f\"Quick comparison of the {metric_quick} results for {configuration} of the different experiments:\")\n",
    "print(df_lrap_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce8c01",
   "metadata": {},
   "source": [
    "min_class_size = 2\n",
    "Quick comparison of the lrap results for baseline_trainable of the different experiments:\n",
    "       experiment_name      mean       std  count\n",
    "0  disease_association  0.842077  0.012655     10\n",
    "2      species_kingdom  0.959948  0.001940     10\n",
    "3        tissue_sample  0.745605  0.021077     10\n",
    "\n",
    "vs min_class_size = 32\n",
    "Quick comparison of the lrap results for baseline_trainable of the different experiments:\n",
    "            experiment_name      mean       std  count\n",
    "0    disease_assoc_e5_min32  0.918973  0.012601     10\n",
    "1  species_kingdom_e5_min32  0.964062  0.002279     10\n",
    "2    tissue_sample_e1_min32  0.827414  0.020074     10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82393494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate Performance Tables (Mean ± Std) for a Specific Metric ---\n",
    "\n",
    "# Settings\n",
    "table_metric = 'lrap' # Choose the metric to analyze ('lrap', 'loss', 'ndcg' 'time')\n",
    "table_name = f\"Core table({table_metric})\" # Name of the table to be generated\n",
    "\n",
    "# Filter data for the chosen metric type\n",
    "df_filtered_metric = df_melted_results[df_melted_results['metric_type'] == table_metric]\n",
    "\n",
    "# Aggregate mean and std\n",
    "aggregated_stats = df_filtered_metric.groupby(['model_config', 'experiment_name', 'metric_type'])['value'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Format 'mean' and 'std' into a single 'mean ± std' string\n",
    "aggregated_stats['mean_std_string'] = aggregated_stats.apply(\n",
    "    lambda row: f\"{row['mean']:.3f} ± {row['std']:.3f}\", axis=1\n",
    ")\n",
    "\n",
    "# Pivot the table\n",
    "pivoted_table = aggregated_stats.pivot_table(\n",
    "    index='experiment_name',\n",
    "    columns='model_config',\n",
    "    values='mean_std_string',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Print the table\n",
    "print(f\"\\n--- Generated Table ---\")\n",
    "print(pivoted_table) \n",
    "\n",
    "# Save the table to CSV\n",
    "output_filename = os.path.join(STAT_DIR, f\"{table_name}.csv\")\n",
    "pivoted_table.to_csv(output_filename, index=True) \n",
    "print(f\"\\nTable saved to: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ff5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- t-Test Statistical Comparisons ----\n",
    "\n",
    "\n",
    "stat_comp_name = \"Base_stats\" # statistical comparison file name\n",
    "\n",
    "# Initialize a list to store t-test results\n",
    "t_test_results = []\n",
    "\n",
    "# Iterate through the defined statistical comparisons\n",
    "for comp_def in statistical_comparisons:\n",
    "    metric_type = comp_def['metric_type']\n",
    "    group1_config = comp_def['group1_config']\n",
    "    group2_config = comp_def['group2_config']\n",
    "\n",
    "    # Filter data for the specific metric type\n",
    "    df_metric = df_melted_results[df_melted_results['metric_type'] == metric_type]\n",
    "\n",
    "    # Per-Experiment Comparisons\n",
    "    for experiment in df_metric['experiment_name'].unique():\n",
    "        data_g1 = df_metric[(df_metric['experiment_name'] == experiment) & \n",
    "                            (df_metric['model_config'] == group1_config)]['value']\n",
    "        data_g2 = df_metric[(df_metric['experiment_name'] == experiment) & \n",
    "                            (df_metric['model_config'] == group2_config)]['value']\n",
    "\n",
    "        t_stat, p_value = stats.ttest_ind(data_g1, data_g2)\n",
    "        \n",
    "        t_test_results.append({\n",
    "            'comparison_name': comp_def['name'],\n",
    "            'metric_type': metric_type,\n",
    "            'experiment_name': experiment,\n",
    "            'group1': group1_config,\n",
    "            'group2': group2_config,\n",
    "            'mean_g1': data_g1.mean(),\n",
    "            'std_g1': data_g1.std(),\n",
    "            'mean_g2': data_g2.mean(),\n",
    "            'std_g2': data_g2.std(),\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant_p_05': p_value < 0.05\n",
    "        })\n",
    "\n",
    "    # Overall Comparison\n",
    "    overall_data_g1 = df_metric[df_metric['model_config'] == group1_config]['value']\n",
    "    overall_data_g2 = df_metric[df_metric['model_config'] == group2_config]['value']\n",
    "\n",
    "    t_stat_overall, p_value_overall = stats.ttest_ind(overall_data_g1, overall_data_g2)\n",
    "    \n",
    "    t_test_results.append({\n",
    "        'comparison_name': comp_def['name'],\n",
    "        'metric_type': metric_type,\n",
    "        'experiment_name': 'Overall',\n",
    "        'group1': group1_config,\n",
    "        'group2': group2_config,\n",
    "        'mean_g1': overall_data_g1.mean(),\n",
    "        'std_g1': overall_data_g1.std(),\n",
    "        'mean_g2': overall_data_g2.mean(),\n",
    "        'std_g2': overall_data_g2.std(),\n",
    "        't_statistic': t_stat_overall,\n",
    "        'p_value': p_value_overall,\n",
    "        'significant_p_05': p_value_overall < 0.05\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame and save\n",
    "df_t_test_summary = pd.DataFrame(t_test_results)\n",
    "output_filename_t_test = os.path.join(STAT_DIR, f\"{stat_comp_name}.csv\")\n",
    "df_t_test_summary.to_csv(output_filename_t_test, index=False)\n",
    "print(f\"\\nStatistical comparison results saved to: {output_filename_t_test}\")\n",
    "\n",
    "# Print head of the results DataFrame\n",
    "print(\"\\n--- T-Test Summary Results Head ---\")\n",
    "print(df_t_test_summary.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e58e14",
   "metadata": {},
   "source": [
    "## Test set Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21093c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Find Best Model ---\n",
    "# you need to run the Load and Prepare All Summary Results from CSV files in DIR cell before this section to have df_melted_results ready.\n",
    "\n",
    "# Define the metric and strategy for finding the best model\n",
    "metric_to_find_best_by = 'lrap' # 'lrap', 'loss', 'ndcg', 'time'\n",
    "strategy_for_best = 'max' # 'max' for LRAP/NDCG, 'min' for Loss/Time\n",
    "\n",
    "# --- Perform the search ---\n",
    "\n",
    "# Filter for the specific metric type (no filtering by model_config_types)\n",
    "df_filtered_by_metric = df_melted_results[df_melted_results['metric_type'] == metric_to_find_best_by].copy()\n",
    "\n",
    "# Find the row with the best metric value across ALL configurations\n",
    "if strategy_for_best == 'max':\n",
    "    best_row_info = df_filtered_by_metric.loc[df_filtered_by_metric['value'].idxmax()]\n",
    "else: # strategy_for_best == 'min'\n",
    "    best_row_info = df_filtered_by_metric.loc[df_filtered_by_metric['value'].idxmin()]\n",
    "\n",
    "# Extract and print the details of the best model\n",
    "best_experiment_name = best_row_info['experiment_name']\n",
    "best_model_config = best_row_info['model_config'] # This will now be the specific config found\n",
    "best_metric_value = best_row_info['value']\n",
    "original_row_index_in_combined_df = best_row_info.name\n",
    "run_number_within_experiment = (original_row_index_in_combined_df % NUM_RUNS) + 1\n",
    "\n",
    "print(f\"\\n--- Overall Best Model Found (by {metric_to_find_best_by} - {strategy_for_best}) ---\")\n",
    "print(f\"  Best model was found in configuration: {best_model_config}\")\n",
    "print(f\"  Experiment: {best_experiment_name}\")\n",
    "print(f\"  Best {metric_to_find_best_by.upper()} Value: {best_metric_value:.4f}\")\n",
    "print(f\"  Run Number within Experiment: {run_number_within_experiment}/{NUM_RUNS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bbfb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run test tester on best model from experiment ---\n",
    "\n",
    "# Construct relevant paths\n",
    "model_state_path = os.path.join(DIR, f\"saved_models_{best_experiment_name}\",\n",
    "                                f\"{best_model_config}_run_{run_number_within_experiment}_state_dict.pth\")\n",
    "test_set_path = os.path.join(DIR, f\"test_set_{best_experiment_name}.pkl\")\n",
    "experiment_settings_path = os.path.join(DIR, f\"experiment_settings_{best_experiment_name}.yaml\")\n",
    "\n",
    "\n",
    "# Load Experiment Settings to get Model Architecture Parameters\n",
    "with open(experiment_settings_path, 'r') as f:\n",
    "    exp_settings_for_model = yaml.safe_load(f)\n",
    "\n",
    "num_classes = exp_settings_for_model['num_classes']\n",
    "batch_size = exp_settings_for_model['global_parameters_for_this_experiment'].get('batch_size') \n",
    "    \n",
    "    \n",
    "\n",
    "# Instantiate SweetNet Model and Load State Dict \n",
    "lib_size_for_model = len(lib)\n",
    "model_best = SweetNet(lib_size=lib_size_for_model, num_classes=num_classes, hidden_dim=320)\n",
    "model_best.to(device) \n",
    "\n",
    "model_best.load_state_dict(torch.load(model_state_path, map_location=device))\n",
    "\n",
    "''' Using old data that saved the whole dataset, I should actually revert that change and keep this commented out \n",
    "test_data = pickle_loader(test_set_path)\n",
    "test_glycans = test_data['test_glycans']\n",
    "test_labels = test_data['test_labels']\n",
    "test_dataloader = dataset_to_dataloader(glycan_list = test_glycans, \n",
    "                                        labels = test_labels,\n",
    "                                        batch_size = batch_size,\n",
    "                                        drop_last = False,\n",
    "                                        augment_prob = 0, \n",
    "                                        generalization_prob = 0\n",
    "                                        )\n",
    "'''\n",
    "\n",
    "test_dataloader =  pickle_loader(test_set_path)\n",
    "\n",
    "# --- Evaluate the Best Model on the Test Set ---\n",
    "test_metrics = test_model(model_best, test_dataloader, num_classes)\n",
    "\n",
    "print(\"\\n--- Final Test Set Performance for Overall Best Model ---\")\n",
    "print(f\"  Test Loss: {test_metrics['loss']:.4f}\")\n",
    "print(f\"  Test LRAP: {test_metrics['lrap']:.4f}\")\n",
    "print(f\"  Test NDCG: {test_metrics['ndcg']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f54019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweetnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
