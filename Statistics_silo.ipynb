{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports ||||Run on Restart|||\n",
    "\n",
    "from utils import get_embeddings_from_state_dict, seed_everything\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c14b4",
   "metadata": {},
   "source": [
    "# Statistics Silo\n",
    "\n",
    "This is where you pipe in the data generated by the HBBS or ABBS to compare results and get statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba010cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters to set before running the script ---\n",
    "\n",
    "BASE_RANDOM_STATE = 42  # Initial seed for reproducibility of the entire sequence of experiments\n",
    "DIR = \"../Embedding_experiments\" # Path to the directory containing the models in relation to this \n",
    "EXPERIMENT = \"kingdom_emb1\" # Name of the experiment to test\n",
    "NUM_RUNS = 10 # Number of runs of experiment\n",
    "RUN_NUMBER = 7 # Used for t-SNE and other analyzes that take a single run\n",
    "NORMALIZE = True # Normalize the embeddings before t-SNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c3bad",
   "metadata": {},
   "source": [
    "## Euclidian distance analysis\n",
    "Created on a whim. \n",
    "This allows you to compare euclidian distances (that annoying copilot autocomplete thing suggested it below code I was writing for t-SNE analysis and I got intrigued and let it finish the code. after editing the code to be actually useful I did some research. seems like this analysis could actually be useful for my project)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d2a39",
   "metadata": {},
   "source": [
    "### Simple framework for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple comparison framework to compare the embeddings of different models\n",
    "\n",
    "# Construct paths to the relevant state dictionaries\n",
    "baseline_trainable_path = f\"{DIR}/saved_models_{EXPERIMENT}/baseline_trainable_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "untrained_baseline_trainable_path = f\"{DIR}/untrained_models_{EXPERIMENT}/baseline_trainable_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "baseline_fixed_path = f\"{DIR}/saved_models_{EXPERIMENT}/baseline_fixed_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "untrained_baseline_fixed_path = f\"{DIR}/untrained_models_{EXPERIMENT}/baseline_fixed_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "\n",
    "# Load the embeddings from the state dictionaries\\n\",\n",
    "baseline_emb = get_embeddings_from_state_dict(baseline_trainable_path)\n",
    "untrained_baseline_emb = get_embeddings_from_state_dict(untrained_baseline_trainable_path)\n",
    "fixed_baseline_emb = get_embeddings_from_state_dict(baseline_fixed_path)\n",
    "untrained_fixed_baseline_emb2 = get_embeddings_from_state_dict(untrained_baseline_fixed_path)\n",
    "\n",
    "# Calculate the Euclidean distance between the embeddings\\n\",\n",
    "fixed_trained_euclidean_distance = np.linalg.norm(baseline_emb - fixed_baseline_emb)\n",
    "fixed_untrained_euclidean_distance = np.linalg.norm(baseline_emb - untrained_baseline_emb)\n",
    "trainable_untrained_euclidean_distance = np.linalg.norm(fixed_baseline_emb - untrained_fixed_baseline_emb2)\n",
    "\n",
    "\n",
    "# Print the Euclidean distance values\n",
    "print(f\"Euclidean distance between baseline trainable and fixed embeddings: {fixed_trained_euclidean_distance}\")\n",
    "print(f\"Euclidean distance between baseline trainable and untrained baseline embeddings: {fixed_untrained_euclidean_distance}\")\n",
    "print(f\"Euclidean distance between fixed baseline and untrained fixed baseline embeddings: {trainable_untrained_euclidean_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a6df5",
   "metadata": {},
   "source": [
    "Reproducability\\untrained_models_kingdom_test1\\baseline_fixed_run_2_state_dict.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16230ee",
   "metadata": {},
   "source": [
    "### Proper analytical framwork running on all runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate a dictionary of lists containing the results of euclidian distance analysis of all embeddings of the experiment specified above ---\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed_everything(BASE_RANDOM_STATE, silent=True)\n",
    "\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "Euclidian_results = {\"baseline_euclidean_distance\": [],\n",
    "                    \"glylm_euclidean_distance\": [],\n",
    "                    \"baseline_glylm_euclidean_distance\": [],\n",
    "                    \"baseline_runs_euclidean_distance\": []}\n",
    "\n",
    "print(f\"Calculating the Euclidean distance of the embeddings of the experiment {EXPERIMENT} with {NUM_RUNS} runs...\")\n",
    "print()\n",
    "\n",
    "for runs in range(NUM_RUNS):\n",
    "    # --- load the embeddings ---\n",
    "    # Construct paths to the relevant state dictionaries\n",
    "    other_run = runs + 2 if runs < NUM_RUNS - 1 else 1 \n",
    "    baseline_trainable_path = f\"{DIR}/saved_models_{EXPERIMENT}/baseline_trainable_run_{runs+1}_state_dict.pth\"\n",
    "    baseline_trainable_comp_path = f\"{DIR}/untrained_models_{EXPERIMENT}/baseline_trainable_run_{runs+1}_state_dict.pth\"\n",
    "    baseline_fixed_path = f\"{DIR}/saved_models_{EXPERIMENT}/baseline_fixed_run_{runs+1}_state_dict.pth\"\n",
    "    infused_trainable_path = f\"{DIR}/saved_models_{EXPERIMENT}/infused_trainable_run_{runs+1}_state_dict.pth\"\n",
    "    infused_fixed_path = f\"{DIR}/saved_models_{EXPERIMENT}/infused_fixed_run_{runs+1}_state_dict.pth\"\n",
    "\n",
    "    # Load the embeddings from the state dictionaries\n",
    "    baseline_emb = get_embeddings_from_state_dict(baseline_trainable_path)\n",
    "    baseline_comp_emb = get_embeddings_from_state_dict(baseline_trainable_comp_path)\n",
    "    fixed_baseline_emb = get_embeddings_from_state_dict(baseline_fixed_path)\n",
    "    glylm_emb = get_embeddings_from_state_dict(infused_fixed_path)\n",
    "    trained_glylm_emb = get_embeddings_from_state_dict(infused_trainable_path)\n",
    "\n",
    "    # run L2 normalization on the embeddings\n",
    "    baseline_emb = baseline_emb / np.max(np.linalg.norm(baseline_emb, axis=1, keepdims=True))\n",
    "    baseline_comp_emb = baseline_comp_emb / np.max(np.linalg.norm(baseline_comp_emb, axis=1, keepdims=True))\n",
    "    fixed_baseline_emb = fixed_baseline_emb / np.max(np.linalg.norm(fixed_baseline_emb, axis=1, keepdims=True))\n",
    "    glylm_emb = glylm_emb / np.max(np.linalg.norm(glylm_emb, axis=1, keepdims=True))\n",
    "    trained_glylm_emb = trained_glylm_emb / np.max(np.linalg.norm(trained_glylm_emb, axis=1, keepdims=True))\n",
    "\n",
    "    # Calculate the Euclidean distance between the embeddings\n",
    "    baseline_euclidean_dist = np.linalg.norm(baseline_emb - fixed_baseline_emb)\n",
    "    glylm_euclidean_dist = np.linalg.norm(glylm_emb - trained_glylm_emb)\n",
    "    baseline_glylm_euclidean_dist = np.linalg.norm(baseline_emb - glylm_emb)\n",
    "    # Calculate the Euclidean distance between the embeddings of two runs\n",
    "    baseline_runs_euclidean_dist = np.linalg.norm(baseline_emb - baseline_comp_emb)\n",
    "\n",
    "    # add the euclidean distance of the current run to the list of results\n",
    "    Euclidian_results[\"baseline_euclidean_distance\"].append(baseline_euclidean_dist)\n",
    "    Euclidian_results[\"glylm_euclidean_distance\"].append(glylm_euclidean_dist)\n",
    "    Euclidian_results[\"baseline_glylm_euclidean_distance\"].append(baseline_glylm_euclidean_dist)\n",
    "    Euclidian_results[\"baseline_runs_euclidean_distance\"].append(baseline_runs_euclidean_dist)\n",
    "\n",
    "# --- Calculate the average of the euclidean distances ---\n",
    "baseline_euclidean_dist_avg = np.mean(Euclidian_results[\"baseline_euclidean_distance\"])\n",
    "glylm_euclidean_dist_avg = np.mean(Euclidian_results[\"glylm_euclidean_distance\"])\n",
    "baseline_glylm_euclidean_dist_avg = np.mean(Euclidian_results[\"baseline_glylm_euclidean_distance\"])\n",
    "baseline_runs_euclidean_dist_avg = np.mean(Euclidian_results[\"baseline_runs_euclidean_distance\"])\n",
    "\n",
    "# Calculate the standard deviation of the euclidean distances\n",
    "baseline_euclidean_dist_std = np.std(Euclidian_results[\"baseline_euclidean_distance\"])\n",
    "glylm_euclidean_dist_std = np.std(Euclidian_results[\"glylm_euclidean_distance\"])\n",
    "baseline_glylm_euclidean_dist_std = np.std(Euclidian_results[\"baseline_glylm_euclidean_distance\"])\n",
    "baseline_runs_euclidean_dist_std = np.std(Euclidian_results[\"baseline_runs_euclidean_distance\"])\n",
    "\n",
    "# Print the average euclidean distance values with their standard deviation\n",
    "#print(f\"Average Baseline (fixed to trainable) Euclidean Distance: {baseline_euclidean_dist_avg:.2f} ± {baseline_euclidean_dist_std:.2f}\")\n",
    "print(f\"Average Baseline (fixed to trainable) Euclidean Distance: {baseline_runs_euclidean_dist_avg:.3f} ± {baseline_runs_euclidean_dist_std:.3f}\")\n",
    "print(f\"Average GlyLM (fixed to trainable) Euclidean Distance: {glylm_euclidean_dist_avg:.3f} ± {glylm_euclidean_dist_std:.3f}\")\n",
    "print(f\"Average Baseline-GlyLM [trainable] Euclidean Distance: {baseline_glylm_euclidean_dist_avg:.3f} ± {baseline_glylm_euclidean_dist_std:.3f}\")\n",
    "print(f\"Average Baseline [trainable] (comparing runs) Euclidean Distance: {baseline_runs_euclidean_dist_avg:.3f} ± {baseline_runs_euclidean_dist_std:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb87be51",
   "metadata": {},
   "source": [
    "#### Before  normalization\n",
    "```\n",
    "Calculating the Euclidean distance of the embeddings of the experiment Kingdom1 with 10 runs...\n",
    "\n",
    "Average Baseline (fixed to trainable) Euclidean Distance: 1281.91 ± 0.83 <-- ignore this, that comparison was flawed\n",
    "Average GlyLM (fixed to trainable) Euclidean Distance: 30.51 ± 1.66\n",
    "Average Baseline-GlyLM [trainable] Euclidean Distance: 1217.21 ± 0.54\n",
    "Average Baseline [trainable] (comparing runs) Euclidean Distance: 1282.16 ± 1.21\n",
    "```\n",
    "#### After normalization\n",
    "```\n",
    "Calculating the Euclidean distance of the embeddings of the experiment Kingdom1 with 10 runs...\n",
    "\n",
    "Average Baseline (fixed to trainable) Euclidean Distance needs to be calculated with the untrained baseline\n",
    "Average GlyLM (fixed to trainable) Euclidean Distance: 5.23 ± 0.99\n",
    "Average Baseline-GlyLM [trainable] Euclidean Distance: 61.78 ± 1.04\n",
    "Average Baseline [trainable] (comparing runs) Euclidean Distance: 60.21 ± 1.00\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b98e4",
   "metadata": {},
   "source": [
    "## t-SNE Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fde7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "def prepare_tsne_data(embedding_arrays: List[np.ndarray], \n",
    "                      embedding_names: List[str],\n",
    "                      normalize: bool = True\n",
    "                      ) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Prepares embedding data for t-SNE visualization by normalizing and concatenating.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_arrays : List[np.ndarray]\n",
    "        A list of NumPy arrays, where each array contains a set of embeddings\n",
    "        (e.g., [baseline_embs, raw_glm_embs, infused_embs]).\n",
    "        All arrays in the list must have the same number of rows (glycowords)\n",
    "        and same number of columns (embedding dimensions).\n",
    "    embedding_names : List[str]\n",
    "        A list of string names corresponding to each array in `embedding_arrays`.\n",
    "        These names will be used as labels in the t-SNE plot legend.\n",
    "    normalize : bool, optional\n",
    "        If True, each embedding array will be normalized before concatenation.\n",
    "        Default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, List[str]]\n",
    "        A tuple containing:\n",
    "        - tsne_embeddings (np.ndarray): All input embeddings, normalized and vertically stacked.\n",
    "        - tsne_labels (List[str]): Corresponding labels for each row in all_embs_for_tsne.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the number of embedding arrays does not match the number of names,\n",
    "        are empty, don't have the same number of rows,\n",
    "        or if the arrays have inconsistent shapes.\n",
    "    Exception\n",
    "        If an error occurs during normalization.\n",
    "    \"\"\"\n",
    "    if len(embedding_arrays) != len(embedding_names):\n",
    "        raise ValueError(\"Number of embedding arrays must match number of embedding names.\")\n",
    "    if not embedding_arrays: # Handle empty input list\n",
    "        raise ValueError(\"No embedding arrays provided.\")\n",
    "\n",
    "    # Get the number of glycowords (rows) from the first embedding array\n",
    "    num_glycowords = embedding_arrays[0].shape[0]\n",
    "\n",
    "    # Normalize each embedding array and collect them if that flag is set\n",
    "    \n",
    "    normalized_arrays = []\n",
    "    for arr in embedding_arrays:\n",
    "        if arr.shape[0] != num_glycowords:\n",
    "            raise ValueError(\"All embedding arrays must have the same number of rows (glycowords).\")\n",
    "        if normalize:\n",
    "            try:\n",
    "                arr = arr / np.max(np.linalg.norm(arr, axis=1, keepdims=True))\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error normalizing array: {e}\")\n",
    "            normalized_arrays.append(arr)\n",
    "            embedding_arrays = normalized_arrays\n",
    "\n",
    "    # Concatenate all normalized arrays vertically\n",
    "    tsne_embeddings = np.concatenate(embedding_arrays, axis=0)\n",
    "    \n",
    "    # Create the combined list of labels\n",
    "    tsne_labels = []\n",
    "    for name in embedding_names:\n",
    "        tsne_labels.extend([name] * num_glycowords) # Extend with 'num_glycowords' repetitions of each name\n",
    "\n",
    "    return tsne_embeddings, tsne_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loading the embeddings of the specified run number for t-SNE ---\n",
    "\n",
    "# Construct paths to the relevant state dictionaries for the specified run number\n",
    "baseline_trained_path = f\"{DIR}/saved_models_{EXPERIMENT}/baseline_trainable_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "baseline_fixed_path = f\"{DIR}/untrained_models_{EXPERIMENT}/baseline_trainable_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "infused_trained_path = f\"{DIR}/saved_models_{EXPERIMENT}/infused_trainable_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "infused_fixed_path = f\"{DIR}/saved_models_{EXPERIMENT}/infused_fixed_run_{RUN_NUMBER}_state_dict.pth\"\n",
    "\n",
    "# comparing different runs of the same model\n",
    "other_run = RUN_NUMBER + 1 if RUN_NUMBER != 5 else 1 \n",
    "\n",
    "baseline_trained_comp_path = f\"{DIR}/saved_models_{EXPERIMENT}/baseline_trainable_run_{other_run}_state_dict.pth\"\n",
    "baseline_fixed_comp_path = f\"{DIR}/untrained_models_{EXPERIMENT}/baseline_trainable_run_{other_run}_state_dict.pth\"\n",
    "infused_trained_comp_path = f\"{DIR}/saved_models_{EXPERIMENT}/infused_trainable_run_{other_run}_state_dict.pth\"\n",
    "\n",
    "# Load the embeddings from the state dictionaries\n",
    "baseline_trained_emb = get_embeddings_from_state_dict(baseline_trained_path)\n",
    "baseline_fixed_emb = get_embeddings_from_state_dict(baseline_fixed_path)\n",
    "infused_trained_emb = get_embeddings_from_state_dict(infused_trained_path)\n",
    "infused_fixed_emb = get_embeddings_from_state_dict(infused_fixed_path)\n",
    "\n",
    "baseline_trained_comp_emb = get_embeddings_from_state_dict(baseline_trained_comp_path)\n",
    "baseline_fixed_comp_emb = get_embeddings_from_state_dict(baseline_fixed_comp_path)\n",
    "infused_trained_comp_emb = get_embeddings_from_state_dict(infused_trained_comp_path)\n",
    "\n",
    "# --- Construct a dictionary of the lists of embedding arrays and labels for prepare_tsne_data ---\n",
    "all_tsne_embeddings = [\n",
    "\n",
    "# All embeddings for t-SNE\n",
    "{'name' : 'all', 'embeddings' : [baseline_trained_emb, baseline_fixed_emb, infused_trained_emb, infused_fixed_emb],\n",
    "'labels' : [\"Baseline (trained)\", \"Baseline (original)\", \"GlyLM (trained)\", \"GlyLM (original)\"]},\n",
    "# Baseline comparison before and after training\n",
    "{'name' : 'baseline', 'embeddings' : [baseline_trained_emb, baseline_fixed_emb],\n",
    "'labels' : [\"Baseline (trained)\", \"Baseline (original)\"]},\n",
    "# Infused comparison before and after training\n",
    "{'name' : 'infused', 'embeddings' : [infused_trained_emb, infused_fixed_emb],\n",
    "'labels' : [\"Infused (trained)\", \"Infused (original)\"]},\n",
    "# Baseline vs Infused comparison\n",
    "{'name' : 'baseline_infused', 'embeddings' : [baseline_trained_emb, infused_trained_emb],\n",
    "'labels' : [\"Baseline (trained)\", \"Infused (trained)\"]},\n",
    "# Just the Baseline embeddings\n",
    "{'name' : 'base', 'embeddings' : [baseline_trained_emb],\n",
    "'labels' : [\"Baseline embeddings after training\"]},\n",
    "# Just the Infused embeddings\n",
    "{'name' : 'GlyLM', 'embeddings' : [infused_trained_emb],\n",
    "'labels' : [\"GlyLM embeddings\"]},\n",
    "# Baseline comparison between two runs\n",
    "{'name' : 'baseline_comp', 'embeddings' : [baseline_trained_comp_emb, baseline_trained_emb],\n",
    "'labels' : [\"Baseline (trained)\", \"Baseline (original)\"]},\n",
    "# Infused comparison between two runs\n",
    "{'name' : 'infused_comp', 'embeddings' : [infused_trained_comp_emb, infused_trained_emb],\n",
    "'labels' : [\"Infused (trained)\", \"Infused (original)\"]},\n",
    "# Baseline Fixed comparison between two runs\n",
    "{'name' : 'baseline_fixed_comp', 'embeddings' : [baseline_fixed_comp_emb, baseline_fixed_emb],\n",
    "'labels' : [\"Baseline (fixed)\", \"Baseline (original)\"]},\n",
    "# Add more comparisons as needed\n",
    "]   \n",
    "\n",
    "\n",
    "# Prepare the data for t-SNE\n",
    "for tsne_target in all_tsne_embeddings:\n",
    "    tsne_target['embeddings'], tsne_target['labels'] = prepare_tsne_data(\n",
    "        tsne_target['embeddings'], tsne_target['labels'], normalize=NORMALIZE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39feaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- t-SNE analysis ---\n",
    "# possible tsne_target_ids: baseline_infused, infused, baseline, base, GlyLM, all \n",
    "# Comparison between runs of the same model: baseline_comp, infused_comp, baseline_fixed_comp\n",
    "\n",
    "tsne_target_id = \"baseline\"\n",
    "for tsne_target in all_tsne_embeddings:\n",
    "    if tsne_target['name'] == tsne_target_id:\n",
    "        tsne_embeddings = tsne_target['embeddings']\n",
    "        tsne_labels = tsne_target['labels']\n",
    "        break\n",
    "else:\n",
    "    raise ValueError(f\"Target '{tsne_target}' not found in the list of embeddings.\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components=2, \n",
    "            random_state=BASE_RANDOM_STATE, \n",
    "            perplexity=30, \n",
    "            learning_rate=200,\n",
    "            max_iter=1000)\n",
    "# Experiment with perplexity, learning_rate, and n_iter later for optimal visualization.\n",
    "# (create a function to do this automatically)\n",
    "\n",
    "# Initialize the t-SNE model and fit it to the embeddings\n",
    "tsne_coords = tsne.fit_transform(tsne_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9fcf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot t-SNE\n",
    "\n",
    "# Generate a DataFrame for the t-SNE coordinates and labels\n",
    "plot_df = pd.DataFrame(tsne_coords, columns=['x', 'y'])\n",
    "plot_df['Embedding Source'] = tsne_labels\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8)) # Adjust figure size as needed\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='Embedding Source', # Color points by their source (Trained Baseline, Raw GlyLM, Trained Infused)\n",
    "    palette='tab10', # Choose a color palette (e.g., 'viridis', 'tab10', 'Paired')\n",
    "    data=plot_df,\n",
    "    legend='full',\n",
    "    alpha=0.7, # Transparency for overlapping points\n",
    "    s=5 # Size of points\n",
    ")\n",
    "plt.title('t-SNE Visualization of Glycan Embeddings')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.grid(True, linestyle='--', alpha=0.6) # Add a subtle grid\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweetnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
