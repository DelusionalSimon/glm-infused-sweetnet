# Prework
- Deep delve
- Lipophilicity GNN project to familiarize myself
- Planned project
  - Started out in more complex project planning system
  - Defined scope
  - Broke down into milestones
  - Moved to simplified PDD
- Planned first prototype
# Prototype 0 Basic sweetnet
- Registered on Github
- Forked glycowork
- Made a new branch of dev branch
- Installed Git
- Local clone set up
- Installed VS Code to work in
- Installed Miniconda for virtual environment
- editable install setup for glycowork
- Set up second repo for my experimentation to live in
- Couldn't get basic stuff to work 
- Google colab tests as sanity check
- About to skip editable install
- Caught an error in my install.
- I did a fresh conda environment and got it working
- Set up pre-trained SweetNet
- imported data to test it
- Started writing evaluator
- Couldn't figure out what data it expected
- Found code snippet to train a quick SweetNet
- Couln't get it to work due to auroc errors
- tried to fix the bug
- managed to train the model nicely!
- Found that my fix just made auroc Nan
- Reverted back using git
- created a divergence by mistake
- Fixed with force. will use revert rather than reset in future
- First sweetnet is up and running and working well. its accuracy matches the values found in the sweetnet paper
- Prototype 0 is done, time to define prototype 1
# Prototype 1 GLM-Infused Sweetnet
- Set up Jupyter notebook structure 
- Got Embedding data imported
- Through exploration of embeddings
- Loaded glyconet library
- They correspond one to one!
- I successfully ran the SweetNet, init_weights, and Prep_model code from cells, and the  training worked
- I split the kingdom model running code into two cells to not have to load the data each time
- tried to fix auroc again, for some reaon my commenting out broke the graphing of training. 
- Abandoned that again
- I seem to get single-class batches in my training and validation data (bug uncovered while tinkering with auroc code) This may impact the validity of my metrics, I would have to edit the 
- Added code to model_training.py to detect single-class batches
- 2 single batches in ten epochs. 10 in 100, 9 where batch label 1 one batch label 2 Amoebozoa': 0, 'Animalia': 1, 'Bacteria': 2,
- Doesn't seem to matter that much, leaving it behind for now
- I've been running SweetNet set on a classification task (assuming multi-class, single-label classification)
- in reality the problem is a multi-class, multi-label task. 
- Number of unique items: 18896, Number of duplicates: 660
- running that as a classification task is no good. data leakage and all kinds of problems
- I will need to work with how I load the data, using gifflar code
- USe gifflar code benchmarks.py and look for other data and code. code to turn pairs into multi-label
- Good thing I caught this now, I was so close to just being done in the wrong way.
- Lets load the data properly using functions in GIFFLAR
- Forked and cloned Gifflar Repo
- Figured out how to run functions from it. 
- Loaded kingdom data using get_data function (took 40 min)
- Loaded data into dataframe
- Melted dataframe into long format and split into glycans and labels using prepare_multilabel
- Made into graphs and loaded into dataloader
- Made my own split function and used the split_data_to_train for hyperefficent loading
- Tested the split function, it doesn't work as expected
- Built a new cleaner split pipeline
- Validation still didn't work
- Turns out the validator wasn't working as expected, it does work, and probably did with the old split.
- Validated graphs from dataloaders
- Managed to run training on a multi-label multi-class sweetnet, and the accuracy is the same as the old flawed method!
- Now I can tackle the embeddings. (the new data loading pipeline will make it quite easy to use otehr data for my final paper) 
- First tried to edit the forward function of Sweetnet, 
- Changed my mind and did a function to add the embeddings directly to the dataloaders
- Trying to get it to work
- Editing training_model to pipe in data from loaders correctly
- The model trains (super slowly (6 hours vs 15 minutes)) 15 times slower
- And with very low accuracy 25 epochs in an an lrap of 0.0012 after 45 minutes
- Abortimng and trying again with a learning rate of 0.005 rather than 0.0005
- LRAP seems to go up slightly quicker
- Training LRAP stalled quickly, going to 0.01
- I've created fixed embeddings which is a way to do it, but it doesn't seem that effective
- Fixed Sweetnet and Prep_model to work with no infused embeddings as well
- Running the model without embeddings takes just as long, so I must hjave broken something in train_model
-   -------------------------------Monday meeting 2025-04-28--------------------------------------------
- The horrendous speed was due to unneccecary to.device calls when adding the embeddings in the training loop
- Now to find why the glm-infused model doesn't converge (which should still be an issue)
- Looking cloesly at the training reports there seems to be an issue with the final reported LRAP value, not being find in any epoch report.
- Trying to debug I found that all embedding vectors are the same, no wonder the system behaves badly, especially since they are fixed (which may be a good idea, because otherwise they would have changed and probably made things look better than they are)
- Couldn't figure out how my embeddings to data function was loading the same embedding into all vectors eventually I found that the embedding file was filled with just one value, roman seem to have messed up a bit.
- I only ever looked at the keeys when exploring the data of the embeddings, I should have looked at more of the embeddings themselves
- While I wait on new embeddings from roman I'll refactor the code and generalize it to work with other kinds of data to solve other problems. 
- made a pipeline to load glycowork datasets and splitting them into a chosen category
- created multilabel_split function to take glycans and labels and splitting them using StratifiedShuffleSplit
- Now I need to remove rare classes
- Rebuilt data loading pipeline into a function that also does the filtering
- Explored different settings for the min_class_size until I found 6 to be the minimum that would lead to successful stratification
- transformed into graphs and Loaded into dataloaders
- Ran successful training'
- Can't get new embedding data since the models and data were stored on Saarbruchen university servers which were struck by a big power outage
- Explored different batch sizes but although I gained some speed with larger batches performance they didn't train as well.
- After restarting the kernel I found higher batch sizes to be effective, but they didn't really give better performance after 128 so I stuck there
- Created a Skeleton for the paper of my thesis.
- Ran a training cycle for disease_association data, got quite good LRAP values
- Edited build_multilabel_dataset to output label names as well, to enable me to descifer label associations later
- My filter doesn't remove the labels that have no members, that could pose a problem
- Removed those columns, in the disease data 60 was cut down to 18 which is significant, and may have effected my results (and perhaps be why my LRAPs from epochs were weird)
- Ran training and my results were better!
- Modified prepare_multilabel in glycowork to sort the class_list to ensure reproducerability
- Let's rewrite the way I handle the embeddings in the more standard way within Sweetnet Itself to enable trainable embeddings and other possible edits later
- Copied glycowork functions again to work on integrating embeddings there (to keep old fucntionality)
- Rolled back SweetNet to the base from glycowork, I can handle the embeddings in the prep_model function
- Got a bit of the way on a new prep_infused_sweetnet function that I will use rather than modifying prep_model
- Added a nice docstring
- Changed all of my docstrings to NumPy/SciPy foprmat from google 
- Changed the standard docstring format to make the defaults clearer
- Added behaviour to use one-hot embeddings in prep_infused_sweetnet as well
- Changed flags and logic for better flow and clarity with the addition of one-hot encodings
- Reordered parameters so that prep_infused_sweetnet(15, embeddings) works
- Rather than using the more standard way to handle OOV by setting the glycowords not in the dictionary to zero, I will initialize all to random variables and only replace the ones I have embeddings for, to keep the comparison as clean as possible (I don't want the zero vectors to negatively influsence my results)
- all I did was perhaps make the index 0 random instead of 0. This would have been good to know when discussing the virtues of OOV handling. I guess the function is more robust now and can handle embedding files that are smaller than the lib
- Random was easy to do
- One-hot seems flawed, since it needs to have the same hidden dimension as there are members in the library, will need to squeeze dimensions or make hidden dimensions match the lkibrary, although I don't know if that will make a fair comparison
- Got good embeddings from roman and ran a static round of training using my old pipeline, which whowed comparable results to the baseline even without learning
- With the new pipeline the results are weird, I'll have to explore further another day.
  -------------------------------Monday meeting 2025-05-05--------------------------------------------
- I think I may be able to use the new pipeline as a general way to try infusion with different gnn models
- I've been running some more training runs today but the results seem weird. I get really good results with non-trainable random embeddings. Perhaps the variation when using the smaller dataset is a bit too big. perhaps it might be a good idea to try out the kingdom prediction task again as those results seemed more stable? (although they take an order of magnitude longer to complete)
- Since my pipeline is quite stable now, perhaps I should iterate and make a new jupyter notebook optimized for evaluation (moving my functions to an utils.py, and building a framework to run several trials in a row)
- The new embedding pipeline is around twise as effective. the new prep funciton seeems more effective than the base one as well. 
- Briefly trying to fix LRAP 
- Got lost getting nowhere. 
- Reverting course
- I need to run several trials and run statistics to evaluate the core hypothesis
- Setting up new optimized Notebook to handle this evaluation: Infusion_Evaluation_System
- Moved my custom functions to utils.py file
- Perhaps I'll make automated functions to run several training runs and average the stats
- Skeleton of Infusion_Evaluation_System set up and tested. 
- before iterating the whole evaluation system, lets just run ten trials manually and use excel to get some averages. using the same split for each run of three (random, infused with training, and infused without training)
- basic system set up with data saving into a pickle file
- Lets do the run (starting at 0722) (ended around 8)
- Test: Ran five trials and tabulated the results. Infusion is not effective. 
- working on some quick hyperparameter optimization
- ran a trial of several different learning rate, and at 0.05 I might get better results than the base model!
- The baseline also benifits from a higher learning rate (might be due to the smallness of the dataset)
- I shouldn't try to meddle with the model until I get the results I want, the fact that the infused model performs worse is also a result. 
- I've tested my hypothesis. I have a basic thesis that I could write. I still have quite a bit of time left, what more could I experiment with?
- let's set up automated evaluation system, it will save me time later
- Built a seed_everything function for true reproducability
- Moved to utils.py file
- Set up basic Experiment Loop, let's see if it runs
- It seems to work! 
- Tomorrow I'll have to implement a way to store the data (and perhaps models)
- Then I'll add some analytics (deep dive into pandas)
- Did a quick sanity check, making sure the embeddings match the original dictionary
- Cleaned up glycowork repo, removing unneeded modifications and keeping good updates using cherry picking
- Back to the experiment loop
- Feat: epoch-wise metrics and summary results collected and saved
- Let's do a proper run over breakfast
- Experiment with ten trials and four configurations took 25 minutes
- Running long test with Kingdom prediction
- Had meeting with Roman, learnt a lot. 
- My testing strategy isn't really valid, I would be polluting my models. the test set should only be used once at the end on the best model to test its performance, I don't think I can even use it between models?
- Kingdom prediction task gives accuracy much closer to random model, perhaps the small dataset was an issue
- I ran a trial with tissue data and it didn't even seem to reach its max within 100 epochs, trying 200 epochs now
- Added patiance as global parameter in automated evaluation system
- I Think I'll try to implement a hyperloop around the experiment loop foir basic hyperparameter exploration
- I should split off the test portion before running the loop, (add a flag to only split into )
- Set patience to 25 rather than 50
- Running proper run of Kingdom
- Did basic averages of my runs so far
- Interestingly the traibability of the embeddings mattered very little
- Preenting my work to the Bojar Lab
-------------------------------Monday meeting 2025-05-12--------------------------------------------
- Time to iterate the project using my PDD. 
- PDD used to generate new requirements
- Using Paper writing catech to define paper.
- Spent a day and a half developing a barrel-aging metaphor to describe the infusion process
- Spent more time on the metaphor. unable to let go. lets move on
- Updated multilabel_split to include a flag for 2-way splitting
- Tested updated function
- Interestingly I can set my min_class_size lower (4 instead of 6) in my build_multilabel_dataset function when I use my splitting function twice in 2-way splitting mode rather than once using 3-way split, shouldn't the two be pretty much identical?
- Feat(multilabel_split): When testing 2-way split I found that I could use a lower min_class size  (4 vs. 6) in build_multilabel_dataset compared ton when using 3 way split. I'm changing the internal ordering of the splits to mirror the 2-way split behaviour.
- Feat(multilabel_split): Fixed how the logic deals with the ratios of the splits. For some arcane reason the split works with num_classes as low as 2 which should be impossible for a 3-way split. SSS may be doing some weird stuff.
- Interestingly, setting the train_size to something lower like 0.5 makes the function require a higher min_class_size
- Break point is below 0.593
- Now I am able to train with more data!
- Used the updated multilabel_split to iterate the Experimental loop thing. Now it splits and saves the test dataloader before the experiment
- Feat: Now saves state dictionaries of the best models from each training run
- I Think I need to restructure my folder structure next, it is getting messy
- But first I'll jump down a bit in the requirements list and create the hyperloop structure to make fully automated data aquisition possible
- The whole thing grew in my head as I made the last couple of changes
- Hyperautomatic Barrel Batching system started. put in a new notebook to not overcomplicate things for people that try to use my stuff
- I wonder why my projects always seem to end with loops within loops within loops until my mind starts to break. Why do all my projects somehow converge on that?
- I kinda like that feeling of being on the verge of infinity.
- Hyperautomatic system done!
-------------------------------Monday meeting 2025-05-19--------------------------------------------
- Updated environment file just to make sure before setting up desktop clone to run automated experiments while I finalize the project.
- Got system cloned to desktop
- added thing to save the experimental settings to YAML file
- Changed code to save all data to specific data dir to not clutter main folder
- cleaned up main folder by moving all old shit to old_data dir
- Setup to redo all old experiments using hyperautomation
- 3 full experiments for tissue, kingdom, and disease (120 training runs) done in 110 minutes
- Improved hyperautomation loop to make the progress easier to follow. 
  - silenced train_model
  - added silent flags to all utils functions
  - silenced sigmoid errors
  - added tqdm.auto progress bars
- Set up big round of hyperoptimization experiments
- Added a time metric to keep track of how long each model takes to train, could be interesting
- Noticed synching to github took a long time. All of the data I generate takes up a lot of space, moved the saving location to an untracked location
- implemented and tested function to load embeddings from the state dictionaries that I save in my experiment data. 
- Feat: Added get_embeddings_from_state_dict function to utils.py and improved docstring and flow
- Added a thing to compare euclidian distances on a whim (that annoying copilot autocomplete thing suggested it below code I was writing and I got intrigued and let it finish the code. after editing the code to be actually useful I did some research. seems like this analysis could actually be useful for my project).
- Looped that other thing and calculated statistically relevant values.
- Quickly added a thing to remove most trailing decemals
-  I added a thing to my hyperautomation system to save all prepped models before training and ran a small experiment. Piping that data into a simplified euclidian distance analysis thing showed me that the distance of a trained baseline model and it's untrained base embeddings were of a similar magnitude as the difference between a trainable and fixed infused model.
-  Roman suggested to divide every embedding by the L2-norm of the longest embedding vector. This brought down the range and changed some results 
   -  Some embeddings may be more zoomed in than others
   -  It is interesting that the values comparing different baseline runs now has a higher value than the comparison between the baseline and infused models.
-  Made my pickle loading thing into a function
-   Iterated my HBBS to use the new pickle loader within the main loop to enable me to sequentially use different embeddings as a new parameter. to generate data for different embeddings
- I'm about to rerun all base experiments using the 5 new embeddings
- Got a bunch of "[glycoword] is not in library, keeping its initial random value."... prints
  - Never silenced that when I addewd the silent flag, which was very good, otherwise I wouldn't have caught that the new embeddings lack 143 glycowords
  - Also very good that I added the thing to make sure that my system would work with embedding files that don't match the library on a flawed assumption
  - Most of those seem like they could be removed from the main library 
  - Made the t-SNE thing into a function
  -  Set up all different t-SNEs I might want to do on a given run
  -  Feat: Iterated preparation system for t_SNE using a list of dicts for ease of automation later
  -  Feat: load the embeddings and labels for a given t-SNE target
  -  Added density plot to t-SNE part of Statistics Silo
  -  Time To tackle the rest of the analysis. Time is running out
  -  added a cell to load all of the summary statistics from an experiment
  -  Seems like t-tests will be good enough for my base comparisons
     -  Might do effect size measures (Cohen's d) as well
  -  Made a little thing similar to what I do in the HBBS and t-SNE to define my sets of experiments
  -  Let's make a thing to aggregate all of the data in the set of experiments. 
  -  First I did a thing to quickly compare experiments. 
     -  Embedding 1 is the best. Running that going forward.
  -  Now lets save the data I need for the tables
  -  Found a core error in my HBBS loop that doesn't reset the variable that I use to save the summary data, it accumulates between experiments. 
  -  I see now. I'm actually initializing the dictionaries outside of the experiment loop, they just get set once at the start of the cell. this is why my csvs are aggregating all values. all of the data I have captured thus far is fucked. Good thing everything is so automated
  -  Fixed bug and briefly experimented with paralellization to speed up things. it did not.
  - I'll have to build the rest of my statistical pipeline on fucked data. 
  - MAde a thing to generate the tables from one experiment for one metric.
  - Tried my statistic tools on a fresh batch of data and discovered a fatal bug, I never piped in what dataset to run the HBBS on. I'äve just ran disease the last hundrreds of experiments. 
  - Fixed that bug but found some more. 
    - Settings used in one experiment but not another overrode the base settings
    - I figured out why I wasn't able to incrrease the num_workers, can't save dataloaders made with num_workers>0
    - Tried that again but need to do too much work to only perhaps get a performance increase. lets leave that behind.
  - Performance is way worse than expected, I ran the smallest dataset on all runs. I won't have time to run all possible experiments. 
  - Finally finalized test_model function4
  - moved to utils
  - Feat (test_model): Iterated test_model function to calculate its own criterion to simplify use. 
  - roughly made a script to instantiate the best model of an experiment and running its test set through it using test_model. 
  - Added basic way to   show common glycowords on t-SNE plot
  - Ran some quick experiments to verify a difference I saw when I lowered the min_class size after my splitting optimization
    - There was a significand improvement in all
    - but this would lower generalizability or coverage, I'm basically reducing the complexity of the task. 
  - got my small fix to glycoworks pushed to the origin!
  - This insures that my code can run without a local install. 
  - Now I need to focus hard oin finishing the paper, just a couple of days left
  - I've been looking at kingdom runs when doing t-sne and euclidian distance analysis, looking at another task, the difference is striking, the embeddings change much more. The kingdom task bottoms out too quickly
  - 

  
