# Prework
- Deep delve
- Lipophilicity GNN project to familiarize myself
- Planned project
  - Started out in more complex project planning system
  - Defined scope
  - Broke down into milestones
  - Moved to simplified PDD
- Planned first prototype
# Prototype 0 Basic sweetnet
- Registered on Github
- Forked glycowork
- Made a new branch of dev branch
- Installed Git
- Local clone set up
- Installed VS Code to work in
- Installed Miniconda for virtual environment
- editable install setup for glycowork
- Set up second repo for my experimentation to live in
- Couldn't get basic stuff to work 
- Google colab tests as sanity check
- About to skip editable install
- Caught an error in my install.
- I did a fresh conda environment and got it working
- Set up pre-trained SweetNet
- imported data to test it
- Started writing evaluator
- Couldn't figure out what data it expected
- Found code snippet to train a quick SweetNet
- Couln't get it to work due to auroc errors
- tried to fix the bug
- managed to train the model nicely!
- Found that my fix just made auroc Nan
- Reverted back using git
- created a divergence by mistake
- Fixed with force. will use revert rather than reset in future
- First sweetnet is up and running and working well. its accuracy matches the values found in the sweetnet paper
- Prototype 0 is done, time to define prototype 1
# Prototype 1 GLM-Infused Sweetnet
- Set up Jupyter notebook structure 
- Got Embedding data imported
- Through exploration of embeddings
- Loaded glyconet library
- They correspond one to one!
- I successfully ran the SweetNet, init_weights, and Prep_model code from cells, and the  training worked
- I split the kingdom model running code into two cells to not have to load the data each time
- tried to fix auroc again, for some reaon my commenting out broke the graphing of training. 
- Abandoned that again
- I seem to get single-class batches in my training and validation data (bug uncovered while tinkering with auroc code) This may impact the validity of my metrics, I would have to edit the 
- Added code to model_training.py to detect single-class batches
- 2 single batches in ten epochs. 10 in 100, 9 where batch label 1 one batch label 2 Amoebozoa': 0, 'Animalia': 1, 'Bacteria': 2,
- Doesn't seem to matter that much, leaving it behind for now
- I've been running SweetNet set on a classification task (assuming multi-class, single-label classification)
- in reality the problem is a multi-class, multi-label task. 
- Number of unique items: 18896, Number of duplicates: 660
- running that as a classification task is no good. data leakage and all kinds of problems
- I will need to work with how I load the data, using gifflar code
- USe gifflar code benchmarks.py and look for other data and code. code to turn pairs into multi-label
- Good thing I caught this now, I was so close to just being done in the wrong way.
- Lets load the data properly using functions in GIFFLAR
- Forked and cloned Gifflar Repo
- Figured out how to run functions from it. 
- Loaded kingdom data using get_data function (took 40 min)
- Loaded data into dataframe
- Melted dataframe into long format and split into glycans and labels using prepare_multilabel
- Made into graphs and loaded into dataloader
- Made my own split function and used the split_data_to_train for hyperefficent loading
- Tested the split function, it doesn't work as expected
- Built a new cleaner split pipeline
- Validation still didn't work
- Turns out the validator wasn't working as expected, it does work, and probably did with the old split.
- Validated graphs from dataloaders
- Managed to run training on a multi-label multi-class sweetnet, and the accuracy is the same as the old flawed method!
- Now I can tackle the embeddings. (the new data loading pipeline will make it quite easy to use otehr data for my final paper) 
- First tried to edit the forward function of Sweetnet, 
- Changed my mind and did a function to add the embeddings directly to the dataloaders
- Trying to get it to work
- Editing training_model to pipe in data from loaders correctly
- The model trains (super slowly (6 hours vs 15 minutes)) 15 times slower
- And with very low accuracy 25 epochs in an an lrap of 0.0012 after 45 minutes
- Abortimng and trying again with a learning rate of 0.005 rather than 0.0005
- LRAP seems to go up slightly quicker
- Training LRAP stalled quickly, going to 0.01
- I've created fixed embeddings which is a way to do it, but it doesn't seem that effective
- Fixed Sweetnet and Prep_model to work with no infused embeddings as well
- Running the model without embeddings takes just as long, so I must hjave broken something in train_model
- The horrendous speed was due to unneccecary to.device calls when adding the embeddings in the training loop
- Now to find why the glm-infused model doesn't converge (which should still be an issue)
- Looking cloesly at the training reports there seems to be an issue with the final reported LRAP value, not being find in any epoch report.
- Trying to debug I found that all embedding vectors are the same, no wonder the system behaves badly, especially since they are fixed (which may be a good idea, because otherwise they would have changed and probably made things look better than they are)
- Couldn't figure out how my embeddings to data function was loading the same embedding into all vectors eventually I found that the embedding file was filled with just one value, roman seem to have messed up a bit.
- I only ever looked at the keeys when exploring the data of the embeddings, I should have looked at more of the embeddings themselves
- While I wait on new embeddings from roman I'll refactor the code and generalize it to work with other kinds of data to solve other problems. 
- made a pipeline to load glycowork datasets and splitting them into a chosen category
- created multilabel_split function to take glycans and labels and splitting them using StratifiedShuffleSplit
- Now I need to remove rare classes
- Rebuilt data loading pipeline into a function that also does the filtering
- Explored different settings for the min_class_size until I found 6 to be the minimum that would lead to successful stratification
- transformed into graphs and Loaded into dataloaders
- Ran successful training'
- Can't get new embedding data since the models and data were stored on Saarbruchen university servers which were struck by a big power outage
- Explored different batch sizes but although I gained some speed with larger batches performance they didn't train as well.
- After restarting the kernel I found higher batch sizes to be effective, but they didn't really give better performance after 128 so I stuck there
- Created a Skeleton for the paper of my thesis.
- Ran a training cycle for disease_association data, got quite good LRAP values
- Edited build_multilabel_dataset to output label names as well, to enable me to descifer label associations later
- My filter doesn't remove the labels that have no members, that could pose a problem
- Removed those columns, in the disease data 60 was cut down to 18 which is significant, and may have effected my results (and perhaps be why my LRAPs from epochs were weird)
- Ran training and my results were better!
- Modified prepare_multilabel in glycowork to sort the class_list to ensure reproducerability
- Let's rewrite the way I handle the embeddings in the more standard way within Sweetnet Itself to enable trainable embeddings and other possible edits later
- Copied glycowork functions again to work on integrating embeddings there (to keep old fucntionality)
- Rolled back SweetNet to the base from glycowork, I can handle the embeddings in the prep_model function
- Got a bit of the way on a new prep_infused_sweetnet function that I will use rather than modifying prep_model
- Added a nice docstring
- Changed all of my docstrings to NumPy/SciPy foprmat from google 
- Changed the standard docstring format to make the defaults clearer
- Added behaviour to use one-hot embeddings in prep_infused_sweetnet as well
- Changed flags and logic for better flow and clarity with the addition of one-hot encodings
- Reordered parameters so that prep_infused_sweetnet(15, embeddings) works
- 

  
