# Prework
- Deep delve
- Lipophilicity GNN project to familiarize myself
- Planned project
  - Started out in more complex project planning system
  - Defined scope
  - Broke down into milestones
  - Moved to simplified PDD
- Planned first prototype
# Prototype 0 Basic sweetnet
- Registered on Github
- Forked glycowork
- Made a new branch of dev branch
- Installed Git
- Local clone set up
- Installed VS Code to work in
- Installed Miniconda for virtual environment
- editable install setup for glycowork
- Set up second repo for my experimentation to live in
- Couldn't get basic stuff to work 
- Google colab tests as sanity check
- About to skip editable install
- Caught an error in my install.
- I did a fresh conda environment and got it working
- Set up pre-trained SweetNet
- imported data to test it
- Started writing evaluator
- Couldn't figure out what data it expected
- Found code snippet to train a quick SweetNet
- Couln't get it to work due to auroc errors
- tried to fix the bug
- managed to train the model nicely!
- Found that my fix just made auroc Nan
- Reverted back using git
- created a divergence by mistake
- Fixed with force. will use revert rather than reset in future
- First sweetnet is up and running and working well. its accuracy matches the values found in the sweetnet paper
- Prototype 0 is done, time to define prototype 1
# Prototype 1 GLM-Infused Sweetnet
- Set up Jupyter notebook structure 
- Got Embedding data imported
- Through exploration of embeddings
- Loaded glyconet library
- They correspond one to one!
- I successfully ran the SweetNet, init_weights, and Prep_model code from cells, and the  training worked
- I split the kingdom model running code into two cells to not have to load the data each time
- tried to fix auroc again, for some reaon my commenting out broke the graphing of training. 
- Abandoned that again
- I seem to get single-class batches in my training and validation data (bug uncovered while tinkering with auroc code) This may impact the validity of my metrics, I would have to edit the 
- Added code to model_training.py to detect single-class batches
- 2 single batches in ten epochs. 10 in 100, 9 where batch label 1 one batch label 2 Amoebozoa': 0, 'Animalia': 1, 'Bacteria': 2,
- Doesn't seem to matter that much, leaving it behind for now
- I've been running SweetNet set on a classification task (assuming multi-class, single-label classification)
- in reality the problem is a multi-class, multi-label task. 
- Number of unique items: 18896, Number of duplicates: 660
- running that as a classification task is no good. data leakage and all kinds of problems
- I will need to work with how I load the data, using gifflar code
- USe gifflar code benchmarks.py and look for other data and code. code to turn pairs into multi-label
- Good thing I caught this now, I was so close to just being done in the wrong way.
- Lets load the data properly using functions in GIFFLAR
- Forked and cloned Gifflar Repo
- Figured out how to run functions from it. 
- Loaded kingdom data using get_data function (took 40 min)
- Loaded data into dataframe
- Melted dataframe into long format and split into glycans and labels using prepare_multilabel
- Made into graphs and loaded into dataloader
- Made my own split function and used the split_data_to_train for hyperefficent loading
- Tested the split function, it doesn't work as expected
- Built a new cleaner split pipeline
- Validation still didn't work
- Turns out the validator wasn't working as expected, it does work, and probably did with the old split.
- Validated graphs from dataloaders
- 