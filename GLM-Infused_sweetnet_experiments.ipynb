{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2416a1b",
   "metadata": {},
   "source": [
    "# GLM-Infused SweetNet Experiments\n",
    "\n",
    "Experimenting with a modified version of SweetNet that allows it to take pre-trained embeddings as input. To get there I need a way to take the embeddings Iâ€™ve gotten from roman and transform them into nice inputs for the model, and a way to set the initial features using these inputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652c8f8",
   "metadata": {},
   "source": [
    "## Importing and exploring the GLM Embedding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0485fe3",
   "metadata": {},
   "source": [
    "### Exploring embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick thing to load a pickle file\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'Embeddings/embeddings_model1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            loaded_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do some quick exploration\n",
    "\n",
    "# --- Explore the loaded data ---\n",
    "print(f\"Type of loaded object: {type(loaded_embeddings)}\")\n",
    "\n",
    "# Common formats for embeddings: dictionary or numpy array\n",
    "if isinstance(loaded_embeddings, dict):\n",
    "    print(f\"Number of items (if dictionary): {len(loaded_embeddings)}\")\n",
    "    # print some keys to see what they look like\n",
    "    print(f\"Example keys (first 5): {list(loaded_embeddings.keys())[:5]}\")\n",
    "elif hasattr(loaded_embeddings, 'shape'):\n",
    "    print(f\"Shape (if array/tensor): {loaded_embeddings.shape}\")\n",
    "    if hasattr(loaded_embeddings, 'dtype'):\n",
    "         print(f\"Data type (if array/tensor): {loaded_embeddings.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e6aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(loaded_embeddings.keys())[5:30]) # Print more keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_key = '1,4-Anhydro-Gal-ol' \n",
    "if example_key in loaded_embeddings:\n",
    "    embedding_vector = loaded_embeddings[example_key]\n",
    "    print(f\"Type of value for '{example_key}': {type(embedding_vector)}\")\n",
    "    if hasattr(embedding_vector, 'shape'):\n",
    "        print(f\"Shape of value: {embedding_vector.shape}\") # This gives dimensionality!\n",
    "        print(f\"Dtype of value: {embedding_vector.dtype}\")\n",
    "    print(embedding_vector) # Print the vector itself if it's not too long\n",
    "else:\n",
    "    print(f\"Key '{example_key}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the keys a bit more closely\n",
    "\n",
    "import collections\n",
    "\n",
    "key_types = collections.defaultdict(int)\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        key_types['linkage_or_modification'] += 1\n",
    "    elif key[0].isalpha():\n",
    "        key_types['monosaccharide'] += 1\n",
    "    else:\n",
    "        key_types['other'] += 1\n",
    "\n",
    "print(key_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore those Other keys \n",
    "\n",
    "other_keys = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        other_keys.append(key)\n",
    "\n",
    "print(f\"Number of 'other' keys: {len(other_keys)}\")\n",
    "print(f\"Examples of 'other' keys: {other_keys[:20]}\") # Print the first 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398bd9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at 50 more keys\n",
    "\n",
    "print(f\"More Examples of 'other' keys: {other_keys[20:70]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore those monosaccharide keys\n",
    "monosaccharide = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        monosaccharide.append(key)\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'monosaccharide' keys: {len(monosaccharide)}\")\n",
    "print(f\"Examples of 'monosaccharide' keys: {monosaccharide[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ddd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be throughough, let's look at 50 Linkage or Modification keys as well\n",
    "linkage_or_modification = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        linkage_or_modification.append(key)\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'linkage_or_modification' keys: {len(linkage_or_modification)}\")\n",
    "print(f\"Examples of 'linkage_or_modification' keys: {linkage_or_modification[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df823d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait, a couple of weeks later and I couldn't figure out why my glm-infused model wasn't converging, \n",
    "# and I just realised that the embeddings were all the same, no wonder it didn't work. \n",
    "# I should have looked at the embeddings themselves, not just the keys.\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_file_path = 'Embeddings/embeddings_model1.pkl'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    loaded_embeddings = pickle.load(f)\n",
    "\n",
    "first_embedding = None\n",
    "all_same = True\n",
    "if loaded_embeddings:\n",
    "    first_key = next(iter(loaded_embeddings))\n",
    "    first_embedding = loaded_embeddings[first_key]\n",
    "    for key, embedding in loaded_embeddings.items():\n",
    "        if not (embedding == first_embedding).all():\n",
    "            all_same = False\n",
    "            print(f\"Found a different embedding for key: {key}\")\n",
    "            break\n",
    "\n",
    "if all_same and first_embedding is not None:\n",
    "    print(\"All embeddings in the loaded dictionary appear to be the same.\")\n",
    "elif first_embedding is None:\n",
    "    print(\"The embedding dictionary is empty.\")\n",
    "else:\n",
    "    print(\"Embeddings in the dictionary are not all the same.\")\n",
    "\n",
    "print(f\"Number of embeddings in the dictionary: {len(loaded_embeddings)}\")\n",
    "if first_embedding is not None:\n",
    "    print(f\"First embedding:\")\n",
    "    print(first_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9ad5a",
   "metadata": {},
   "source": [
    "### Load the glycowork libr\n",
    "\n",
    "I'll load the glycowork library and compare it to the keys in the embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b14506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glycowork.glycan_data import loader\n",
    "\n",
    "glycowork_vocabulary = loader.lib\n",
    "\n",
    "print(f\"Number of items in glycowork vocabulary: {len(glycowork_vocabulary)}\")\n",
    "print(f\"Example keys from glycowork vocabulary (first 20): {list(glycowork_vocabulary.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e1f3d",
   "metadata": {},
   "source": [
    "Nice, they seem to correspond one to one!\n",
    "\n",
    "That saves me a lot of work down the line (Thanks Roman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at one of the keys in the glycowork vocabulary to see what they return\n",
    "example_glycowork_key = '-10'\n",
    "if example_glycowork_key in glycowork_vocabulary:\n",
    "    glycowork_value = glycowork_vocabulary[example_glycowork_key]\n",
    "    print(f\"Type of value for '{example_glycowork_key}': {type(glycowork_value)}\")\n",
    "    print(glycowork_value)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f827a0",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54413ae2",
   "metadata": {},
   "source": [
    "### Load, filter, and transform glycowork data into glycan_loaders ||||run on kernel restart||||\n",
    "This is used to load and filter glycowork data for a specific prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b4d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the full sugarbase dataset and make a custom dataframe for the problem you want to tackle\n",
    "\n",
    "from glycowork.glycan_data.loader import build_custom_df, df_glycan\n",
    "from glycowork.ml.train_test_split import prepare_multilabel\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "def build_multilabel_dataset(glycan_dataset: str = 'df_species',\n",
    "                          glycan_class: str = 'Kingdom',\n",
    "                          min_class_size: int = 6) -> Tuple[List[str], List[List[float]], List[str]]:\n",
    "    \"\"\"\n",
    "    Loads glycan data, prepares it for multi-label classification, and filters it.\n",
    "\n",
    "    Removes glycans with rare label combinations and filters out individual\n",
    "    labels that have no positive examples in the remaining glycans.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    glycan_dataset : str, optional, default = 'df_species'\n",
    "        The glycowork dataset to use. Options include:\n",
    "        - 'df_species'\n",
    "        - 'df_tissue'\n",
    "        - 'df_disease'\n",
    "    glycan_class : str, optional, default = 'Kingdom'\n",
    "        The class to predict from the chosen dataset. Options depend on\n",
    "        `glycan_dataset`:\n",
    "        - 'df_species': 'Species', 'Genus', 'Family', 'Order', 'Class', 'Phylum', 'Kingdom', 'Domain', 'ref'\n",
    "        - 'df_tissue': 'tissue_sample', 'tissue_species', 'tissue_id', 'tissue_ref'\n",
    "        - 'df_disease': 'disease_association', 'disease_sample', 'disease_direction', 'disease_species', 'disease_id', 'disease_ref'\n",
    "    min_class_size : int, optional, default = 6\n",
    "        Minimum number of samples required for a specific multi-label combination\n",
    "        to be included. Set to 1 to include all combinations. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[str], List[List[float]], List[str]]\n",
    "        A tuple containing:\n",
    "        - glycan_sequences (List[str]): List of glycan strings after filtering.\n",
    "        - binary_labels_filtered (List[List[float]]): List of corresponding\n",
    "          multi-label binary vectors with columns for inactive labels removed.\n",
    "        - label_names_filtered (List[str]): The ordered list of names for each\n",
    "          position in the binary vectors, containing only labels with at\n",
    "          least one positive example.\n",
    "\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    all_glycan_data = df_glycan\n",
    "\n",
    "    # Build custom dataframe\n",
    "    custom_glycan_df = build_custom_df(all_glycan_data, glycan_dataset)\n",
    "\n",
    "    # Extract the list of unique individual labels from the chosen class from the custom_glycan_df\n",
    "    # These are used to dechipher the labels when the model is used for prediction\n",
    "    all_possible_label_names = sorted(list(custom_glycan_df[glycan_class].unique()))\n",
    "    print(f\"Found {len(all_possible_label_names)} unique individual classes/labels.\")\n",
    "\n",
    "\n",
    "    # Prepare for multi-label prediction\n",
    "    glycans, labels = prepare_multilabel(custom_glycan_df, glycan_class)\n",
    "\n",
    "    # if needed, removes classes with fewer than min_class_size samples.\n",
    "    if(min_class_size > 1):\n",
    "\n",
    "        # Convert labels to string representation for counting\n",
    "        label_strings = [''.join(map(str, label)) for label in labels]\n",
    "\n",
    "        # Count occurrences of each label combination\n",
    "        label_counts = Counter(label_strings)\n",
    "\n",
    "        # Filter glycans and labels based on class size\n",
    "        glycan_sequences = [glycans[i] for i, label_str in enumerate(label_strings) if label_counts[label_str] >= min_class_size]\n",
    "        binary_labels_unfiltered = [labels[i] for i, label_str in enumerate(label_strings) if label_counts[label_str] >= min_class_size]\n",
    "        print(f\"Number of unique glycans left after filtering rare classes (size >= {min_class_size}): {len(glycan_sequences)}/{len(glycans)}\")\n",
    "        \n",
    "        # Filter out individual labels with no positive examples after glycan filtering\n",
    "\n",
    "        # Convert binary_labels to numpy array for easier column manipulation\n",
    "        binary_labels_np = np.array(binary_labels_unfiltered)\n",
    "\n",
    "        # Find indices of labels with at least one positive example\n",
    "        # Sum across rows (axis=0) to get count for each label\n",
    "        label_sums = binary_labels_np.sum(axis=0)\n",
    "        active_label_indices = np.where(label_sums > 0)[0]\n",
    "\n",
    "        # Create the final list of label names using the active indices\n",
    "        # Use the initially generated sorted list (all_possible_label_names)\n",
    "        # because its order matches the columns of binary_labels after prepare_multilabel\n",
    "        label_names = [all_possible_label_names[i] for i in active_label_indices]\n",
    "\n",
    "        # Create the final filtered binary label vectors, keeping only the active columns\n",
    "        binary_labels = binary_labels_np[:, active_label_indices].tolist() # Convert back to list of lists\n",
    "\n",
    "        print(f\"Number of unique labels left after filtering: {len(binary_labels[0])}\")\n",
    "\n",
    "    else:\n",
    "        glycan_sequences = glycans\n",
    "        binary_labels = labels\n",
    "        print(f\"Number of unique glycans: {len(glycan_sequences)}\")\n",
    "\n",
    "    return glycan_sequences, binary_labels, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glycans, labels, label_names = build_multilabel_dataset(glycan_dataset='df_disease', glycan_class='disease_association', min_class_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b77c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c440705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick inspection of the data\n",
    "\n",
    "print(f\"Number of unique glycans: {len(glycans)}\")\n",
    "print(f\"Number of label vectors: {len(labels)}\")\n",
    "print(f\"Shape of first label vector (number of members in class): {len(labels[0])}\")\n",
    "print(f\"\\nFirst 5 glycans:\\n{glycans[:5]}\")\n",
    "print(f\"\\nFirst 5 label vectors:\\n{labels[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and testing sets using StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import List, Union\n",
    "\n",
    "def multilabel_split(glycans: List[str], # list of IUPAC-condensed glycans\n",
    "                 labels: List[Union[float, int, str]], # list of prediction labels\n",
    "                 train_size: float = 0.7, # size of train set, the rest is split into validation and test sets\n",
    "                 random_state: int = 42 # random state for reproducibility\n",
    "                )-> Tuple[List[str], List[str], List[str], List[List[float]], List[List[float]], List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and testing sets using StratifiedShuffleSplit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    glycans : List[str]\n",
    "        List of glycan strings (IUPAC-condensed).\n",
    "    labels : List[Union[float, int, str]]\n",
    "        List of label vectors or single labels for stratification. \n",
    "    train_size : float, optional, default = 0.7\n",
    "        Proportion of the dataset to include in the training split. The remaining\n",
    "        data is split equally into validation and test sets\n",
    "    random_state : int, optional, default = 42\n",
    "        Controls the randomness of the split for reproducibility. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[str], List[str], List[str], List[List[float]], List[List[float]], List[List[float]]]\n",
    "        A tuple containing:\n",
    "        - train_glycans (List[str]): Glycans for the training set.\n",
    "        - val_glycans (List[str]): Glycans for the validation set.\n",
    "        - test_glycans (List[str]): Glycans for the testing set.\n",
    "        - train_labels (List[List[float]]): Labels for the training set.\n",
    "        - val_labels (List[List[float]]): Labels for the validation set.\n",
    "        - test_labels (List[List[float]]): Labels for the testing set.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert labels to a suitable format for stratification (string representation)\n",
    "    label_strings = [''.join(map(str, label)) for label in labels]\n",
    "\n",
    "    # Initial split for train vs. (val + test)\n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = 1 - train_size, random_state = random_state)\n",
    "    train_index, temp_index = next(sss.split(glycans, label_strings))\n",
    "    train_glycans = [glycans[i] for i in train_index]\n",
    "    train_labels = [labels[i] for i in train_index]\n",
    "    temp_glycans = [glycans[i] for i in temp_index]\n",
    "    temp_labels = [labels[i] for i in temp_index]\n",
    "\n",
    "    # Split the remaining (val + test) into validation and test sets\n",
    "    sss_val_test = StratifiedShuffleSplit(n_splits = 1, test_size = 0.5, random_state = random_state)\n",
    "    val_index, test_index = next(sss_val_test.split(temp_glycans, [''.join(map(str, label)) for label in temp_labels]))\n",
    "    val_glycans = [temp_glycans[i] for i in val_index]\n",
    "    val_labels = [temp_labels[i] for i in val_index]\n",
    "    test_glycans = [temp_glycans[i] for i in test_index]\n",
    "    test_labels = [temp_labels[i] for i in test_index]\n",
    "\n",
    "    print(\"Split complete!\")\n",
    "    print(f\"Train set size: {len(train_glycans)}\")\n",
    "    print(f\"Validation set size: {len(val_glycans)}\")\n",
    "    print(f\"Test set size: {len(test_glycans)}\")\n",
    "        \n",
    "    return train_glycans, val_glycans, test_glycans, train_labels, val_labels, test_labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use the split function\n",
    "train_glycans, val_glycans, test_glycans, train_labels, val_labels, test_labels = multilabel_split(glycans, labels, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms IUPAC into graphs and makes the data loaders for the training and validation sets\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "\n",
    "glycan_loaders = split_data_to_train(\n",
    "    glycan_list_train = train_glycans,\n",
    "    glycan_list_val = val_glycans,\n",
    "    labels_train = train_labels,\n",
    "    labels_val = val_labels,\n",
    "    batch_size = 128,  # 32 or 128 seem to work well on this system\n",
    "    drop_last = False,\n",
    "    augment_prob = 0.0,  # Adjust if you want augmentation for training\n",
    "    generalization_prob = 0.2  # Adjust if you want generalization for training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae2f77",
   "metadata": {},
   "source": [
    "### GIFFLAR Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a5ce2",
   "metadata": {},
   "source": [
    "#### Load GIFFLAR Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the GIFFLAR dataset for the Taxonomy Kingdom (takes a long time to run(~40m), just use the file it generated)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../GIFFLAR') \n",
    "\n",
    "from gifflar.benchmarks import get_dataset\n",
    "import pathlib\n",
    "\n",
    "data_config_kingdom = {\"name\": \"Taxonomy_Kingdom\"}\n",
    "root_dir = pathlib.Path(\"./data_gifflar\")  # Choose a directory to save the data\n",
    "root_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "kingdom_dataset_config = get_dataset(data_config_kingdom, root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataset configuration\n",
    "print(kingdom_dataset_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a488a",
   "metadata": {},
   "source": [
    "#### Load and transform GIFFLAR Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339c831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load the dataset into a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "filepath = 'data_gifflar/taxonomy_Kingdom.tsv'\n",
    "#filepath = kingdom_dataset_config['filepath'] #If you've loaded it recently, which you shouldn't\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "multilabel_kingdom_df = pd.read_csv(filepath, sep=\"\\t\")\n",
    "# Display the first few rows of the DataFrame\n",
    "print(multilabel_kingdom_df.head())\n",
    "# Display the shape of the DataFrame\n",
    "print(f\"Shape of the DataFrame: {multilabel_kingdom_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b5a99",
   "metadata": {},
   "source": [
    "That looks fine\n",
    "\n",
    "Let's convert them into graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multilabel_kingdom_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ce3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's try the prepare multilabel function from the train_test_split module\n",
    "\n",
    "from glycowork.ml.train_test_split import prepare_multilabel\n",
    "\n",
    "# Prepare the multilabel dataset using the melt function\n",
    "# The melt function is used to transform the DataFrame from wide format to long format\n",
    "kingdom_df_melted = multilabel_kingdom_df.melt(\n",
    "    id_vars=['IUPAC', 'split'],\n",
    "    value_vars=['Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae', 'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae', 'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista', 'Riboviria'],\n",
    "    var_name='Kingdom',\n",
    "    value_name='Association'\n",
    ")\n",
    "\n",
    "# Filter for associations where the glycan belongs to the kingdom (Association == 1)\n",
    "kingdom_df_melted = kingdom_df_melted[kingdom_df_melted['Association'] == 1]\n",
    "\n",
    "# Splitting the dataset using the 'split' column\n",
    "train_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'train'].drop(columns=['split'])\n",
    "val_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'val'].drop(columns=['split'])\n",
    "test_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'test'].drop(columns=['split'])\n",
    "\n",
    "# Finally using the prepare_multilabel function to prepare the data for training\n",
    "glycan_train, label_train = prepare_multilabel(train_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "glycan_val, label_val = prepare_multilabel(val_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "glycan_test, label_test = prepare_multilabel(test_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ad291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make them into graphs again, but hyper efficiently this time\n",
    "\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "\n",
    "multilabel_kingdom_loaders = split_data_to_train(\n",
    "    glycan_list_train=glycan_train,\n",
    "    glycan_list_val=glycan_val,\n",
    "    labels_train=label_train,\n",
    "    labels_val=label_val,\n",
    "    batch_size=32,  # Adjust as needed\n",
    "    drop_last=False,\n",
    "    augment_prob=0.0,  # Adjust if you want augmentation for training\n",
    "    generalization_prob=0.0  # Adjust if you want generalization for training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26e9af",
   "metadata": {},
   "source": [
    "#### Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2104e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's validate the split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "filepath = 'data_gifflar/taxonomy_Kingdom.tsv'\n",
    "\n",
    "# 1. Load the original DataFrame\n",
    "multilabel_kingdom_df_original = pd.read_csv(filepath, sep=\"\\t\")\n",
    "kingdom_cols = ['Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae', 'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae', 'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista', 'Riboviria']\n",
    "\n",
    "# 2. Split the original DataFrame by 'split'\n",
    "train_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'train']\n",
    "val_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'val']\n",
    "test_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'test']\n",
    "\n",
    "# 3. Extract glycans and labels directly\n",
    "glycan_train_list = train_df['IUPAC'].tolist()\n",
    "label_train_list = [train_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_train_list = list(zip(*label_train_list)) # Transpose\n",
    "\n",
    "glycan_val_list = val_df['IUPAC'].tolist()\n",
    "label_val_list = [val_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_val_list = list(zip(*label_val_list)) # Transpose\n",
    "\n",
    "glycan_test_list = test_df['IUPAC'].tolist()\n",
    "label_test_list = [test_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_test_list = list(zip(*label_test_list)) # Transpose\n",
    "\n",
    "print(f\"Number of training glycans: {len(glycan_train_list)}\")\n",
    "print(f\"Number of validation glycans: {len(glycan_val_list)}\")\n",
    "print(f\"Number of test glycans: {len(glycan_test_list)}\")\n",
    "print(f\"Shape of training labels: {len(label_train_list)} x {len(label_train_list[0]) if label_train_list else 0}\")\n",
    "print(f\"Shape of validation labels: {len(label_val_list)} x {len(label_val_list[0]) if label_val_list else 0}\")\n",
    "print(f\"Shape of test labels: {len(label_test_list)} x {len(label_test_list[0]) if label_test_list else 0}\")\n",
    "\n",
    "# Now re-run the checking function (modified for this new label extraction)\n",
    "def check_example_direct(glycan_list, label_list, split_name, original_df, kingdom_cols):\n",
    "    if glycan_list:\n",
    "        example_index = 0\n",
    "        example_glycan = glycan_list[example_index]\n",
    "        example_labels_split = list(label_list[example_index])\n",
    "\n",
    "        original_row = original_df[original_df['IUPAC'] == example_glycan].iloc[0]\n",
    "        labels_original = np.array([original_row[col] for col in kingdom_cols], dtype=np.float32).tolist()\n",
    "\n",
    "        print(f\"--- Checking example from {split_name} set (Direct) ---\")\n",
    "        print(f\"Glycan: {example_glycan}\")\n",
    "        print(f\"Split in original data: {original_row['split']}\")\n",
    "        print(f\"Labels in split data: {example_labels_split}\")\n",
    "        print(f\"Labels in original data: {labels_original}\")\n",
    "        if labels_original == example_labels_split:\n",
    "            print(\"Labels match!\")\n",
    "        else:\n",
    "            print(\"Labels DO NOT match!\")\n",
    "    else:\n",
    "        print(f\"{split_name} set is empty.\")\n",
    "\n",
    "check_example_direct(glycan_train_list, label_train_list, 'train', multilabel_kingdom_df_original, kingdom_cols)\n",
    "check_example_direct(glycan_val_list, label_val_list, 'val', multilabel_kingdom_df_original, kingdom_cols)\n",
    "check_example_direct(glycan_test_list, label_test_list, 'test', multilabel_kingdom_df_original, kingdom_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multilabel_kingdom_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check one of the graphs in the dataloader\n",
    "\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataLoader is called 'multilabel_kingdom_loader'\n",
    "inspected_loader = multilabel_kingdom_loaders_emb['train'] # Access the train DataLoader\n",
    "\n",
    "# Get the first batch of data\n",
    "try:\n",
    "    batch = next(iter(inspected_loader))\n",
    "    print(\"Batch:\", batch)\n",
    "    print(\"Number of graphs in batch:\", batch.num_graphs)\n",
    "\n",
    "    # Extract the first graph from the batch\n",
    "    first_graph_data = batch[2] #change to check other graphs\n",
    "    print(\"\\nFirst graph data:\", first_graph_data)\n",
    "    print(\"Node features (x):\", first_graph_data.x)\n",
    "    print(\"Edge indices (edge_index):\", first_graph_data.edge_index)\n",
    "    print(\"Labels (y):\", first_graph_data.y)\n",
    "    print(\"String labels:\", first_graph_data.string_labels)\n",
    "\n",
    "    # Convert the PyG Data object to a NetworkX graph for visualization\n",
    "    nx_graph = to_networkx(first_graph_data) \n",
    "\n",
    "    # Visualize the NetworkX graph\n",
    "    nx_graph = to_networkx(first_graph_data)\n",
    "    pos = nx.spring_layout(nx_graph)\n",
    "    nx.draw(nx_graph, pos, with_labels=True, node_size=500, node_color=\"red\", font_size=10, font_weight=\"bold\", arrows=False)\n",
    "    plt.title(\"First Graph from DataLoader (PyG)\")\n",
    "    plt.show()\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"The DataLoader is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87dc8a6",
   "metadata": {},
   "source": [
    "## Let's make a function to add embeddings to a dataloader object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the embeddings again here, so I don't have to jump up and down whenever I reload the kernel)\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            glm_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add GLM embeddings to a dictionary of dataloaders before loading them into the model\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def add_glm_embeddings_to_dataloaders(dataloaders, glm_embeddings):\n",
    "    embedded_loaders = {}\n",
    "    embedding_dim = 320\n",
    "    for split, loader in dataloaders.items():\n",
    "        embedded_data_list = []\n",
    "        for batch in loader:\n",
    "            for graph in batch.to_data_list():\n",
    "                node_embeddings = []\n",
    "                if hasattr(graph, 'string_labels'):\n",
    "                    for label in graph.string_labels:\n",
    "                        if label in glm_embeddings:\n",
    "                            embedding = glm_embeddings[label]\n",
    "                            node_embeddings.append(torch.tensor(embedding))\n",
    "                        else:\n",
    "                            node_embeddings.append(torch.zeros(embedding_dim))\n",
    "                    graph.x = torch.stack(node_embeddings).float()\n",
    "                    #print(f\"Shape of graph.x after adding embeddings: {graph.x.shape}\")\n",
    "                embedded_data_list.append(graph)\n",
    "\n",
    "        embedded_loaders[split] = DataLoader(embedded_data_list, batch_size=32)\n",
    "        print(f\"GLM embeddings added to {split} DataLoader.\")\n",
    "    return embedded_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b83ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the function\n",
    "\n",
    "glycan_loaders_emb = add_glm_embeddings_to_dataloaders(glycan_loaders, glm_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef713ec2",
   "metadata": {},
   "source": [
    "## Lets look at the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e13ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at string labels\n",
    "\n",
    "train_loader = multilabel_kingdom_loaders_emb['train'] \n",
    "batch = next(iter(train_loader))\n",
    "first_graph_labels = batch[2].string_labels\n",
    "print(\"Sample string_labels from the first graph:\")\n",
    "print(first_graph_labels[:20])  # Print the first 20 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6368a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the embeddings themselves\n",
    "\n",
    "train_loader = multilabel_kingdom_loaders_emb['train'] \n",
    "batch = next(iter(train_loader))\n",
    "first_graph_embeddings = batch[2].x\n",
    "print(\"Sample embeddings from the first graph:\")\n",
    "print(first_graph_embeddings[:20])  # Print the first 20 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4130ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample keys from glm_embeddings:\")\n",
    "print(list(glm_embeddings.keys())[:200])  # Print the first 20 keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fafdb1",
   "metadata": {},
   "source": [
    "## Copies of glycowork functions edited to use embeddings directly ||||run on kernel restart||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original SweetNet class from the glycowork package\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Union, Literal\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.nn import GraphConv\n",
    "    from torch_geometric.nn import global_mean_pool as gap\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "  raise ImportError(\"<torch or torch_geometric missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from glycowork.glycan_data.loader import lib, download_model \n",
    "\n",
    "class SweetNet(torch.nn.Module):\n",
    "    def __init__(self, lib_size: int, # number of unique tokens for graph nodes\n",
    "                 num_classes: int = 1, # number of output classes (>1 for multilabel)\n",
    "                 hidden_dim: int = 128 # dimension of hidden layers\n",
    "                ) -> None:\n",
    "        \"given glycan graphs as input, predicts properties via a graph neural network\"\n",
    "        super(SweetNet, self).__init__()\n",
    "        # Convolution operations on the graph\n",
    "        self.conv1 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GraphConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Node embedding\n",
    "        self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "        # Fully connected part\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, 1024)\n",
    "        self.lin2 = torch.nn.Linear(1024, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, num_classes)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(1024)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.act1 = torch.nn.LeakyReLU()\n",
    "        self.act2 = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor,\n",
    "                inference: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        # Getting node features\n",
    "        x = self.item_embedding(x)\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        # Graph convolution operations\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = gap(x, batch)\n",
    "\n",
    "        # Fully connected part\n",
    "        x = self.act1(self.bn1(self.lin1(x)))\n",
    "        x_out = self.bn2(self.lin2(x))\n",
    "        x = F.dropout(self.act2(x_out), p = 0.5, training = self.training)\n",
    "\n",
    "        x = self.lin3(x).squeeze(1)\n",
    "\n",
    "        if inference:\n",
    "          return x, x_out\n",
    "        else:\n",
    "          return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333ffcc",
   "metadata": {},
   "source": [
    "### Other functions I might modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model function\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import torch\n",
    "    # Choose the correct computing architecture\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "    raise ImportError(\"<torch missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, mean_squared_error, \\\n",
    "    label_ranking_average_precision_score, ndcg_score, roc_auc_score, mean_absolute_error, r2_score\n",
    "from glycowork.motif.annotate import annotate_dataset\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 7, # epochs to wait after last improvement\n",
    "                 verbose: bool = False # whether to print messages\n",
    "                ) -> None:\n",
    "        \"Early stops the training if validation loss doesn't improve after a given patience\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = 0\n",
    "\n",
    "    def __call__(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), 'drive/My Drive/checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def sigmoid(x: float # input value\n",
    "          ) -> float: # sigmoid transformed value\n",
    "    \"Apply sigmoid transformation to input\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def disable_running_stats(model: torch.nn.Module # model to disable batch norm\n",
    "                       ) -> None:\n",
    "    \"Disable batch normalization running statistics\"\n",
    "\n",
    "    def _disable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "\n",
    "def enable_running_stats(model: torch.nn.Module # model to enable batch norm\n",
    "                      ) -> None:\n",
    "    \"Enable batch normalization running statistics\"\n",
    "\n",
    "    def _enable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def train_model(model: torch.nn.Module, # graph neural network for analyzing glycans\n",
    "               dataloaders: Dict[str, torch.utils.data.DataLoader], # dict with 'train' and 'val' loaders\n",
    "               criterion: torch.nn.Module, # PyTorch loss function\n",
    "               optimizer: torch.optim.Optimizer, # PyTorch optimizer, has to be SAM if mode != \"regression\"\n",
    "               scheduler: torch.optim.lr_scheduler._LRScheduler, # PyTorch learning rate decay\n",
    "               num_epochs: int = 25, # number of epochs for training\n",
    "               patience: int = 50, # epochs without improvement until early stop\n",
    "               mode: str = 'classification', # 'classification', 'multilabel', or 'regression'\n",
    "               mode2: str = 'multi', # 'multi' or 'binary' classification\n",
    "               return_metrics: bool = False, # whether to return metrics\n",
    "               use_external_embeddings: bool = False # whether to use external embeddings\n",
    "              ) -> Union[torch.nn.Module, tuple[torch.nn.Module, dict[str, dict[str, list[float]]]]]: # best model from training and the training and validation metrics\n",
    "    \"trains a deep learning model on predicting glycan properties\"\n",
    "\n",
    "    since = time.time()\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "    best_lead_metric = float(\"inf\")\n",
    "\n",
    "    if mode == 'classification':\n",
    "         # Removed auroc from metrics to avoid calculation issues for now\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": []}\n",
    "    elif mode == 'multilabel':\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": [], \"lrap\": [], \"ndcg\": []}\n",
    "    else:\n",
    "        blank_metrics = {\"loss\": [], \"mse\": [], \"mae\": [], \"r2\": []}\n",
    "\n",
    "    metrics = {\"train\": copy.deepcopy(blank_metrics), \"val\": copy.deepcopy(blank_metrics)}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_metrics = copy.deepcopy(blank_metrics)\n",
    "            running_metrics[\"weights\"] = []\n",
    "\n",
    "            for data in dataloaders[phase]:\n",
    "                # Get all relevant node attributes\n",
    "                #print(f\"Phase: {phase}, Data: {data}\")\n",
    "                #print(f\"Phase: {phase}, Data.x: {getattr(data, 'x', None)}\") # Check if x exists and its value\n",
    "                if use_external_embeddings:\n",
    "                    x, y, edge_index, batch = data.x, data.y, data.edge_index, data.batch\n",
    "                else:\n",
    "                    x, y, edge_index, batch = data.labels, data.y, data.edge_index, data.batch\n",
    "                prot = getattr(data, 'train_idx', None)\n",
    "                if prot is not None:\n",
    "                    prot = prot.view(max(batch) + 1, -1).to(device)\n",
    "                x = x.to(device)\n",
    "                if mode == 'multilabel':\n",
    "                    y = y.view(max(batch) + 1, -1).to(device)\n",
    "                else:\n",
    "                    y = y.to(device)\n",
    "                y = y.view(-1, 1) if mode == 'regression' else y\n",
    "                edge_index = edge_index.to(device)\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # First forward pass\n",
    "                    if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                        enable_running_stats(model)\n",
    "                    pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                    loss = criterion(pred, y)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                            optimizer.first_step(zero_grad = True)\n",
    "                            # Second forward pass\n",
    "                            disable_running_stats(model)\n",
    "                            second_pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                            criterion(second_pred, y).backward()\n",
    "                            optimizer.second_step(zero_grad = True)\n",
    "                        else:\n",
    "                            optimizer.step()\n",
    "                \n",
    "                # Check for single-class batches\n",
    "                unique_classes = torch.unique(y).cpu().numpy()\n",
    "                if len(unique_classes) == 1:\n",
    "                    print(f\"WARNING: Single-class batch detected in {phase} phase at epoch {epoch}!\")\n",
    "                    print(f\"Batch labels: {unique_classes}\")\n",
    "\n",
    "                # Collecting relevant metrics\n",
    "                running_metrics[\"loss\"].append(loss.item())\n",
    "                running_metrics[\"weights\"].append(batch.max().cpu() + 1)\n",
    "\n",
    "                y_det = y.detach().cpu().numpy()\n",
    "                pred_det = pred.cpu().detach().numpy()\n",
    "                if mode == 'classification':\n",
    "                    if mode2 == 'multi':\n",
    "                        pred_proba = np.exp(pred_det) / np.sum(np.exp(pred_det), axis = 1, keepdims = True)  # numpy softmax\n",
    "                        pred2 = np.argmax(pred_det, axis = 1)\n",
    "                    else:\n",
    "                        pred_proba = sigmoid(pred_det)\n",
    "                        pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det, pred2))\n",
    "                    # commented out auroc because it was throwing errors\n",
    "                    #running_metrics[\"auroc\"].append(roc_auc_score(y_det.astype(int), pred_proba if mode2 == 'binary' else pred_proba[:, 1]))\n",
    "                elif mode == 'multilabel':\n",
    "                    pred_proba = sigmoid(pred_det)\n",
    "                    pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det.flatten(), pred2.flatten()))\n",
    "                    running_metrics[\"lrap\"].append(label_ranking_average_precision_score(y_det.astype(int), pred_proba))\n",
    "                    running_metrics[\"ndcg\"].append(ndcg_score(y_det.astype(int), pred_proba))\n",
    "                else:\n",
    "                    running_metrics[\"mse\"].append(mean_squared_error(y_det, pred_det))\n",
    "                    running_metrics[\"mae\"].append(mean_absolute_error(y_det, pred_det))\n",
    "                    running_metrics[\"r2\"].append(r2_score(y_det, pred_det))\n",
    "\n",
    "            # Averaging metrics at end of epoch\n",
    "            for key in running_metrics:\n",
    "                if key == \"weights\":\n",
    "                    continue\n",
    "                metrics[phase][key].append(np.average(running_metrics[key], weights = running_metrics[\"weights\"]))\n",
    "\n",
    "            if mode == 'classification':\n",
    "                print('{} Loss: {:.4f} Accuracy: {:.4f} MCC: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            elif mode == 'multilabel':\n",
    "                print('{} Loss: {:.4f} LRAP: {:.4f} NDCG: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            else:\n",
    "                print('{} Loss: {:.4f} MSE: {:.4f} MAE: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"mse\"][-1], metrics[phase][\"mae\"][-1]))\n",
    "\n",
    "            # Keep best model state_dict\n",
    "            if phase == \"val\":\n",
    "                if metrics[phase][\"loss\"][-1] <= best_loss:\n",
    "                    best_loss = metrics[phase][\"loss\"][-1]\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                    # Extract the lead metric (ACC, LRAP, or MSE) of the new best model\n",
    "                    if mode == 'classification':\n",
    "                        best_lead_metric = metrics[phase][\"acc\"][-1]\n",
    "                    elif mode == 'multilabel':\n",
    "                        best_lead_metric = metrics[phase][\"lrap\"][-1]\n",
    "                    else:\n",
    "                        best_lead_metric = metrics[phase][\"mse\"][-1]\n",
    "\n",
    "                # Check Early Stopping & adjust learning rate if needed\n",
    "                early_stopping(metrics[phase][\"loss\"][-1], model)\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(metrics[phase][\"loss\"][-1])\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "        if mode == 'classification':\n",
    "            print('Best val loss: {:4f}, best Accuracy score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "        elif mode == 'multilabel':\n",
    "            print('Best val loss: {:4f}, best LRAP score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "        else:\n",
    "            print('Best val loss: {:4f}, best MSE score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "        model.load_state_dict(best_model_wts)\n",
    "\n",
    "        if return_metrics:\n",
    "            return model, metrics\n",
    "\n",
    "    # Plot loss & score over the course of training\n",
    "    _, _ = plt.subplots(nrows=2, ncols=1)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(epoch + 1), metrics[\"val\"][\"loss\"])\n",
    "    plt.title('Model Training')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.legend(['Validation Loss'], loc='best')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    if mode == 'classification':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"acc\"])\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "        plt.legend(['Validation Accuracy'], loc='best')\n",
    "    elif mode == 'multilabel':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"lrap\"])\n",
    "        plt.ylabel('Validation LRAP')\n",
    "        plt.legend(['Validation LRAP'], loc='best')\n",
    "    else:\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"mse\"])\n",
    "        plt.ylabel('Validation MSE')\n",
    "        plt.legend(['Validation MSE'], loc='best')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init_weights function\n",
    "\n",
    "def init_weights(model: torch.nn.Module, # neural network for analyzing glycans\n",
    "                mode: str = 'sparse', # initialization algorithm: 'sparse', 'kaiming', 'xavier'\n",
    "                sparsity: float = 0.1 # proportion of sparsity after initialization\n",
    "               ) -> None:\n",
    "    \"initializes linear layers of PyTorch model with a weight initialization\"\n",
    "    #print(\"Using init_weights from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if isinstance(model, torch.nn.Linear):\n",
    "        if mode == 'sparse':\n",
    "            torch.nn.init.sparse_(model.weight, sparsity = sparsity)\n",
    "        elif mode == 'kaiming':\n",
    "            torch.nn.init.kaiming_uniform_(model.weight)\n",
    "        elif mode == 'xavier':\n",
    "            torch.nn.init.xavier_uniform_(model.weight)\n",
    "        else:\n",
    "            print(\"This initialization option is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e02dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_model function\n",
    "\n",
    "def prep_model(model_type: Literal[\"SweetNet\", \"LectinOracle\", \"LectinOracle_flex\", \"NSequonPred\"], # type of model to create\n",
    "              num_classes: int, # number of unique classes for classification\n",
    "              libr: Optional[Dict[str, int]] = None, # dictionary of form glycoletter:index\n",
    "              trained: bool = False, # whether to use pretrained model\n",
    "              # set hidden_dim to 320 rather than 128 for the pretrained model\n",
    "              # but 128 is the default for the model in the paper\n",
    "              hidden_dim: int = 320, # hidden dimension for the model (SweetNet/LectinOracle only)\n",
    "              use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "             ) -> torch.nn.Module: # initialized PyTorch model\n",
    "    \"wrapper to instantiate model, initialize it, and put it on the GPU\"\n",
    "    #print(\"Using prep_model from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if libr is None:\n",
    "      libr = lib\n",
    "    if model_type == 'SweetNet':\n",
    "      model = SweetNet(len(libr), num_classes = num_classes, hidden_dim = hidden_dim, use_external_embeddings = use_external_embeddings)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'sparse'))\n",
    "      if trained:\n",
    "        if hidden_dim != 128:\n",
    "          raise ValueError(\"Hidden dimension must be 128 for pretrained model\")\n",
    "        model_path = download_model(\"glycowork_sweetnet_species.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle':\n",
    "      model = LectinOracle(len(libr), num_classes = num_classes, input_size_prot = int(10*hidden_dim))\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle_flex':\n",
    "      model = LectinOracle_flex(len(libr), num_classes = num_classes)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle_flex.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'NSequonPred':\n",
    "      model = NSequonPred()\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"NSequonPred_batch32.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    else:\n",
    "      print(\"Invalid Model Type\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2749ab38",
   "metadata": {},
   "source": [
    "## Copies of glycowork functions edited for glm embedded dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SweetNet class\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Union, Literal\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.nn import GraphConv\n",
    "    from torch_geometric.nn import global_mean_pool as gap\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "  raise ImportError(\"<torch or torch_geometric missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from glycowork.glycan_data.loader import lib, download_model \n",
    "\n",
    "class SweetNet(torch.nn.Module):\n",
    "    def __init__(self, lib_size: int, # number of unique tokens for graph nodes\n",
    "                 num_classes: int = 1, # number of output classes (>1 for multilabel)\n",
    "                 hidden_dim: int = 320, # dimension of hidden layers (changed from 128 to 320)\n",
    "                 use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "                ) -> None:\n",
    "        \"given glycan graphs as input, predicts properties via a graph neural network\"\n",
    "        #print(\"Using SweetNet from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "        super(SweetNet, self).__init__()\n",
    "        self.use_external_embeddings = use_external_embeddings\n",
    "        # Convolution operations on the graph\n",
    "        self.conv1 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GraphConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Node embedding\n",
    "        if use_external_embeddings:\n",
    "            self.embedding_dim = hidden_dim\n",
    "        else:\n",
    "            self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    " \n",
    "\n",
    "        #self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    "        # Fully connected part\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, 1024)\n",
    "        self.lin2 = torch.nn.Linear(1024, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, num_classes)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(1024)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.act1 = torch.nn.LeakyReLU()\n",
    "        self.act2 = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor,\n",
    "                inference: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \n",
    "        # Getting node features\n",
    "        if self.use_external_embeddings:\n",
    "           # Use external embeddings (already in x)\n",
    "            pass # x is already the embeddings\n",
    "        else:\n",
    "             # Use internal embedding\n",
    "            x = self.item_embedding(x).squeeze(1)\n",
    "            \n",
    "        \n",
    "        # Graph convolution operations (now using GLM embeddings as x)\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = gap(x, batch)\n",
    "\n",
    "        # Fully connected part\n",
    "        x = self.act1(self.bn1(self.lin1(x)))\n",
    "        x_out = self.bn2(self.lin2(x))\n",
    "        x = F.dropout(self.act2(x_out), p = 0.5, training = self.training)\n",
    "\n",
    "        x = self.lin3(x).squeeze(1)\n",
    "\n",
    "        if inference:\n",
    "          return x, x_out\n",
    "        else:\n",
    "          return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03900750",
   "metadata": {},
   "source": [
    "### Other functions I might modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model function\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import torch\n",
    "    # Choose the correct computing architecture\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "    raise ImportError(\"<torch missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, mean_squared_error, \\\n",
    "    label_ranking_average_precision_score, ndcg_score, roc_auc_score, mean_absolute_error, r2_score\n",
    "from glycowork.motif.annotate import annotate_dataset\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 7, # epochs to wait after last improvement\n",
    "                 verbose: bool = False # whether to print messages\n",
    "                ) -> None:\n",
    "        \"Early stops the training if validation loss doesn't improve after a given patience\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = 0\n",
    "\n",
    "    def __call__(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), 'drive/My Drive/checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def sigmoid(x: float # input value\n",
    "          ) -> float: # sigmoid transformed value\n",
    "    \"Apply sigmoid transformation to input\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def disable_running_stats(model: torch.nn.Module # model to disable batch norm\n",
    "                       ) -> None:\n",
    "    \"Disable batch normalization running statistics\"\n",
    "\n",
    "    def _disable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "\n",
    "def enable_running_stats(model: torch.nn.Module # model to enable batch norm\n",
    "                      ) -> None:\n",
    "    \"Enable batch normalization running statistics\"\n",
    "\n",
    "    def _enable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def train_model(model: torch.nn.Module, # graph neural network for analyzing glycans\n",
    "               dataloaders: Dict[str, torch.utils.data.DataLoader], # dict with 'train' and 'val' loaders\n",
    "               criterion: torch.nn.Module, # PyTorch loss function\n",
    "               optimizer: torch.optim.Optimizer, # PyTorch optimizer, has to be SAM if mode != \"regression\"\n",
    "               scheduler: torch.optim.lr_scheduler._LRScheduler, # PyTorch learning rate decay\n",
    "               num_epochs: int = 25, # number of epochs for training\n",
    "               patience: int = 50, # epochs without improvement until early stop\n",
    "               mode: str = 'classification', # 'classification', 'multilabel', or 'regression'\n",
    "               mode2: str = 'multi', # 'multi' or 'binary' classification\n",
    "               return_metrics: bool = False, # whether to return metrics\n",
    "               use_external_embeddings: bool = False # whether to use external embeddings\n",
    "              ) -> Union[torch.nn.Module, tuple[torch.nn.Module, dict[str, dict[str, list[float]]]]]: # best model from training and the training and validation metrics\n",
    "    \"trains a deep learning model on predicting glycan properties\"\n",
    "\n",
    "    since = time.time()\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "    best_lead_metric = float(\"inf\")\n",
    "\n",
    "    if mode == 'classification':\n",
    "         # Removed auroc from metrics to avoid calculation issues for now\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": []}\n",
    "    elif mode == 'multilabel':\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": [], \"lrap\": [], \"ndcg\": []}\n",
    "    else:\n",
    "        blank_metrics = {\"loss\": [], \"mse\": [], \"mae\": [], \"r2\": []}\n",
    "\n",
    "    metrics = {\"train\": copy.deepcopy(blank_metrics), \"val\": copy.deepcopy(blank_metrics)}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_metrics = copy.deepcopy(blank_metrics)\n",
    "            running_metrics[\"weights\"] = []\n",
    "\n",
    "            for data in dataloaders[phase]:\n",
    "                # Get all relevant node attributes\n",
    "                #print(f\"Phase: {phase}, Data: {data}\")\n",
    "                #print(f\"Phase: {phase}, Data.x: {getattr(data, 'x', None)}\") # Check if x exists and its value\n",
    "                if use_external_embeddings:\n",
    "                    x, y, edge_index, batch = data.x, data.y, data.edge_index, data.batch\n",
    "                else:\n",
    "                    x, y, edge_index, batch = data.labels, data.y, data.edge_index, data.batch\n",
    "                prot = getattr(data, 'train_idx', None)\n",
    "                if prot is not None:\n",
    "                    prot = prot.view(max(batch) + 1, -1).to(device)\n",
    "                x = x.to(device)\n",
    "                if mode == 'multilabel':\n",
    "                    y = y.view(max(batch) + 1, -1).to(device)\n",
    "                else:\n",
    "                    y = y.to(device)\n",
    "                y = y.view(-1, 1) if mode == 'regression' else y\n",
    "                edge_index = edge_index.to(device)\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # First forward pass\n",
    "                    if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                        enable_running_stats(model)\n",
    "                    pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                    loss = criterion(pred, y)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                            optimizer.first_step(zero_grad = True)\n",
    "                            # Second forward pass\n",
    "                            disable_running_stats(model)\n",
    "                            second_pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                            criterion(second_pred, y).backward()\n",
    "                            optimizer.second_step(zero_grad = True)\n",
    "                        else:\n",
    "                            optimizer.step()\n",
    "                \n",
    "                # Check for single-class batches\n",
    "                unique_classes = torch.unique(y).cpu().numpy()\n",
    "                if len(unique_classes) == 1:\n",
    "                    print(f\"WARNING: Single-class batch detected in {phase} phase at epoch {epoch}!\")\n",
    "                    print(f\"Batch labels: {unique_classes}\")\n",
    "\n",
    "                # Collecting relevant metrics\n",
    "                running_metrics[\"loss\"].append(loss.item())\n",
    "                running_metrics[\"weights\"].append(batch.max().cpu() + 1)\n",
    "\n",
    "                y_det = y.detach().cpu().numpy()\n",
    "                pred_det = pred.cpu().detach().numpy()\n",
    "                if mode == 'classification':\n",
    "                    if mode2 == 'multi':\n",
    "                        pred_proba = np.exp(pred_det) / np.sum(np.exp(pred_det), axis = 1, keepdims = True)  # numpy softmax\n",
    "                        pred2 = np.argmax(pred_det, axis = 1)\n",
    "                    else:\n",
    "                        pred_proba = sigmoid(pred_det)\n",
    "                        pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det, pred2))\n",
    "                    # commented out auroc because it was throwing errors\n",
    "                    #running_metrics[\"auroc\"].append(roc_auc_score(y_det.astype(int), pred_proba if mode2 == 'binary' else pred_proba[:, 1]))\n",
    "                elif mode == 'multilabel':\n",
    "                    pred_proba = sigmoid(pred_det)\n",
    "                    pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det.flatten(), pred2.flatten()))\n",
    "                    running_metrics[\"lrap\"].append(label_ranking_average_precision_score(y_det.astype(int), pred_proba))\n",
    "                    running_metrics[\"ndcg\"].append(ndcg_score(y_det.astype(int), pred_proba))\n",
    "                else:\n",
    "                    running_metrics[\"mse\"].append(mean_squared_error(y_det, pred_det))\n",
    "                    running_metrics[\"mae\"].append(mean_absolute_error(y_det, pred_det))\n",
    "                    running_metrics[\"r2\"].append(r2_score(y_det, pred_det))\n",
    "\n",
    "            # Averaging metrics at end of epoch\n",
    "            for key in running_metrics:\n",
    "                if key == \"weights\":\n",
    "                    continue\n",
    "                metrics[phase][key].append(np.average(running_metrics[key], weights = running_metrics[\"weights\"]))\n",
    "\n",
    "            if mode == 'classification':\n",
    "                print('{} Loss: {:.4f} Accuracy: {:.4f} MCC: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            elif mode == 'multilabel':\n",
    "                print('{} Loss: {:.4f} LRAP: {:.4f} NDCG: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            else:\n",
    "                print('{} Loss: {:.4f} MSE: {:.4f} MAE: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"mse\"][-1], metrics[phase][\"mae\"][-1]))\n",
    "\n",
    "            # Keep best model state_dict\n",
    "            if phase == \"val\":\n",
    "                if metrics[phase][\"loss\"][-1] <= best_loss:\n",
    "                    best_loss = metrics[phase][\"loss\"][-1]\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                    # Extract the lead metric (ACC, LRAP, or MSE) of the new best model\n",
    "                    if mode == 'classification':\n",
    "                        best_lead_metric = metrics[phase][\"acc\"][-1]\n",
    "                    elif mode == 'multilabel':\n",
    "                        best_lead_metric = metrics[phase][\"lrap\"][-1]\n",
    "                    else:\n",
    "                        best_lead_metric = metrics[phase][\"mse\"][-1]\n",
    "\n",
    "                # Check Early Stopping & adjust learning rate if needed\n",
    "                early_stopping(metrics[phase][\"loss\"][-1], model)\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(metrics[phase][\"loss\"][-1])\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    if mode == 'classification':\n",
    "        print('Best val loss: {:4f}, best Accuracy score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    elif mode == 'multilabel':\n",
    "        print('Best val loss: {:4f}, best LRAP score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    else:\n",
    "        print('Best val loss: {:4f}, best MSE score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    if return_metrics:\n",
    "        return model, metrics\n",
    "\n",
    "    # Plot loss & score over the course of training\n",
    "    _, _ = plt.subplots(nrows=2, ncols=1)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(epoch + 1), metrics[\"val\"][\"loss\"])\n",
    "    plt.title('Model Training')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.legend(['Validation Loss'], loc='best')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    if mode == 'classification':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"acc\"])\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "        plt.legend(['Validation Accuracy'], loc='best')\n",
    "    elif mode == 'multilabel':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"lrap\"])\n",
    "        plt.ylabel('Validation LRAP')\n",
    "        plt.legend(['Validation LRAP'], loc='best')\n",
    "    else:\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"mse\"])\n",
    "        plt.ylabel('Validation MSE')\n",
    "        plt.legend(['Validation MSE'], loc='best')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a375b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init_weights function\n",
    "\n",
    "def init_weights(model: torch.nn.Module, # neural network for analyzing glycans\n",
    "                mode: str = 'sparse', # initialization algorithm: 'sparse', 'kaiming', 'xavier'\n",
    "                sparsity: float = 0.1 # proportion of sparsity after initialization\n",
    "               ) -> None:\n",
    "    \"initializes linear layers of PyTorch model with a weight initialization\"\n",
    "    #print(\"Using init_weights from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if isinstance(model, torch.nn.Linear):\n",
    "        if mode == 'sparse':\n",
    "            torch.nn.init.sparse_(model.weight, sparsity = sparsity)\n",
    "        elif mode == 'kaiming':\n",
    "            torch.nn.init.kaiming_uniform_(model.weight)\n",
    "        elif mode == 'xavier':\n",
    "            torch.nn.init.xavier_uniform_(model.weight)\n",
    "        else:\n",
    "            print(\"This initialization option is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_model function\n",
    "\n",
    "def prep_model(model_type: Literal[\"SweetNet\", \"LectinOracle\", \"LectinOracle_flex\", \"NSequonPred\"], # type of model to create\n",
    "              num_classes: int, # number of unique classes for classification\n",
    "              libr: Optional[Dict[str, int]] = None, # dictionary of form glycoletter:index\n",
    "              trained: bool = False, # whether to use pretrained model\n",
    "              # set hidden_dim to 320 rather than 128 for the pretrained model\n",
    "              # but 128 is the default for the model in the paper\n",
    "              hidden_dim: int = 320, # hidden dimension for the model (SweetNet/LectinOracle only)\n",
    "              use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "             ) -> torch.nn.Module: # initialized PyTorch model\n",
    "    \"wrapper to instantiate model, initialize it, and put it on the GPU\"\n",
    "    #print(\"Using prep_model from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if libr is None:\n",
    "      libr = lib\n",
    "    if model_type == 'SweetNet':\n",
    "      model = SweetNet(len(libr), num_classes = num_classes, hidden_dim = hidden_dim, use_external_embeddings = use_external_embeddings)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'sparse'))\n",
    "      if trained:\n",
    "        if hidden_dim != 128:\n",
    "          raise ValueError(\"Hidden dimension must be 128 for pretrained model\")\n",
    "        model_path = download_model(\"glycowork_sweetnet_species.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle':\n",
    "      model = LectinOracle(len(libr), num_classes = num_classes, input_size_prot = int(10*hidden_dim))\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle_flex':\n",
    "      model = LectinOracle_flex(len(libr), num_classes = num_classes)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle_flex.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'NSequonPred':\n",
    "      model = NSequonPred()\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"NSequonPred_batch32.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    else:\n",
    "      print(\"Invalid Model Type\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbbca33",
   "metadata": {},
   "source": [
    "## Testing using same framework as iteration 0 (basic kingdom sweetnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# testing the modified SweetNet model on the GlycoWork dataset \n",
    "from glycowork.glycan_data.loader import df_species\n",
    "from glycowork.ml.train_test_split import hierarchy_filter\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "from glycowork.ml import model_training\n",
    "\n",
    "# silence the avalanche of \"undefined\" warnings\n",
    "#import warnings\n",
    "#from sklearn.exceptions import UndefinedMetricWarning\n",
    "#warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "train_x, val_x, train_y, val_y, id_val, class_list, class_converter = hierarchy_filter(df_species,\n",
    "                                                                                       rank = 'Kingdom')\n",
    "\n",
    "dataloaders = split_data_to_train(train_x, val_x, train_y, val_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets split out the training code so I don't have to load the data each time\n",
    "\n",
    "model = prep_model('SweetNet', len(class_list))\n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = len(class_list))\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'classification',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4382d5b4",
   "metadata": {},
   "source": [
    "### Extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdade799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the Kingdoms in the original data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "kingdom_counts = df_species['Kingdom'].value_counts()\n",
    "print(\"Kingdom distribution in the original data:\")\n",
    "print(kingdom_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f112562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Print the first few rows to get an overview\n",
    "print(df_species.head())\n",
    "\n",
    "# Print the data types of the columns\n",
    "print(df_species.info())\n",
    "\n",
    "# Check for duplicated glycans\n",
    "print(f\"Number of unique glycans: {df_species['glycan'].nunique()}\")\n",
    "print(f\"Total number of rows: {len(df_species)}\")\n",
    "\n",
    "# If you suspect a column contains multiple kingdoms:\n",
    "if 'Kingdom' in df_species.columns:  # Replace 'Kingdom' with the actual column name\n",
    "    # Check the first few values of that column\n",
    "    print(df_species['Kingdom'].head())\n",
    "    # If it's a string with delimiters, count the delimiters\n",
    "    if isinstance(df_species['Kingdom'][0], str) and ',' in df_species['Kingdom'][0]: #assuming ',' is the delimiter\n",
    "        print(f\"Example value: {df_species['Kingdom'][0]}\")\n",
    "        print(f\"Number of commas in the first value: {df_species['Kingdom'][0].count(',')}\")\n",
    "\n",
    "    # Check for multiple columns representing kingdoms\n",
    "    for col in df_species.columns:\n",
    "        if col in ['Animalia', 'Bacteria', 'Plantae']:  # Add all possible kingdom column names\n",
    "            print(f\"Column '{col}': Data type = {df_species[col].dtype}, Unique values = {df_species[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set()\n",
    "duplicates_count = 0\n",
    "\n",
    "for item in train_x:\n",
    "    if item in seen:\n",
    "        duplicates_count += 1\n",
    "    else:\n",
    "        seen.add(item)\n",
    "\n",
    "print(f\"Number of duplicates: {duplicates_count}\")\n",
    "print(f\"Number of unique items: {len(seen)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd3127",
   "metadata": {},
   "source": [
    "## New custom prep function for the GLM-Infused Sweetnet ||||run on kernel restart||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fdcb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the embeddings again here, so I don't have to jump up and down whenever I reload the kernel)\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            glm_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb565ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Literal \n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    from torch_geometric.nn import GraphConv\n",
    "    from torch_geometric.nn import global_mean_pool as gap\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "  raise ImportError(\"<torch or torch_geometric missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from glycowork.glycan_data.loader import lib\n",
    "\n",
    "def prep_infused_sweetnet(num_classes: int, # number of unique classes for classification\n",
    "                           embeddings_dict: Optional[Dict[str, np.ndarray]] = None, # embeddings for 'external' method\n",
    "                           initialization_method: Literal['external', 'random', 'one_hot'] = 'external', # specifies initialization method\n",
    "                           trainable_embeddings: bool = True, # whether the external embeddings should be trainable\n",
    "                           hidden_dim: int = 320, # hidden dimension for the model (be sure to match dimension of embeddings)  \n",
    "                           libr: Optional[Dict[str, int]] = None, # dictionary of form glycoletter:index\n",
    "                          ) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Instantiates and prepares a SweetNet model with specified embedding initialization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_classes : int\n",
    "        Number of unique classes for classification. (REQUIRED)\n",
    "    embeddings_dict : Optional[Dict[str, np.ndarray]], optional, default = None\n",
    "        The loaded external embeddings dictionary {glycan_word: embedding_vector}.\n",
    "        Required if initialization_method is 'external'.\n",
    "    initialization_method : {'external', 'random', 'one_hot'}, optional, default = 'external'\n",
    "        The method to initialize the embedding layer:\n",
    "        - 'external': Initialize with embeddings from embeddings_dict.\n",
    "        - 'random': Randomly initialized embeddings (train from scratch).\n",
    "        - 'one_hot': Initialize with one-hot encoded vectors.\n",
    "    trainable_embeddings : bool, optional, default = True\n",
    "        Whether the embedding layer should be trainable during training.\n",
    "    hidden_dim : int, optional, default = 320\n",
    "        Dimension of hidden layers. Must match the dimension of the embeddings\n",
    "        used if initialization_method is 'external'.\n",
    "    libr : Optional[Dict[str, int]], optional, default = None\n",
    "        Dictionary of form glycoletter:index.\n",
    "        If None, the standard glycowork library is used. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Module\n",
    "        An initialized PyTorch model (SweetNet).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        - If initialization_method is 'external' but embeddings_dict is None.\n",
    "        - If initialization_method is 'external' and embedding dimension does not match hidden_dim.\n",
    "        - If initialization_method is 'one_hot' and hidden_dim does not match library size.\n",
    "        - If initialization_method is 'random' or 'one_hot' and libr is None.\n",
    "        - If an unknown initialization_method is provided.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #  Check if libr is provided, if not, use the default library\n",
    "    if libr is None:\n",
    "        libr = lib\n",
    "\n",
    "    # Check if the required components are available in the current context\n",
    "    if 'SweetNet' not in globals() or not callable(globals()['SweetNet']) or \\\n",
    "       'init_weights' not in globals() or not callable(globals()['init_weights']):\n",
    "         raise ValueError(\"Required glycowork components (SweetNet, init_weights) not available or not callable. Please ensure they are imported correctly.\")\n",
    "\n",
    "    # Instantiate the SweetNet model\n",
    "    model = SweetNet(lib_size=len(libr), num_classes=num_classes, hidden_dim=hidden_dim)\n",
    "    print(f\"SweetNet model instantiated with lib_size={len(libr)}, num_classes={num_classes}, hidden_dim={hidden_dim}.\")\n",
    "\n",
    "\n",
    "    # Apply initial weights to all layers (embedding and non-embedding)\n",
    "    model = model.apply(lambda module: init_weights(module, mode = 'sparse')) # Experiment with 'sparse', 'xavier', or 'kaiming'\n",
    "    \n",
    "    # move model to the device (CPU or GPU)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if initialization_method == 'external':\n",
    "        print(\"Handling 'external' initialization method.\")\n",
    "        \n",
    "        # Check if embeddings_dict is provided\n",
    "        # If not, raise an error (this is a required parameter for 'external' method)\n",
    "        if embeddings_dict is None:\n",
    "            raise ValueError(\"embeddings_dict must be provided when initialization_method is 'external'.\")\n",
    "        \n",
    "        # Check that the dimension of the embeddings_dict matches hidden_dim\n",
    "        embedding_key = next(iter(embeddings_dict))\n",
    "        external_embedding_dim = embeddings_dict[embedding_key].shape[0]\n",
    "        if external_embedding_dim != hidden_dim:\n",
    "             raise ValueError(f\"External embedding dimension ({external_embedding_dim}) must match model's hidden_dim ({hidden_dim}).\")\n",
    "        \n",
    "        # Get the tensor of the embedding layer's weights. It already has initial random values.\n",
    "        embedding_tensor_to_populate = model.item_embedding.weight.data\n",
    "\n",
    "        #\n",
    "        with torch.no_grad():\n",
    "                 # Iterate through the library (which gives us the index for each glycan word)\n",
    "                 for glycan_word, index in libr.items():\n",
    "                    if glycan_word in embeddings_dict:\n",
    "\n",
    "                        # Get the embedding vector from the dictionary\n",
    "                        embedding_vector = embeddings_dict[glycan_word] \n",
    "\n",
    "                        # Convert to tensor, ensure correct dtype, and move to the same device\n",
    "                        embedding_vector_tensor = torch.tensor(embedding_vector, dtype=torch.float32).to(embedding_tensor_to_populate.device)\n",
    "\n",
    "                        # Copy the vector into the correct row of the model's embedding tensor\n",
    "                        # Relying on index from libr being valid for embedding_tensor_to_populate size\n",
    "                        embedding_tensor_to_populate[index].copy_(embedding_vector_tensor)\n",
    "                        \n",
    "                        #print(glycan_word)\n",
    "                        #print(embedding_vector)\n",
    "                    else:\n",
    "                        # If a glycan word in libr is NOT in embeddings_dict, its initial random value is preserved (for smaller dictionaries).\n",
    "                        print(f\"{glycan_word} is not in library, keeping its initial random value.\")\n",
    "                        pass # Explicitly do nothing, keeping the initial random value\n",
    "         \n",
    "\n",
    "    elif initialization_method == 'random':\n",
    "        print(\"Handling 'random' initialization method (training from scratch).\")\n",
    "        \n",
    "        # The item_embedding layer was already initialized randomly by the standard initialization loop above.\n",
    "\n",
    "        pass \n",
    "\n",
    "        \n",
    "    elif initialization_method == 'one_hot':\n",
    "        print(\" 'one_hot' initialization method not implemented yet\")\n",
    "        \n",
    "    # either I need the hidden_dim to be the same as the number of glycoletters in the library\n",
    "    # or I need to find a way to reduce the dimensionality of the one-hot encoding to match the hidden_dim\n",
    "    #or do what Roman suggested and reduce the dictionary to the 319 most common glycowords\n",
    "    # Let's tackle this later given time \n",
    "\n",
    "\n",
    "        # Determine the required embedding dimension for one-hot encoding\n",
    "        #required_hidden_dim = len(libr) + 1\n",
    "\n",
    "        # Create the one-hot embedding matrix (identity matrix)\n",
    "        #one_hot_matrix = torch.eye(required_hidden_dim, dtype=torch.float32)\n",
    "\n",
    "        # Copy one_hot_matrix to model.item_embedding.weight.data\n",
    "        #one_hot_matrix = one_hot_matrix.to(model.item_embedding.weight.device)\n",
    "\n",
    "        # Copy the one-hot matrix into the model's item_embedding.weight.data\n",
    "        #with torch.no_grad():\n",
    "           # model.item_embedding.weight.copy_(one_hot_matrix)\n",
    "            \n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        # This case should ideally be caught by the Literal type hint and docstring,\n",
    "        # but adding a runtime check is robust.\n",
    "        raise ValueError(f\"Unknown initialization_method: {initialization_method}\")\n",
    "\n",
    "\n",
    "    # Set trainability based on trainable_embeddings flag (outside the branches)\n",
    "    # This happens AFTER the initialization logic in the branches above.\n",
    "    # The logic for setting requires_grad is the same regardless of initialization method.\n",
    "    model.item_embedding.weight.requires_grad = trainable_embeddings\n",
    "    print(f\"SweetNet item_embedding layer set to trainable: {trainable_embeddings}.\")\n",
    "\n",
    "    \n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "#test = prep_infused_sweetnet(num_classes=10,embeddings_dict=glm_embeddings, initialization_method='external')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e20752",
   "metadata": {},
   "source": [
    "## Time to train a multi-class multi-label Sweetnet on our properly loaded data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdeaa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using new prep_infused_sweetnet to generate glm-infused SweetNet model\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders\n",
    "model =  prep_infused_sweetnet(\n",
    "            initialization_method = 'random',\n",
    "            num_classes = classes,\n",
    "            embeddings_dict = glm_embeddings, \n",
    "            trainable_embeddings = True\n",
    "            ) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes)\n",
    "model_ft = train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 5, mode = 'multilabel',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd57110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using new prep_infused_sweetnet to generate glm-infused SweetNet model\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders\n",
    "model =  prep_infused_sweetnet(\n",
    "            initialization_method = 'external',\n",
    "            num_classes = classes,\n",
    "            embeddings_dict = glm_embeddings, \n",
    "            trainable_embeddings = True\n",
    "            ) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.005, num_classes = classes)\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'multilabel',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glycowork.ml import model_training\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders\n",
    "model =  prep_infused_sweetnet(\n",
    "            initialization_method = 'external',\n",
    "            num_classes = classes,\n",
    "            embeddings_dict = glm_embeddings, \n",
    "            trainable_embeddings = True\n",
    "            ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.item_embedding.weight.data[3])\n",
    "print(model.item_embedding.weight.data[10])\n",
    "print(model.item_embedding.weight.data[42])\n",
    "\n",
    "print(model_ft.item_embedding.weight.data[3])\n",
    "print(model_ft.item_embedding.weight.data[10])\n",
    "print(model_ft.item_embedding.weight.data[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59cf29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using standard prep_model\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders\n",
    "model =  prep_model('SweetNet', classes) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes)\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'multilabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Try with my new glm-infused data\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders_emb\n",
    "model = prep_model('SweetNet', classes, use_external_embeddings = True) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes) #changed to 0.01 from 0.005 from 0.0005\n",
    "model_ft = train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'multilabel', use_external_embeddings = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5025834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweetnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
