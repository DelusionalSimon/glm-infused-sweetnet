{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2416a1b",
   "metadata": {},
   "source": [
    "# GLM-Infused SweetNet Experiments\n",
    "\n",
    "Experimenting with a modified version of SweetNet that allows it to take pre-trained embeddings as input. To get there I need a way to take the embeddings Iâ€™ve gotten from roman and transform them into nice inputs for the model, and a way to set the initial features using these inputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652c8f8",
   "metadata": {},
   "source": [
    "## Importing and exploring the GLM Embedding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0485fe3",
   "metadata": {},
   "source": [
    "### Exploring embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: glm_embeddings_1.pkl\n",
      "Embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# quick thing to load a pickle file\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            loaded_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of loaded object: <class 'dict'>\n",
      "Number of items (if dictionary): 2565\n",
      "Example keys (first 5): ['!GlcNAc', '-10', '-12', '-2', '-4']\n"
     ]
    }
   ],
   "source": [
    "# lets do some quick exploration\n",
    "\n",
    "# --- Explore the loaded data ---\n",
    "print(f\"Type of loaded object: {type(loaded_embeddings)}\")\n",
    "\n",
    "# Common formats for embeddings: dictionary or numpy array\n",
    "if isinstance(loaded_embeddings, dict):\n",
    "    print(f\"Number of items (if dictionary): {len(loaded_embeddings)}\")\n",
    "    # print some keys to see what they look like\n",
    "    print(f\"Example keys (first 5): {list(loaded_embeddings.keys())[:5]}\")\n",
    "elif hasattr(loaded_embeddings, 'shape'):\n",
    "    print(f\"Shape (if array/tensor): {loaded_embeddings.shape}\")\n",
    "    if hasattr(loaded_embeddings, 'dtype'):\n",
    "         print(f\"Data type (if array/tensor): {loaded_embeddings.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e6aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-6', '-8', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic', '1,5-Anhydro-Man-ol', '1,5-Anhydro-ManNAc-ol', '1,5-Anhydro-Xyl-ol', '1,5-Anhydro-Xyl2F-ol', '1-1', '1-2', '1-3', '1-4', '1-5', '1-6']\n"
     ]
    }
   ],
   "source": [
    "print(list(loaded_embeddings.keys())[5:30]) # Print more keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of value for '-8': <class 'numpy.ndarray'>\n",
      "Shape of value: (320,)\n",
      "Dtype of value: float32\n",
      "[ 9.33886290e-01 -7.57189512e-01 -5.22765040e-01  4.93726492e-01\n",
      "  3.03156078e-01 -1.72754931e+00  2.03015614e+00 -1.13539708e+00\n",
      " -8.32044244e-01 -6.09763384e-01 -5.63947335e-02 -2.68140852e-01\n",
      " -6.37493312e-01  1.45667583e-01 -7.75620103e-01 -1.39048725e-01\n",
      "  1.06042847e-01 -3.74972522e-01  7.91566074e-01 -1.03034627e+00\n",
      " -1.12639211e-01 -3.78986076e-03  5.92547238e-01  2.81559825e-01\n",
      " -5.21002829e-01  9.35327411e-01  2.56601274e-01 -3.91364455e-01\n",
      "  2.72188634e-02  5.00928342e-01 -5.55309415e-01  1.28289807e+00\n",
      " -6.45282388e-01  5.19899249e-01  6.10100806e-01  1.84122849e+00\n",
      "  3.11432898e-01 -7.64928609e-02 -1.05589128e+00  6.50653005e-01\n",
      "  9.70111132e-01  7.40227938e-01  8.39829683e-01 -3.04328918e-01\n",
      " -1.06630003e+00  4.53770608e-01  4.27673876e-01 -6.02427721e-01\n",
      "  4.39536482e-01 -1.16493046e+00 -2.04154789e-01  1.13036299e+00\n",
      "  2.51586974e-01  1.04393315e+00  2.60879964e-01  4.63881493e-02\n",
      "  8.49927664e-01 -1.21275747e+00 -5.25301337e-01 -7.54553556e-01\n",
      " -5.36846638e-01  1.71898973e+00  1.07118464e+00  1.25938666e+00\n",
      "  7.28268623e-01  2.50012755e-01 -8.84264708e-01  3.54878515e-01\n",
      " -9.51814711e-01  1.92197442e-01  6.22674108e-01 -7.19715357e-02\n",
      " -2.53418744e-01  6.10054433e-01 -1.37844992e+00  1.10613918e+00\n",
      " -7.89550483e-01  4.11728621e-01 -1.39660871e+00 -2.74130464e-01\n",
      " -4.85218346e-01 -1.64008796e+00 -2.54515797e-01 -4.76354361e-02\n",
      "  1.70321250e+00  1.37953115e+00  6.62403643e-01  1.23904690e-01\n",
      " -2.03382596e-02  2.49572158e-01 -1.19476050e-01  1.01610112e+00\n",
      "  1.54832602e-01 -3.18885893e-01  1.02479362e+00  2.19304472e-01\n",
      " -1.77515924e-01 -2.96848416e-01 -1.51161349e+00 -1.55658543e+00\n",
      "  6.01615787e-01 -1.18876457e+00  6.75462842e-01 -1.21065450e+00\n",
      "  1.00956786e+00  5.41580915e-01  4.89682317e-01 -4.31063682e-01\n",
      " -6.99561596e-01 -9.50598717e-01 -4.71236914e-01  8.96337509e-01\n",
      "  1.97975963e-01  6.51351273e-01 -1.65811467e+00 -2.37476051e-01\n",
      "  1.22424424e+00  3.85935336e-01  1.74970782e+00  1.08295810e+00\n",
      " -2.08416104e-01 -1.44780791e+00 -3.18115175e-01 -2.69204080e-02\n",
      " -7.30906725e-01  3.65380794e-01 -5.23220778e-01 -1.59638667e+00\n",
      "  9.76120412e-01  4.75375116e-01  1.10794783e+00 -9.16275680e-01\n",
      "  8.67535770e-01 -2.21260801e-01  3.58714461e-02 -1.62487292e+00\n",
      "  9.47338939e-01  2.52621353e-01 -2.44861484e-01  4.85217899e-01\n",
      " -1.72671735e-01  1.49431840e-01 -9.26872373e-01 -6.38668120e-01\n",
      " -1.37115136e-01  1.30791855e+00  1.25448748e-01  3.05962026e-01\n",
      " -2.51638025e-01  6.88706279e-01 -6.43941760e-01  6.10008895e-01\n",
      "  2.45932966e-01  1.53176570e+00 -2.05617994e-01  5.01646757e-01\n",
      " -4.11370814e-01 -5.36742508e-01 -1.23477876e-02  6.50121808e-01\n",
      " -3.78578186e-01  6.62264466e-01  1.53327346e-01 -9.97333288e-01\n",
      "  2.86916673e-01 -3.98133188e-01  1.19174033e-01 -1.07086766e+00\n",
      "  5.68605885e-02  8.55352730e-02 -2.43456244e-01 -5.13940752e-01\n",
      "  9.52608764e-01 -3.56329709e-01 -9.76832956e-02  1.55454218e-01\n",
      "  1.07665420e-01  7.78901517e-01  1.94103813e+00  5.98729789e-01\n",
      "  1.49250478e-01  6.60319090e-01 -9.16693985e-01 -1.80390513e+00\n",
      " -1.08837974e+00 -5.85823774e-01 -5.17625034e-01  1.13187218e+00\n",
      " -3.11186165e-01 -1.56313211e-01  4.89638031e-01  6.32191420e-01\n",
      " -9.01452422e-01  3.40963513e-01  3.77618819e-01  4.78747129e-01\n",
      " -1.26142776e+00  1.63014054e+00 -7.38181099e-02 -8.88819635e-01\n",
      " -9.81908560e-01 -3.11309278e-01 -2.87041283e+00 -6.68797910e-01\n",
      "  1.15292573e+00  1.82262063e+00  6.86679184e-01  3.54639411e-01\n",
      "  1.14279723e+00  1.23592412e+00 -4.26488072e-01  5.78116417e-01\n",
      "  2.67315298e-01  1.73516899e-01 -6.95198655e-01 -7.84443021e-01\n",
      "  1.87699527e-01  7.76465774e-01  1.17747712e+00  2.98208922e-01\n",
      "  1.80739570e+00 -6.55146241e-02  2.10267353e+00 -1.49224257e+00\n",
      "  1.67633876e-01 -5.96812427e-01  4.02143002e-01 -5.80711842e-01\n",
      " -6.86030865e-01  2.82077312e-01  4.62324202e-01 -8.51680398e-01\n",
      " -6.37305975e-01 -1.97909772e-01  8.27008903e-01 -2.47440666e-01\n",
      "  5.40550411e-01  2.20697820e-02 -3.67172241e-01  1.37753654e+00\n",
      "  2.57560164e-01  1.12044883e+00  1.47008979e+00 -3.09366286e-01\n",
      "  1.41206241e+00 -1.07911384e+00 -3.82883579e-01  1.15288660e-01\n",
      "  6.46931171e-01 -1.63524508e+00 -4.82143342e-01 -2.22676694e-02\n",
      " -2.94011176e-01  1.76649165e+00 -1.42879653e+00 -1.01673603e+00\n",
      "  6.92535341e-01  1.08943865e-01 -1.51619220e+00 -1.31418991e+00\n",
      " -5.36556542e-01 -9.08092409e-02 -3.43192220e-02 -5.01663029e-01\n",
      " -4.27816272e-01  5.04320741e-01 -8.19638968e-01  1.27975166e-01\n",
      "  6.98855758e-01  4.11748588e-01 -2.63869703e-01 -1.72789741e+00\n",
      "  2.40177006e-01 -3.30802739e-01  1.47785515e-01  4.70187128e-01\n",
      "  3.38367313e-01 -1.54152012e+00  3.17173868e-01 -1.70832485e-01\n",
      "  9.85031009e-01 -1.51257575e+00  7.86181986e-01  2.95546353e-01\n",
      "  4.57608998e-02 -6.43859148e-01  4.83155847e-01 -1.51108074e+00\n",
      " -1.82736918e-01 -3.47120881e-01 -5.70403397e-01 -1.21720120e-01\n",
      " -1.61197579e+00  1.02913380e-03 -4.93016541e-02 -1.70051694e+00\n",
      " -4.81017500e-01 -9.90746021e-01  3.51191968e-01 -6.38143182e-01\n",
      "  8.80924284e-01  1.06428635e+00 -1.31740403e+00 -1.46576715e+00\n",
      " -8.72395873e-01  1.48068953e+00 -2.76599586e-01 -1.15330029e+00\n",
      "  1.45732999e-01 -1.63671541e+00  2.22910285e-01 -3.31862628e-01\n",
      "  5.65533102e-01 -4.64938819e-01  1.83547580e+00 -7.03186333e-01\n",
      "  2.57217407e-01  1.83000445e+00  1.64521456e-01  1.26764941e+00]\n"
     ]
    }
   ],
   "source": [
    "example_key = '!GlcNAc' \n",
    "if example_key in loaded_embeddings:\n",
    "    embedding_vector = loaded_embeddings[example_key]\n",
    "    print(f\"Type of value for '{example_key}': {type(embedding_vector)}\")\n",
    "    if hasattr(embedding_vector, 'shape'):\n",
    "        print(f\"Shape of value: {embedding_vector.shape}\") # This gives dimensionality!\n",
    "        print(f\"Dtype of value: {embedding_vector.dtype}\")\n",
    "    print(embedding_vector) # Print the vector itself if it's not too long\n",
    "else:\n",
    "    print(f\"Key '{example_key}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7f66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'other': 122, 'linkage_or_modification': 36, 'monosaccharide': 2407})\n"
     ]
    }
   ],
   "source": [
    "# let's look at the keys a bit more closely\n",
    "\n",
    "import collections\n",
    "\n",
    "key_types = collections.defaultdict(int)\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        key_types['linkage_or_modification'] += 1\n",
    "    elif key[0].isalpha():\n",
    "        key_types['monosaccharide'] += 1\n",
    "    else:\n",
    "        key_types['other'] += 1\n",
    "\n",
    "print(key_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd6a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'other' keys: 122\n",
      "Examples of 'other' keys: ['!GlcNAc', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic', '1,5-Anhydro-Man-ol', '1,5-Anhydro-ManNAc-ol', '1,5-Anhydro-Xyl-ol', '1,5-Anhydro-Xyl2F-ol', '1b-4', '1dAlt-ol']\n"
     ]
    }
   ],
   "source": [
    "# Let's explore those Other keys \n",
    "\n",
    "other_keys = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        other_keys.append(key)\n",
    "\n",
    "print(f\"Number of 'other' keys: {len(other_keys)}\")\n",
    "print(f\"Examples of 'other' keys: {other_keys[:20]}\") # Print the first 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398bd9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More Examples of 'other' keys: ['1dEry-ol', '2,3-Anhydro-All', '2,3-Anhydro-Man', '2,3-Anhydro-Rib', '2,5-Anhydro-D-Alt-ol', '2,5-Anhydro-D-Alt3S-ol', '2,5-Anhydro-D-Tal', '2,5-Anhydro-Glc', '2,5-Anhydro-L-Man-ol', '2,5-Anhydro-Man', '2,5-Anhydro-Man-ol', '2,5-Anhydro-Man1S-ol', '2,5-Anhydro-Man3S-ol', '2,5-Anhydro-Man6S', '2,5-Anhydro-Tal-ol', '2,5-Anhydro-Tal6P', '2,6-Anhydro-Glc5NAc-ol', '2,6-Anhydro-L-Gul-ol', '2,6-Anhydro-L-Gul-onic', '2,6-Anhydro-Man-ol', '2,6-Anhydro-Tal5NAc-ol', '2,7-Anhydro-Kdo', '2,7-Anhydro-Kdof', '2dAraHexA', '3,6-Anhydro-Fruf', '3,6-Anhydro-Gal', '3,6-Anhydro-Gal2S', '3,6-Anhydro-Glc', '3,6-Anhydro-L-Gal', '3,6-Anhydro-L-Gal2Me', '3-Anhydro-Gal', '3-Anhydro-Gal2S', '3dFuc', '3dGal', '3dLyxHep-ulosaric', '4,7-Anhydro-Kdo', '4,7-Anhydro-KdoOPEtN', '4,8-Anhydro-Kdo', '4d8dNeu5Ac', '4dAraHex', '4dEry-ol', '4dFuc', '4dGal', '4dNeu5Ac', '4dThrHexNAcA4en', '4eLeg5Ac7Ac', '5dAraf', '5dAraf3Me', '5dLyxf3CFo', '5dLyxf3CMe']\n"
     ]
    }
   ],
   "source": [
    "# Let's look at 50 more keys\n",
    "\n",
    "print(f\"More Examples of 'other' keys: {other_keys[20:70]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3a766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'monosaccharide' keys: 2407\n",
      "Examples of 'monosaccharide' keys: ['Abe', 'Abe1PP', 'Abe2Ac', 'AbeOAc', 'Acarbose', 'AcefA', 'Aci5Ac7Ac', 'AcoNAc', 'All', 'All-ol', 'All1S2S3S4S', 'All2Ac3Ac', 'All2S3S4S', 'All3Ac', 'All6Ac', 'AllN', 'AllN1P', 'AllNAc', 'AllNAc6Me', 'AllOMe', 'Alt', 'AltA', 'AltA2N', 'AltA2S', 'AltAN', 'AltNAc', 'AltNAcA', 'AltNAcA1Prop', 'Altf', 'AltfOAc', 'Amikacin', 'Api', 'ApiOAc', 'ApiOMe-ol', 'Apif', 'Ara', 'Ara-ol', 'Ara1Cer2Ac', 'Ara1Me', 'Ara1N4P', 'Ara1P4N', 'Ara1PP', 'Ara1PP2NAc', 'Ara1PP4N', 'Ara1PP4NFo', 'Ara2Ac', 'Ara2Ac3Ac', 'Ara2Ac3Ac4Ac', 'Ara2Ac4Ac', 'Ara2Ac5P-ol']\n"
     ]
    }
   ],
   "source": [
    "# Let's explore those monosaccharide keys\n",
    "monosaccharide = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        monosaccharide.append(key)\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'monosaccharide' keys: {len(monosaccharide)}\")\n",
    "print(f\"Examples of 'monosaccharide' keys: {monosaccharide[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ddd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'linkage_or_modification' keys: 36\n",
      "Examples of 'linkage_or_modification' keys: ['-10', '-12', '-2', '-4', '-6', '-8', '1-1', '1-2', '1-3', '1-4', '1-5', '1-6', '1-?', '2-3', '2-4', '2-5', '2-6', '3-1', '3-5', '4-1', '4-5', '5-1', '5-2', '5-3', '5-4', '5-5', '5-6', '6-1', '6-3', '6-4', '?1-2', '?1-3', '?1-4', '?1-6', '?1-?', '?2-?']\n"
     ]
    }
   ],
   "source": [
    "# To be throughough, let's look at 50 Linkage or Modification keys as well\n",
    "linkage_or_modification = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        linkage_or_modification.append(key)\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'linkage_or_modification' keys: {len(linkage_or_modification)}\")\n",
    "print(f\"Examples of 'linkage_or_modification' keys: {linkage_or_modification[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "df823d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All embeddings in the loaded dictionary appear to be the same.\n",
      "Number of embeddings in the dictionary: 2565\n",
      "First embedding:\n",
      "[ 9.33886290e-01 -7.57189512e-01 -5.22765040e-01  4.93726492e-01\n",
      "  3.03156078e-01 -1.72754931e+00  2.03015614e+00 -1.13539708e+00\n",
      " -8.32044244e-01 -6.09763384e-01 -5.63947335e-02 -2.68140852e-01\n",
      " -6.37493312e-01  1.45667583e-01 -7.75620103e-01 -1.39048725e-01\n",
      "  1.06042847e-01 -3.74972522e-01  7.91566074e-01 -1.03034627e+00\n",
      " -1.12639211e-01 -3.78986076e-03  5.92547238e-01  2.81559825e-01\n",
      " -5.21002829e-01  9.35327411e-01  2.56601274e-01 -3.91364455e-01\n",
      "  2.72188634e-02  5.00928342e-01 -5.55309415e-01  1.28289807e+00\n",
      " -6.45282388e-01  5.19899249e-01  6.10100806e-01  1.84122849e+00\n",
      "  3.11432898e-01 -7.64928609e-02 -1.05589128e+00  6.50653005e-01\n",
      "  9.70111132e-01  7.40227938e-01  8.39829683e-01 -3.04328918e-01\n",
      " -1.06630003e+00  4.53770608e-01  4.27673876e-01 -6.02427721e-01\n",
      "  4.39536482e-01 -1.16493046e+00 -2.04154789e-01  1.13036299e+00\n",
      "  2.51586974e-01  1.04393315e+00  2.60879964e-01  4.63881493e-02\n",
      "  8.49927664e-01 -1.21275747e+00 -5.25301337e-01 -7.54553556e-01\n",
      " -5.36846638e-01  1.71898973e+00  1.07118464e+00  1.25938666e+00\n",
      "  7.28268623e-01  2.50012755e-01 -8.84264708e-01  3.54878515e-01\n",
      " -9.51814711e-01  1.92197442e-01  6.22674108e-01 -7.19715357e-02\n",
      " -2.53418744e-01  6.10054433e-01 -1.37844992e+00  1.10613918e+00\n",
      " -7.89550483e-01  4.11728621e-01 -1.39660871e+00 -2.74130464e-01\n",
      " -4.85218346e-01 -1.64008796e+00 -2.54515797e-01 -4.76354361e-02\n",
      "  1.70321250e+00  1.37953115e+00  6.62403643e-01  1.23904690e-01\n",
      " -2.03382596e-02  2.49572158e-01 -1.19476050e-01  1.01610112e+00\n",
      "  1.54832602e-01 -3.18885893e-01  1.02479362e+00  2.19304472e-01\n",
      " -1.77515924e-01 -2.96848416e-01 -1.51161349e+00 -1.55658543e+00\n",
      "  6.01615787e-01 -1.18876457e+00  6.75462842e-01 -1.21065450e+00\n",
      "  1.00956786e+00  5.41580915e-01  4.89682317e-01 -4.31063682e-01\n",
      " -6.99561596e-01 -9.50598717e-01 -4.71236914e-01  8.96337509e-01\n",
      "  1.97975963e-01  6.51351273e-01 -1.65811467e+00 -2.37476051e-01\n",
      "  1.22424424e+00  3.85935336e-01  1.74970782e+00  1.08295810e+00\n",
      " -2.08416104e-01 -1.44780791e+00 -3.18115175e-01 -2.69204080e-02\n",
      " -7.30906725e-01  3.65380794e-01 -5.23220778e-01 -1.59638667e+00\n",
      "  9.76120412e-01  4.75375116e-01  1.10794783e+00 -9.16275680e-01\n",
      "  8.67535770e-01 -2.21260801e-01  3.58714461e-02 -1.62487292e+00\n",
      "  9.47338939e-01  2.52621353e-01 -2.44861484e-01  4.85217899e-01\n",
      " -1.72671735e-01  1.49431840e-01 -9.26872373e-01 -6.38668120e-01\n",
      " -1.37115136e-01  1.30791855e+00  1.25448748e-01  3.05962026e-01\n",
      " -2.51638025e-01  6.88706279e-01 -6.43941760e-01  6.10008895e-01\n",
      "  2.45932966e-01  1.53176570e+00 -2.05617994e-01  5.01646757e-01\n",
      " -4.11370814e-01 -5.36742508e-01 -1.23477876e-02  6.50121808e-01\n",
      " -3.78578186e-01  6.62264466e-01  1.53327346e-01 -9.97333288e-01\n",
      "  2.86916673e-01 -3.98133188e-01  1.19174033e-01 -1.07086766e+00\n",
      "  5.68605885e-02  8.55352730e-02 -2.43456244e-01 -5.13940752e-01\n",
      "  9.52608764e-01 -3.56329709e-01 -9.76832956e-02  1.55454218e-01\n",
      "  1.07665420e-01  7.78901517e-01  1.94103813e+00  5.98729789e-01\n",
      "  1.49250478e-01  6.60319090e-01 -9.16693985e-01 -1.80390513e+00\n",
      " -1.08837974e+00 -5.85823774e-01 -5.17625034e-01  1.13187218e+00\n",
      " -3.11186165e-01 -1.56313211e-01  4.89638031e-01  6.32191420e-01\n",
      " -9.01452422e-01  3.40963513e-01  3.77618819e-01  4.78747129e-01\n",
      " -1.26142776e+00  1.63014054e+00 -7.38181099e-02 -8.88819635e-01\n",
      " -9.81908560e-01 -3.11309278e-01 -2.87041283e+00 -6.68797910e-01\n",
      "  1.15292573e+00  1.82262063e+00  6.86679184e-01  3.54639411e-01\n",
      "  1.14279723e+00  1.23592412e+00 -4.26488072e-01  5.78116417e-01\n",
      "  2.67315298e-01  1.73516899e-01 -6.95198655e-01 -7.84443021e-01\n",
      "  1.87699527e-01  7.76465774e-01  1.17747712e+00  2.98208922e-01\n",
      "  1.80739570e+00 -6.55146241e-02  2.10267353e+00 -1.49224257e+00\n",
      "  1.67633876e-01 -5.96812427e-01  4.02143002e-01 -5.80711842e-01\n",
      " -6.86030865e-01  2.82077312e-01  4.62324202e-01 -8.51680398e-01\n",
      " -6.37305975e-01 -1.97909772e-01  8.27008903e-01 -2.47440666e-01\n",
      "  5.40550411e-01  2.20697820e-02 -3.67172241e-01  1.37753654e+00\n",
      "  2.57560164e-01  1.12044883e+00  1.47008979e+00 -3.09366286e-01\n",
      "  1.41206241e+00 -1.07911384e+00 -3.82883579e-01  1.15288660e-01\n",
      "  6.46931171e-01 -1.63524508e+00 -4.82143342e-01 -2.22676694e-02\n",
      " -2.94011176e-01  1.76649165e+00 -1.42879653e+00 -1.01673603e+00\n",
      "  6.92535341e-01  1.08943865e-01 -1.51619220e+00 -1.31418991e+00\n",
      " -5.36556542e-01 -9.08092409e-02 -3.43192220e-02 -5.01663029e-01\n",
      " -4.27816272e-01  5.04320741e-01 -8.19638968e-01  1.27975166e-01\n",
      "  6.98855758e-01  4.11748588e-01 -2.63869703e-01 -1.72789741e+00\n",
      "  2.40177006e-01 -3.30802739e-01  1.47785515e-01  4.70187128e-01\n",
      "  3.38367313e-01 -1.54152012e+00  3.17173868e-01 -1.70832485e-01\n",
      "  9.85031009e-01 -1.51257575e+00  7.86181986e-01  2.95546353e-01\n",
      "  4.57608998e-02 -6.43859148e-01  4.83155847e-01 -1.51108074e+00\n",
      " -1.82736918e-01 -3.47120881e-01 -5.70403397e-01 -1.21720120e-01\n",
      " -1.61197579e+00  1.02913380e-03 -4.93016541e-02 -1.70051694e+00\n",
      " -4.81017500e-01 -9.90746021e-01  3.51191968e-01 -6.38143182e-01\n",
      "  8.80924284e-01  1.06428635e+00 -1.31740403e+00 -1.46576715e+00\n",
      " -8.72395873e-01  1.48068953e+00 -2.76599586e-01 -1.15330029e+00\n",
      "  1.45732999e-01 -1.63671541e+00  2.22910285e-01 -3.31862628e-01\n",
      "  5.65533102e-01 -4.64938819e-01  1.83547580e+00 -7.03186333e-01\n",
      "  2.57217407e-01  1.83000445e+00  1.64521456e-01  1.26764941e+00]\n"
     ]
    }
   ],
   "source": [
    "# wait, a couple of weeks later and I couldn't figure out why my glm-infused model wasn't converging, \n",
    "# and I just realised that the embeddings were all the same, no wonder it didn't work. \n",
    "# I should have looked at the embeddings themselves, not just the keys.\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    loaded_embeddings = pickle.load(f)\n",
    "\n",
    "first_embedding = None\n",
    "all_same = True\n",
    "if loaded_embeddings:\n",
    "    first_key = next(iter(loaded_embeddings))\n",
    "    first_embedding = loaded_embeddings[first_key]\n",
    "    for key, embedding in loaded_embeddings.items():\n",
    "        if not (embedding == first_embedding).all():\n",
    "            all_same = False\n",
    "            print(f\"Found a different embedding for key: {key}\")\n",
    "            break\n",
    "\n",
    "if all_same and first_embedding is not None:\n",
    "    print(\"All embeddings in the loaded dictionary appear to be the same.\")\n",
    "elif first_embedding is None:\n",
    "    print(\"The embedding dictionary is empty.\")\n",
    "else:\n",
    "    print(\"Embeddings in the dictionary are not all the same.\")\n",
    "\n",
    "print(f\"Number of embeddings in the dictionary: {len(loaded_embeddings)}\")\n",
    "if first_embedding is not None:\n",
    "    print(f\"First embedding:\")\n",
    "    print(first_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9ad5a",
   "metadata": {},
   "source": [
    "### Load the glycowork libr\n",
    "\n",
    "I'll load the glycowork library and compare it to the keys in the embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b14506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in glycowork vocabulary: 2565\n",
      "Example keys from glycowork vocabulary (first 20): ['!GlcNAc', '-10', '-12', '-2', '-4', '-6', '-8', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic']\n"
     ]
    }
   ],
   "source": [
    "from glycowork.glycan_data import loader\n",
    "\n",
    "glycowork_vocabulary = loader.lib\n",
    "\n",
    "print(f\"Number of items in glycowork vocabulary: {len(glycowork_vocabulary)}\")\n",
    "print(f\"Example keys from glycowork vocabulary (first 20): {list(glycowork_vocabulary.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e1f3d",
   "metadata": {},
   "source": [
    "Nice, they seem to correspond one to one!\n",
    "\n",
    "That saves me a lot of work down the line (Thanks Roman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of value for '-10': <class 'int'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# let's look at one of the keys in the glycowork vocabulary to see what they return\n",
    "example_glycowork_key = '-10'\n",
    "if example_glycowork_key in glycowork_vocabulary:\n",
    "    glycowork_value = glycowork_vocabulary[example_glycowork_key]\n",
    "    print(f\"Type of value for '{example_glycowork_key}': {type(glycowork_value)}\")\n",
    "    print(glycowork_value)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f827a0",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54413ae2",
   "metadata": {},
   "source": [
    "### Load, filter, and transform glycowork data into glycan_loaders ||||run on kernel restart||||\n",
    "This is used to load and filter glycowork data for a specific prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b4d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the full sugarbase dataset and make a custom dataframe for the problem you want to tackle\n",
    "\n",
    "from glycowork.glycan_data.loader import build_custom_df, df_glycan\n",
    "from glycowork.ml.train_test_split import prepare_multilabel\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "def build_multilabel_dataset(glycan_dataset: str = 'df_species',\n",
    "                          glycan_class: str = 'Kingdom',\n",
    "                          min_class_size: int = 6) -> Tuple[List[str], List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Loads glycan data, prepares it for multi-label classification,\n",
    "    and removes classes with fewer than min_class_size samples.\n",
    "\n",
    "    Args:\n",
    "        glycan_dataset: The glycowork dataset to use. Options include: \n",
    "            'df_species', 'df_tissue', and 'df_disease'.            \n",
    "        glycan_class: The class to predict. Options include:\n",
    "            df_species: 'Species', 'Genus', 'Family', 'Order', 'Class', 'Phylum', 'Kingdom', 'Domain', 'ref'\n",
    "            df_tissue:  'tissue_sample', 'tissue_species', 'tissue_id', 'tissue_ref'\n",
    "            df_disease: 'disease_association', 'disease_sample', 'disease_direction', 'disease_species', 'disease_id', 'disease_ref'\n",
    "        min_class_size: Minimum number of samples required for a class to be included. Set to 1 to include all classes.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - filtered_glycans: List of glycans after filtering rare classes.\n",
    "        - filtered_labels: List of corresponding label vectors.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    all_glycan_data = df_glycan\n",
    "\n",
    "    # Build custom dataframe\n",
    "    custom_glycan_df = build_custom_df(all_glycan_data, glycan_dataset)\n",
    "\n",
    "    # Prepare for multi-label prediction\n",
    "    glycans, labels = prepare_multilabel(custom_glycan_df, glycan_class)\n",
    "\n",
    "    # if needed, removes classes with fewer than min_class_size samples.\n",
    "    if(min_class_size > 1):\n",
    "\n",
    "        # Convert labels to string representation for counting\n",
    "        label_strings = [''.join(map(str, label)) for label in labels]\n",
    "\n",
    "        # Count occurrences of each label combination\n",
    "        label_counts = Counter(label_strings)\n",
    "\n",
    "        # Filter glycans and labels based on class size\n",
    "        filtered_glycans = [glycans[i] for i, label_str in enumerate(label_strings) if label_counts[label_str] >= min_class_size]\n",
    "        filtered_labels = [labels[i] for i, label_str in enumerate(label_strings) if label_counts[label_str] >= min_class_size]\n",
    "        print(f\"Number of unique glycans left after filtering rare classes (size >= {min_class_size}): {len(filtered_glycans)}/{len(glycans)}\")\n",
    "\n",
    "    else:\n",
    "        filtered_glycans = glycans\n",
    "        filtered_labels = labels\n",
    "        print(f\"Number of unique glycans: {len(filtered_glycans)}\")\n",
    "\n",
    "    return filtered_glycans, filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd78302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique glycans left after filtering rare classes (size >= 6): 23355/23503\n"
     ]
    }
   ],
   "source": [
    "glycans, labels = build_multilabel_dataset(glycan_dataset='df_species', glycan_class='Kingdom', min_class_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c440705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique glycans: 23355\n",
      "Number of label vectors: 23355\n",
      "Shape of first label vector (number of members in class): 21\n",
      "\n",
      "First 5 glycans:\n",
      "['[Galf(b1-6)Galf(b1-2)]Man(a1-6)[Gal(a1-6)Galf(b1-2)]Man(a1-6)[Gal(a1-6)Galf(b1-2)]Man(a1-6)Man', 'Fuc(a1-2)Gal(b1-?)GlcNAc(b1-6)[Fuc(a1-?)[Gal(b1-?)]GlcNAc(b1-3)]Gal(b1-3)[GlcNAc(b1-6)]GalNAc', 'Man(a1-3)Man(a1-6)[Man(a1-3)]Man(b1-4)GlcNAc(b1-4)[Fuc(a1-3)]GlcNAc', 'Galf(b1-2)[Man(a1-3)][Man(a1-6)]Man(a1-6)[Man(a1-3)]Man(b1-4)GlcNAc(b1-4)GlcNAc', '[GlcNAcA3NAc4Ac(b1-4)][GlcNAc(b1-6)]Gal(a1-6)Glc(b1-3)GalNAc(b1-3)Gal']\n",
      "\n",
      "First 5 label vectors:\n",
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "# quick inspection of the data\n",
    "\n",
    "print(f\"Number of unique glycans: {len(glycans)}\")\n",
    "print(f\"Number of label vectors: {len(labels)}\")\n",
    "print(f\"Shape of first label vector (number of members in class): {len(labels[0])}\")\n",
    "print(f\"\\nFirst 5 glycans:\\n{glycans[:5]}\")\n",
    "print(f\"\\nFirst 5 label vectors:\\n{labels[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91ff9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and testing sets using StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import List, Union\n",
    "\n",
    "def multilabel_split(glycans: List[str], # list of IUPAC-condensed glycans\n",
    "                 labels: List[Union[float, int, str]], # list of prediction labels\n",
    "                 train_size: float = 0.7, # size of train set, the rest is split into validation and test sets\n",
    "                 random_state: int = 42 # random state for reproducibility\n",
    "                )-> Tuple[List[str], List[str], List[str], List[List[float]], List[List[float]], List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets using StratifiedShuffleSplit.\n",
    "    \n",
    "    Args:\n",
    "        glycans: List of glycans.\n",
    "        labels: List of label vectors.\n",
    "        train_size: Proportion of the dataset to include in the validation and test split.\n",
    "        random_state: Controls the randomness of the split.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - train_glycans: Training set of glycans.\n",
    "            - val_glycans: Validation set of glycans.\n",
    "            - test_glycans: Testing set of glycans.\n",
    "            - train_labels: Training set of labels.\n",
    "            - val_labels: Validation set of labels.\n",
    "            - test_labels: Testing set of labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert labels to a suitable format for stratification (string representation)\n",
    "    label_strings = [''.join(map(str, label)) for label in labels]\n",
    "\n",
    "    # Initial split for train vs. (val + test)\n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = 1 - train_size, random_state = random_state)\n",
    "    train_index, temp_index = next(sss.split(glycans, label_strings))\n",
    "    train_glycans = [glycans[i] for i in train_index]\n",
    "    train_labels = [labels[i] for i in train_index]\n",
    "    temp_glycans = [glycans[i] for i in temp_index]\n",
    "    temp_labels = [labels[i] for i in temp_index]\n",
    "\n",
    "    # Split the remaining (val + test) into validation and test sets\n",
    "    sss_val_test = StratifiedShuffleSplit(n_splits = 1, test_size = 0.5, random_state = random_state)\n",
    "    val_index, test_index = next(sss_val_test.split(temp_glycans, [''.join(map(str, label)) for label in temp_labels]))\n",
    "    val_glycans = [temp_glycans[i] for i in val_index]\n",
    "    val_labels = [temp_labels[i] for i in val_index]\n",
    "    test_glycans = [temp_glycans[i] for i in test_index]\n",
    "    test_labels = [temp_labels[i] for i in test_index]\n",
    "\n",
    "    print(\"Split complete!\")\n",
    "    print(f\"Train set size: {len(train_glycans)}\")\n",
    "    print(f\"Validation set size: {len(val_glycans)}\")\n",
    "    print(f\"Test set size: {len(test_glycans)}\")\n",
    "        \n",
    "    return train_glycans, val_glycans, test_glycans, train_labels, val_labels, test_labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4723022a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete!\n",
      "Train set size: 16348\n",
      "Validation set size: 3503\n",
      "Test set size: 3504\n"
     ]
    }
   ],
   "source": [
    "# Lets use the split function\n",
    "train_glycans, val_glycans, test_glycans, train_labels, val_labels, test_labels = multilabel_split(glycans, labels, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms IUPAC into graphs and makes the data loaders for the training and validation sets\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "\n",
    "glycan_loaders = split_data_to_train(\n",
    "    glycan_list_train = train_glycans,\n",
    "    glycan_list_val = val_glycans,\n",
    "    labels_train = train_labels,\n",
    "    labels_val = val_labels,\n",
    "    batch_size = 128,  # Started with 32 Adjust as needed\n",
    "    drop_last = False,\n",
    "    augment_prob = 0.0,  # Adjust if you want augmentation for training\n",
    "    generalization_prob = 0.2  # Adjust if you want generalization for training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae2f77",
   "metadata": {},
   "source": [
    "### GIFFLAR Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a5ce2",
   "metadata": {},
   "source": [
    "#### Load GIFFLAR Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the GIFFLAR dataset for the Taxonomy Kingdom (takes a long time to run(~40m), just use the file it generated)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../GIFFLAR') \n",
    "\n",
    "from gifflar.benchmarks import get_dataset\n",
    "import pathlib\n",
    "\n",
    "data_config_kingdom = {\"name\": \"Taxonomy_Kingdom\"}\n",
    "root_dir = pathlib.Path(\"./data_gifflar\")  # Choose a directory to save the data\n",
    "root_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "kingdom_dataset_config = get_dataset(data_config_kingdom, root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c4b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Taxonomy_Kingdom', 'filepath': WindowsPath('data_gifflar/taxonomy_Kingdom.tsv')}\n"
     ]
    }
   ],
   "source": [
    "# Print the dataset configuration\n",
    "print(kingdom_dataset_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a488a",
   "metadata": {},
   "source": [
    "#### Load and transform GIFFLAR Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "339c831f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               IUPAC  Amoebozoa  Animalia  \\\n",
      "0  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "1  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "2  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "3  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "4  3,6-Anhydro-L-Gal(a1-3)GalOMe(b1-4)3,6-Anhydro...          0         0   \n",
      "\n",
      "   Bacteria  Bamfordvirae  Chromista  Euryarchaeota  Excavata  Fungi  \\\n",
      "0         1             0          0              0         0      0   \n",
      "1         1             0          0              0         0      0   \n",
      "2         1             0          0              0         0      0   \n",
      "3         1             0          0              0         0      0   \n",
      "4         1             0          0              0         0      0   \n",
      "\n",
      "   Heunggongvirae  Metazoa  Orthornavirae  Pararnavirae  Plantae  Protista  \\\n",
      "0               0        0              0             0        0         0   \n",
      "1               0        0              0             0        0         0   \n",
      "2               0        0              0             0        0         0   \n",
      "3               0        0              0             0        0         0   \n",
      "4               0        0              0             0        0         0   \n",
      "\n",
      "   Riboviria  split  \n",
      "0          0  train  \n",
      "1          0  train  \n",
      "2          0  train  \n",
      "3          0    val  \n",
      "4          0   test  \n",
      "Shape of the DataFrame: (16452, 17)\n"
     ]
    }
   ],
   "source": [
    "# Lets load the dataset into a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "filepath = 'data_gifflar/taxonomy_Kingdom.tsv'\n",
    "#filepath = kingdom_dataset_config['filepath'] #If you've loaded it recently, which you shouldn't\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "multilabel_kingdom_df = pd.read_csv(filepath, sep=\"\\t\")\n",
    "# Display the first few rows of the DataFrame\n",
    "print(multilabel_kingdom_df.head())\n",
    "# Display the shape of the DataFrame\n",
    "print(f\"Shape of the DataFrame: {multilabel_kingdom_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b5a99",
   "metadata": {},
   "source": [
    "That looks fine\n",
    "\n",
    "Let's convert them into graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f975f366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['IUPAC', 'Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae',\n",
      "       'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae',\n",
      "       'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista',\n",
      "       'Riboviria', 'split'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(multilabel_kingdom_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f37ce3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's try the prepare multilabel function from the train_test_split module\n",
    "\n",
    "from glycowork.ml.train_test_split import prepare_multilabel\n",
    "\n",
    "# Prepare the multilabel dataset using the melt function\n",
    "# The melt function is used to transform the DataFrame from wide format to long format\n",
    "kingdom_df_melted = multilabel_kingdom_df.melt(\n",
    "    id_vars=['IUPAC', 'split'],\n",
    "    value_vars=['Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae', 'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae', 'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista', 'Riboviria'],\n",
    "    var_name='Kingdom',\n",
    "    value_name='Association'\n",
    ")\n",
    "\n",
    "# Filter for associations where the glycan belongs to the kingdom (Association == 1)\n",
    "kingdom_df_melted = kingdom_df_melted[kingdom_df_melted['Association'] == 1]\n",
    "\n",
    "# Splitting the dataset using the 'split' column\n",
    "train_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'train'].drop(columns=['split'])\n",
    "val_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'val'].drop(columns=['split'])\n",
    "test_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'test'].drop(columns=['split'])\n",
    "\n",
    "# Finally using the prepare_multilabel function to prepare the data for training\n",
    "glycan_train, label_train = prepare_multilabel(train_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "glycan_val, label_val = prepare_multilabel(val_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "glycan_test, label_test = prepare_multilabel(test_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b68ad291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make them into graphs again, but hyper efficiently this time\n",
    "\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "\n",
    "multilabel_kingdom_loaders = split_data_to_train(\n",
    "    glycan_list_train=glycan_train,\n",
    "    glycan_list_val=glycan_val,\n",
    "    labels_train=label_train,\n",
    "    labels_val=label_val,\n",
    "    batch_size=32,  # Adjust as needed\n",
    "    drop_last=False,\n",
    "    augment_prob=0.0,  # Adjust if you want augmentation for training\n",
    "    generalization_prob=0.0  # Adjust if you want generalization for training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26e9af",
   "metadata": {},
   "source": [
    "#### Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2104e79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training glycans: 11593\n",
      "Number of validation glycans: 3213\n",
      "Number of test glycans: 1646\n",
      "Shape of training labels: 11593 x 15\n",
      "Shape of validation labels: 3213 x 15\n",
      "Shape of test labels: 1646 x 15\n",
      "--- Checking example from train set (Direct) ---\n",
      "Glycan: 3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-Gal(a1-3)Gal\n",
      "Split in original data: train\n",
      "Labels in split data: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Labels in original data: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Labels match!\n",
      "--- Checking example from val set (Direct) ---\n",
      "Glycan: 3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-Gal(a1-3)GalOMe(b1-4)3,6-Anhydro-L-Gal(a1-3)Gal\n",
      "Split in original data: val\n",
      "Labels in split data: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Labels in original data: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Labels match!\n",
      "--- Checking example from test set (Direct) ---\n",
      "Glycan: 3,6-Anhydro-L-Gal(a1-3)GalOMe(b1-4)3,6-Anhydro-L-Gal(a1-3)Gal\n",
      "Split in original data: test\n",
      "Labels in split data: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Labels in original data: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Labels match!\n"
     ]
    }
   ],
   "source": [
    "# Let's validate the split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "filepath = 'data_gifflar/taxonomy_Kingdom.tsv'\n",
    "\n",
    "# 1. Load the original DataFrame\n",
    "multilabel_kingdom_df_original = pd.read_csv(filepath, sep=\"\\t\")\n",
    "kingdom_cols = ['Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae', 'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae', 'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista', 'Riboviria']\n",
    "\n",
    "# 2. Split the original DataFrame by 'split'\n",
    "train_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'train']\n",
    "val_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'val']\n",
    "test_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'test']\n",
    "\n",
    "# 3. Extract glycans and labels directly\n",
    "glycan_train_list = train_df['IUPAC'].tolist()\n",
    "label_train_list = [train_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_train_list = list(zip(*label_train_list)) # Transpose\n",
    "\n",
    "glycan_val_list = val_df['IUPAC'].tolist()\n",
    "label_val_list = [val_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_val_list = list(zip(*label_val_list)) # Transpose\n",
    "\n",
    "glycan_test_list = test_df['IUPAC'].tolist()\n",
    "label_test_list = [test_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_test_list = list(zip(*label_test_list)) # Transpose\n",
    "\n",
    "print(f\"Number of training glycans: {len(glycan_train_list)}\")\n",
    "print(f\"Number of validation glycans: {len(glycan_val_list)}\")\n",
    "print(f\"Number of test glycans: {len(glycan_test_list)}\")\n",
    "print(f\"Shape of training labels: {len(label_train_list)} x {len(label_train_list[0]) if label_train_list else 0}\")\n",
    "print(f\"Shape of validation labels: {len(label_val_list)} x {len(label_val_list[0]) if label_val_list else 0}\")\n",
    "print(f\"Shape of test labels: {len(label_test_list)} x {len(label_test_list[0]) if label_test_list else 0}\")\n",
    "\n",
    "# Now re-run the checking function (modified for this new label extraction)\n",
    "def check_example_direct(glycan_list, label_list, split_name, original_df, kingdom_cols):\n",
    "    if glycan_list:\n",
    "        example_index = 0\n",
    "        example_glycan = glycan_list[example_index]\n",
    "        example_labels_split = list(label_list[example_index])\n",
    "\n",
    "        original_row = original_df[original_df['IUPAC'] == example_glycan].iloc[0]\n",
    "        labels_original = np.array([original_row[col] for col in kingdom_cols], dtype=np.float32).tolist()\n",
    "\n",
    "        print(f\"--- Checking example from {split_name} set (Direct) ---\")\n",
    "        print(f\"Glycan: {example_glycan}\")\n",
    "        print(f\"Split in original data: {original_row['split']}\")\n",
    "        print(f\"Labels in split data: {example_labels_split}\")\n",
    "        print(f\"Labels in original data: {labels_original}\")\n",
    "        if labels_original == example_labels_split:\n",
    "            print(\"Labels match!\")\n",
    "        else:\n",
    "            print(\"Labels DO NOT match!\")\n",
    "    else:\n",
    "        print(f\"{split_name} set is empty.\")\n",
    "\n",
    "check_example_direct(glycan_train_list, label_train_list, 'train', multilabel_kingdom_df_original, kingdom_cols)\n",
    "check_example_direct(glycan_val_list, label_val_list, 'val', multilabel_kingdom_df_original, kingdom_cols)\n",
    "check_example_direct(glycan_test_list, label_test_list, 'test', multilabel_kingdom_df_original, kingdom_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0f833",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multilabel_kingdom_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmultilabel_kingdom_loader\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'multilabel_kingdom_loader' is not defined"
     ]
    }
   ],
   "source": [
    "print(multilabel_kingdom_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e0b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: DataBatch(edge_index=[2, 430], labels=[462], string_labels=[32], y=[480], num_nodes=462, x=[462, 320], batch=[462], ptr=[33])\n",
      "Number of graphs in batch: 32\n",
      "\n",
      "First graph data: Data(edge_index=[2, 8], labels=[9], string_labels=[9], y=[15], x=[9, 320], num_nodes=9)\n",
      "Node features (x): tensor([[ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        ...,\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676]])\n",
      "Edge indices (edge_index): tensor([[1, 2, 3, 4, 5, 6, 7, 8],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7]])\n",
      "Labels (y): tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "String labels: ['Rha', 'a1-3', 'Rha', 'a1-4', 'GalNAcA3Ac', 'a1-3', 'QuiNAc', 'b1-2', 'Rha']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check one of the graphs in the dataloader\n",
    "\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataLoader is called 'multilabel_kingdom_loader'\n",
    "inspected_loader = multilabel_kingdom_loaders_emb['train'] # Access the train DataLoader\n",
    "\n",
    "# Get the first batch of data\n",
    "try:\n",
    "    batch = next(iter(inspected_loader))\n",
    "    print(\"Batch:\", batch)\n",
    "    print(\"Number of graphs in batch:\", batch.num_graphs)\n",
    "\n",
    "    # Extract the first graph from the batch\n",
    "    first_graph_data = batch[2] #change to check other graphs\n",
    "    print(\"\\nFirst graph data:\", first_graph_data)\n",
    "    print(\"Node features (x):\", first_graph_data.x)\n",
    "    print(\"Edge indices (edge_index):\", first_graph_data.edge_index)\n",
    "    print(\"Labels (y):\", first_graph_data.y)\n",
    "    print(\"String labels:\", first_graph_data.string_labels)\n",
    "\n",
    "    # Convert the PyG Data object to a NetworkX graph for visualization\n",
    "    nx_graph = to_networkx(first_graph_data) \n",
    "\n",
    "    # Visualize the NetworkX graph\n",
    "    nx_graph = to_networkx(first_graph_data)\n",
    "    pos = nx.spring_layout(nx_graph)\n",
    "    nx.draw(nx_graph, pos, with_labels=True, node_size=500, node_color=\"red\", font_size=10, font_weight=\"bold\", arrows=False)\n",
    "    plt.title(\"First Graph from DataLoader (PyG)\")\n",
    "    plt.show()\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"The DataLoader is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87dc8a6",
   "metadata": {},
   "source": [
    "## Let's make a function to add embeddings to a dataloader object ||||run on kernel restart||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da9d11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: glm_embeddings_1.pkl\n",
      "Embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Let's load the embeddings again here, so I don't have to jump up and down whenever I reload the kernel)\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            glm_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72ad776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add GLM embeddings to a dictionary of dataloaders before loading them into the model\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def add_glm_embeddings_to_dataloaders(dataloaders, glm_embeddings):\n",
    "    embedded_loaders = {}\n",
    "    embedding_dim = 320\n",
    "    for split, loader in dataloaders.items():\n",
    "        embedded_data_list = []\n",
    "        for batch in loader:\n",
    "            for graph in batch.to_data_list():\n",
    "                node_embeddings = []\n",
    "                if hasattr(graph, 'string_labels'):\n",
    "                    for label in graph.string_labels:\n",
    "                        if label in glm_embeddings:\n",
    "                            embedding = glm_embeddings[label]\n",
    "                            node_embeddings.append(torch.tensor(embedding))\n",
    "                        else:\n",
    "                            node_embeddings.append(torch.zeros(embedding_dim))\n",
    "                    graph.x = torch.stack(node_embeddings).float()\n",
    "                    #print(f\"Shape of graph.x after adding embeddings: {graph.x.shape}\")\n",
    "                embedded_data_list.append(graph)\n",
    "\n",
    "        embedded_loaders[split] = DataLoader(embedded_data_list, batch_size=32)\n",
    "        print(f\"GLM embeddings added to {split} DataLoader.\")\n",
    "    return embedded_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b83ae5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multilabel_kingdom_loaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# testing the function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m multilabel_kingdom_loaders_emb = add_glm_embeddings_to_dataloaders(\u001b[43mmultilabel_kingdom_loaders\u001b[49m, glm_embeddings)\n",
      "\u001b[31mNameError\u001b[39m: name 'multilabel_kingdom_loaders' is not defined"
     ]
    }
   ],
   "source": [
    "# testing the function\n",
    "\n",
    "multilabel_kingdom_loaders_emb = add_glm_embeddings_to_dataloaders(multilabel_kingdom_loaders, glm_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef713ec2",
   "metadata": {},
   "source": [
    "## Lets look at the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "83e13ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample string_labels from the first graph:\n",
      "['6dTal', 'a1-2', 'Rhaf', 'b1-5', 'Sug']\n"
     ]
    }
   ],
   "source": [
    "# Looking at string labels\n",
    "\n",
    "train_loader = multilabel_kingdom_loaders_emb['train'] \n",
    "batch = next(iter(train_loader))\n",
    "first_graph_labels = batch[2].string_labels\n",
    "print(\"Sample string_labels from the first graph:\")\n",
    "print(first_graph_labels[:20])  # Print the first 20 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e6368a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embeddings from the first graph:\n",
      "tensor([[ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676]])\n"
     ]
    }
   ],
   "source": [
    "# looking at the embeddings themselves\n",
    "\n",
    "train_loader = multilabel_kingdom_loaders_emb['train'] \n",
    "batch = next(iter(train_loader))\n",
    "first_graph_embeddings = batch[2].x\n",
    "print(\"Sample embeddings from the first graph:\")\n",
    "print(first_graph_embeddings[:20])  # Print the first 20 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef4130ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample keys from glm_embeddings:\n",
      "['!GlcNAc', '-10', '-12', '-2', '-4', '-6', '-8', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic', '1,5-Anhydro-Man-ol', '1,5-Anhydro-ManNAc-ol', '1,5-Anhydro-Xyl-ol', '1,5-Anhydro-Xyl2F-ol', '1-1', '1-2', '1-3', '1-4', '1-5', '1-6', '1-?', '1b-4', '1dAlt-ol', '1dEry-ol', '2,3-Anhydro-All', '2,3-Anhydro-Man', '2,3-Anhydro-Rib', '2,5-Anhydro-D-Alt-ol', '2,5-Anhydro-D-Alt3S-ol', '2,5-Anhydro-D-Tal', '2,5-Anhydro-Glc', '2,5-Anhydro-L-Man-ol', '2,5-Anhydro-Man', '2,5-Anhydro-Man-ol', '2,5-Anhydro-Man1S-ol', '2,5-Anhydro-Man3S-ol', '2,5-Anhydro-Man6S', '2,5-Anhydro-Tal-ol', '2,5-Anhydro-Tal6P', '2,6-Anhydro-Glc5NAc-ol', '2,6-Anhydro-L-Gul-ol', '2,6-Anhydro-L-Gul-onic', '2,6-Anhydro-Man-ol', '2,6-Anhydro-Tal5NAc-ol', '2,7-Anhydro-Kdo', '2,7-Anhydro-Kdof', '2-3', '2-4', '2-5', '2-6', '2dAraHexA', '3,6-Anhydro-Fruf', '3,6-Anhydro-Gal', '3,6-Anhydro-Gal2S', '3,6-Anhydro-Glc', '3,6-Anhydro-L-Gal', '3,6-Anhydro-L-Gal2Me', '3-1', '3-5', '3-Anhydro-Gal', '3-Anhydro-Gal2S', '3dFuc', '3dGal', '3dLyxHep-ulosaric', '4,7-Anhydro-Kdo', '4,7-Anhydro-KdoOPEtN', '4,8-Anhydro-Kdo', '4-1', '4-5', '4d8dNeu5Ac', '4dAraHex', '4dEry-ol', '4dFuc', '4dGal', '4dNeu5Ac', '4dThrHexNAcA4en', '4eLeg5Ac7Ac', '5-1', '5-2', '5-3', '5-4', '5-5', '5-6', '5dAraf', '5dAraf3Me', '5dLyxf3CFo', '5dLyxf3CMe', '5dPenf3CFo', '6-1', '6-3', '6-4', '6dAll', '6dAll3Me', '6dAlt', '6dAltNAc', '6dAltNAc1PP4N', '6dAltNAc1PP4NAc', '6dAltNAc3PCho', '6dAltOAc', '6dAltf', '6dAltfOAc', '6dFruf', '6dGal', '6dGalNAc', '6dGul', '6dHex', '6dHexN', '6dHexNAc4NAc', '6dManHep', '6dTal', '6dTal1PP', '6dTal2Ac', '6dTal2Ac3Ac', '6dTal2Ac3Ac4Ac', '6dTal2Ac3Me', '6dTal2Ac3Me4Ac', '6dTal2Ac4Ac', '6dTal2Me', '6dTal2Me4Ac', '6dTal3Me', '6dTal4Ac', '6dTalNAc', '6dTalNAc1PP', '6dTalNAc4Ac', '6dTalNAcOAc', '6dTalOAc', '6dTalOAcOAc', '6dTalOAcOMe', '6dTalOMe', '6dTalOMe-ol', '6dTalf', '7dNeu5Ac', '8dNeu5Ac', '8eAci5Ac7Ac', '8eLeg', '8eLeg5Ac7Ac', '8eLeg5Ac7Ac8Ac', '8eLeg5Ac7AcGro', '8eLeg5But7Ac', '8eLegNAcNBut', '8ePse5Ac7Ac', '9dNeu5Ac', '?1-2', '?1-3', '?1-4', '?1-6', '?1-?', '?2-?', 'Abe', 'Abe1PP', 'Abe2Ac', 'AbeOAc', 'Acarbose', 'AcefA', 'Aci5Ac7Ac', 'AcoNAc', 'All', 'All-ol', 'All1S2S3S4S', 'All2Ac3Ac', 'All2S3S4S', 'All3Ac', 'All6Ac', 'AllN', 'AllN1P', 'AllNAc', 'AllNAc6Me', 'AllOMe', 'Alt', 'AltA', 'AltA2N', 'AltA2S', 'AltAN', 'AltNAc', 'AltNAcA', 'AltNAcA1Prop', 'Altf', 'AltfOAc', 'Amikacin', 'Api', 'ApiOAc', 'ApiOMe-ol', 'Apif', 'Ara', 'Ara-ol', 'Ara1Cer2Ac', 'Ara1Me', 'Ara1N4P', 'Ara1P4N', 'Ara1PP']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample keys from glm_embeddings:\")\n",
    "print(list(glm_embeddings.keys())[:200])  # Print the first 20 keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b56aa7",
   "metadata": {},
   "source": [
    "## Copies of glycowork functions for experimentation ||||run on kernel restart||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5658e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SweetNet class\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Union, Literal\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.nn import GraphConv\n",
    "    from torch_geometric.nn import global_mean_pool as gap\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "  raise ImportError(\"<torch or torch_geometric missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from glycowork.glycan_data.loader import lib, download_model \n",
    "\n",
    "class SweetNet(torch.nn.Module):\n",
    "    def __init__(self, lib_size: int, # number of unique tokens for graph nodes\n",
    "                 num_classes: int = 1, # number of output classes (>1 for multilabel)\n",
    "                 hidden_dim: int = 320, # dimension of hidden layers (changed from 128 to 320)\n",
    "                 use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "                ) -> None:\n",
    "        \"given glycan graphs as input, predicts properties via a graph neural network\"\n",
    "        #print(\"Using SweetNet from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "        super(SweetNet, self).__init__()\n",
    "        self.use_external_embeddings = use_external_embeddings\n",
    "        # Convolution operations on the graph\n",
    "        self.conv1 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GraphConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Node embedding\n",
    "        if use_external_embeddings:\n",
    "            self.embedding_dim = hidden_dim\n",
    "        else:\n",
    "            self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    " \n",
    "\n",
    "        #self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    "        # Fully connected part\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, 1024)\n",
    "        self.lin2 = torch.nn.Linear(1024, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, num_classes)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(1024)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.act1 = torch.nn.LeakyReLU()\n",
    "        self.act2 = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor,\n",
    "                inference: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \n",
    "        # Getting node features\n",
    "        if self.use_external_embeddings:\n",
    "           # Use external embeddings (already in x)\n",
    "            pass # x is already the embeddings\n",
    "        else:\n",
    "             # Use internal embedding\n",
    "            x = self.item_embedding(x).squeeze(1)\n",
    "            \n",
    "        \n",
    "        # Graph convolution operations (now using GLM embeddings as x)\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = gap(x, batch)\n",
    "\n",
    "        # Fully connected part\n",
    "        x = self.act1(self.bn1(self.lin1(x)))\n",
    "        x_out = self.bn2(self.lin2(x))\n",
    "        x = F.dropout(self.act2(x_out), p = 0.5, training = self.training)\n",
    "\n",
    "        x = self.lin3(x).squeeze(1)\n",
    "\n",
    "        if inference:\n",
    "          return x, x_out\n",
    "        else:\n",
    "          return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03900750",
   "metadata": {},
   "source": [
    "### Other functions I might modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eccb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model function\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import torch\n",
    "    # Choose the correct computing architecture\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "    raise ImportError(\"<torch missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, mean_squared_error, \\\n",
    "    label_ranking_average_precision_score, ndcg_score, roc_auc_score, mean_absolute_error, r2_score\n",
    "from glycowork.motif.annotate import annotate_dataset\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 7, # epochs to wait after last improvement\n",
    "                 verbose: bool = False # whether to print messages\n",
    "                ) -> None:\n",
    "        \"Early stops the training if validation loss doesn't improve after a given patience\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = 0\n",
    "\n",
    "    def __call__(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), 'drive/My Drive/checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def sigmoid(x: float # input value\n",
    "          ) -> float: # sigmoid transformed value\n",
    "    \"Apply sigmoid transformation to input\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def disable_running_stats(model: torch.nn.Module # model to disable batch norm\n",
    "                       ) -> None:\n",
    "    \"Disable batch normalization running statistics\"\n",
    "\n",
    "    def _disable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "\n",
    "def enable_running_stats(model: torch.nn.Module # model to enable batch norm\n",
    "                      ) -> None:\n",
    "    \"Enable batch normalization running statistics\"\n",
    "\n",
    "    def _enable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def train_model(model: torch.nn.Module, # graph neural network for analyzing glycans\n",
    "               dataloaders: Dict[str, torch.utils.data.DataLoader], # dict with 'train' and 'val' loaders\n",
    "               criterion: torch.nn.Module, # PyTorch loss function\n",
    "               optimizer: torch.optim.Optimizer, # PyTorch optimizer, has to be SAM if mode != \"regression\"\n",
    "               scheduler: torch.optim.lr_scheduler._LRScheduler, # PyTorch learning rate decay\n",
    "               num_epochs: int = 25, # number of epochs for training\n",
    "               patience: int = 50, # epochs without improvement until early stop\n",
    "               mode: str = 'classification', # 'classification', 'multilabel', or 'regression'\n",
    "               mode2: str = 'multi', # 'multi' or 'binary' classification\n",
    "               return_metrics: bool = False, # whether to return metrics\n",
    "               use_external_embeddings: bool = False # whether to use external embeddings\n",
    "              ) -> Union[torch.nn.Module, tuple[torch.nn.Module, dict[str, dict[str, list[float]]]]]: # best model from training and the training and validation metrics\n",
    "    \"trains a deep learning model on predicting glycan properties\"\n",
    "\n",
    "    since = time.time()\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "    best_lead_metric = float(\"inf\")\n",
    "\n",
    "    if mode == 'classification':\n",
    "         # Removed auroc from metrics to avoid calculation issues for now\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": []}\n",
    "    elif mode == 'multilabel':\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": [], \"lrap\": [], \"ndcg\": []}\n",
    "    else:\n",
    "        blank_metrics = {\"loss\": [], \"mse\": [], \"mae\": [], \"r2\": []}\n",
    "\n",
    "    metrics = {\"train\": copy.deepcopy(blank_metrics), \"val\": copy.deepcopy(blank_metrics)}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_metrics = copy.deepcopy(blank_metrics)\n",
    "            running_metrics[\"weights\"] = []\n",
    "\n",
    "            for data in dataloaders[phase]:\n",
    "                # Get all relevant node attributes\n",
    "                print(f\"Phase: {phase}, Data: {data}\")\n",
    "                print(f\"Phase: {phase}, Data.x: {getattr(data, 'x', None)}\") # Check if x exists and its value\n",
    "                if use_external_embeddings:\n",
    "                    x, y, edge_index, batch = data.x, data.y, data.edge_index, data.batch\n",
    "                else:\n",
    "                    x, y, edge_index, batch = data.labels, data.y, data.edge_index, data.batch\n",
    "                prot = getattr(data, 'train_idx', None)\n",
    "                if prot is not None:\n",
    "                    prot = prot.view(max(batch) + 1, -1).to(device)\n",
    "                x = x.to(device)\n",
    "                if mode == 'multilabel':\n",
    "                    y = y.view(max(batch) + 1, -1).to(device)\n",
    "                else:\n",
    "                    y = y.to(device)\n",
    "                y = y.view(-1, 1) if mode == 'regression' else y\n",
    "                edge_index = edge_index.to(device)\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # First forward pass\n",
    "                    if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                        enable_running_stats(model)\n",
    "                    pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                    loss = criterion(pred, y)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                            optimizer.first_step(zero_grad = True)\n",
    "                            # Second forward pass\n",
    "                            disable_running_stats(model)\n",
    "                            second_pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                            criterion(second_pred, y).backward()\n",
    "                            optimizer.second_step(zero_grad = True)\n",
    "                        else:\n",
    "                            optimizer.step()\n",
    "                \n",
    "                # Check for single-class batches\n",
    "                unique_classes = torch.unique(y).cpu().numpy()\n",
    "                if len(unique_classes) == 1:\n",
    "                    print(f\"WARNING: Single-class batch detected in {phase} phase at epoch {epoch}!\")\n",
    "                    print(f\"Batch labels: {unique_classes}\")\n",
    "\n",
    "                # Collecting relevant metrics\n",
    "                running_metrics[\"loss\"].append(loss.item())\n",
    "                running_metrics[\"weights\"].append(batch.max().cpu() + 1)\n",
    "\n",
    "                y_det = y.detach().cpu().numpy()\n",
    "                pred_det = pred.cpu().detach().numpy()\n",
    "                if mode == 'classification':\n",
    "                    if mode2 == 'multi':\n",
    "                        pred_proba = np.exp(pred_det) / np.sum(np.exp(pred_det), axis = 1, keepdims = True)  # numpy softmax\n",
    "                        pred2 = np.argmax(pred_det, axis = 1)\n",
    "                    else:\n",
    "                        pred_proba = sigmoid(pred_det)\n",
    "                        pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det, pred2))\n",
    "                    # commented out auroc because it was throwing errors\n",
    "                    #running_metrics[\"auroc\"].append(roc_auc_score(y_det.astype(int), pred_proba if mode2 == 'binary' else pred_proba[:, 1]))\n",
    "                elif mode == 'multilabel':\n",
    "                    pred_proba = sigmoid(pred_det)\n",
    "                    pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det.flatten(), pred2.flatten()))\n",
    "                    running_metrics[\"lrap\"].append(label_ranking_average_precision_score(y_det.astype(int), pred_proba))\n",
    "                    running_metrics[\"ndcg\"].append(ndcg_score(y_det.astype(int), pred_proba))\n",
    "                else:\n",
    "                    running_metrics[\"mse\"].append(mean_squared_error(y_det, pred_det))\n",
    "                    running_metrics[\"mae\"].append(mean_absolute_error(y_det, pred_det))\n",
    "                    running_metrics[\"r2\"].append(r2_score(y_det, pred_det))\n",
    "\n",
    "            # Averaging metrics at end of epoch\n",
    "            for key in running_metrics:\n",
    "                if key == \"weights\":\n",
    "                    continue\n",
    "                metrics[phase][key].append(np.average(running_metrics[key], weights = running_metrics[\"weights\"]))\n",
    "\n",
    "            if mode == 'classification':\n",
    "                print('{} Loss: {:.4f} Accuracy: {:.4f} MCC: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            elif mode == 'multilabel':\n",
    "                print('{} Loss: {:.4f} LRAP: {:.4f} NDCG: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            else:\n",
    "                print('{} Loss: {:.4f} MSE: {:.4f} MAE: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"mse\"][-1], metrics[phase][\"mae\"][-1]))\n",
    "\n",
    "            # Keep best model state_dict\n",
    "            if phase == \"val\":\n",
    "                if metrics[phase][\"loss\"][-1] <= best_loss:\n",
    "                    best_loss = metrics[phase][\"loss\"][-1]\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                    # Extract the lead metric (ACC, LRAP, or MSE) of the new best model\n",
    "                    if mode == 'classification':\n",
    "                        best_lead_metric = metrics[phase][\"acc\"][-1]\n",
    "                    elif mode == 'multilabel':\n",
    "                        best_lead_metric = metrics[phase][\"lrap\"][-1]\n",
    "                    else:\n",
    "                        best_lead_metric = metrics[phase][\"mse\"][-1]\n",
    "\n",
    "                # Check Early Stopping & adjust learning rate if needed\n",
    "                early_stopping(metrics[phase][\"loss\"][-1], model)\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(metrics[phase][\"loss\"][-1])\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    if mode == 'classification':\n",
    "        print('Best val loss: {:4f}, best Accuracy score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    elif mode == 'multilabel':\n",
    "        print('Best val loss: {:4f}, best LRAP score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    else:\n",
    "        print('Best val loss: {:4f}, best MSE score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    if return_metrics:\n",
    "        return model, metrics\n",
    "\n",
    "    # Plot loss & score over the course of training\n",
    "    _, _ = plt.subplots(nrows=2, ncols=1)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(epoch + 1), metrics[\"val\"][\"loss\"])\n",
    "    plt.title('Model Training')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.legend(['Validation Loss'], loc='best')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    if mode == 'classification':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"acc\"])\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "        plt.legend(['Validation Accuracy'], loc='best')\n",
    "    elif mode == 'multilabel':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"lrap\"])\n",
    "        plt.ylabel('Validation LRAP')\n",
    "        plt.legend(['Validation LRAP'], loc='best')\n",
    "    else:\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"mse\"])\n",
    "        plt.ylabel('Validation MSE')\n",
    "        plt.legend(['Validation MSE'], loc='best')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57a375b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init_weights function\n",
    "\n",
    "def init_weights(model: torch.nn.Module, # neural network for analyzing glycans\n",
    "                mode: str = 'sparse', # initialization algorithm: 'sparse', 'kaiming', 'xavier'\n",
    "                sparsity: float = 0.1 # proportion of sparsity after initialization\n",
    "               ) -> None:\n",
    "    \"initializes linear layers of PyTorch model with a weight initialization\"\n",
    "    #print(\"Using init_weights from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if isinstance(model, torch.nn.Linear):\n",
    "        if mode == 'sparse':\n",
    "            torch.nn.init.sparse_(model.weight, sparsity = sparsity)\n",
    "        elif mode == 'kaiming':\n",
    "            torch.nn.init.kaiming_uniform_(model.weight)\n",
    "        elif mode == 'xavier':\n",
    "            torch.nn.init.xavier_uniform_(model.weight)\n",
    "        else:\n",
    "            print(\"This initialization option is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a059da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_model function\n",
    "\n",
    "def prep_model(model_type: Literal[\"SweetNet\", \"LectinOracle\", \"LectinOracle_flex\", \"NSequonPred\"], # type of model to create\n",
    "              num_classes: int, # number of unique classes for classification\n",
    "              libr: Optional[Dict[str, int]] = None, # dictionary of form glycoletter:index\n",
    "              trained: bool = False, # whether to use pretrained model\n",
    "              # set hidden_dim to 320 rather than 128 for the pretrained model\n",
    "              # but 128 is the default for the model in the paper\n",
    "              hidden_dim: int = 320, # hidden dimension for the model (SweetNet/LectinOracle only)\n",
    "              use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "             ) -> torch.nn.Module: # initialized PyTorch model\n",
    "    \"wrapper to instantiate model, initialize it, and put it on the GPU\"\n",
    "    #print(\"Using prep_model from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if libr is None:\n",
    "      libr = lib\n",
    "    if model_type == 'SweetNet':\n",
    "      model = SweetNet(len(libr), num_classes = num_classes, hidden_dim = hidden_dim, use_external_embeddings = use_external_embeddings)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'sparse'))\n",
    "      if trained:\n",
    "        if hidden_dim != 128:\n",
    "          raise ValueError(\"Hidden dimension must be 128 for pretrained model\")\n",
    "        model_path = download_model(\"glycowork_sweetnet_species.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle':\n",
    "      model = LectinOracle(len(libr), num_classes = num_classes, input_size_prot = int(10*hidden_dim))\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle_flex':\n",
    "      model = LectinOracle_flex(len(libr), num_classes = num_classes)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle_flex.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'NSequonPred':\n",
    "      model = NSequonPred()\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"NSequonPred_batch32.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    else:\n",
    "      print(\"Invalid Model Type\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbbca33",
   "metadata": {},
   "source": [
    "## Testing using same framework as iteration 0 (basic kingdom sweetnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# testing the modified SweetNet model on the GlycoWork dataset \n",
    "from glycowork.glycan_data.loader import df_species\n",
    "from glycowork.ml.train_test_split import hierarchy_filter\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "from glycowork.ml import model_training\n",
    "\n",
    "# silence the avalanche of \"undefined\" warnings\n",
    "#import warnings\n",
    "#from sklearn.exceptions import UndefinedMetricWarning\n",
    "#warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "train_x, val_x, train_y, val_y, id_val, class_list, class_converter = hierarchy_filter(df_species,\n",
    "                                                                                       rank = 'Kingdom')\n",
    "\n",
    "dataloaders = split_data_to_train(train_x, val_x, train_y, val_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets split out the training code so I don't have to load the data each time\n",
    "\n",
    "model = prep_model('SweetNet', len(class_list))\n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = len(class_list))\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'classification',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4382d5b4",
   "metadata": {},
   "source": [
    "### Extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdade799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the Kingdoms in the original data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "kingdom_counts = df_species['Kingdom'].value_counts()\n",
    "print(\"Kingdom distribution in the original data:\")\n",
    "print(kingdom_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f112562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Print the first few rows to get an overview\n",
    "print(df_species.head())\n",
    "\n",
    "# Print the data types of the columns\n",
    "print(df_species.info())\n",
    "\n",
    "# Check for duplicated glycans\n",
    "print(f\"Number of unique glycans: {df_species['glycan'].nunique()}\")\n",
    "print(f\"Total number of rows: {len(df_species)}\")\n",
    "\n",
    "# If you suspect a column contains multiple kingdoms:\n",
    "if 'Kingdom' in df_species.columns:  # Replace 'Kingdom' with the actual column name\n",
    "    # Check the first few values of that column\n",
    "    print(df_species['Kingdom'].head())\n",
    "    # If it's a string with delimiters, count the delimiters\n",
    "    if isinstance(df_species['Kingdom'][0], str) and ',' in df_species['Kingdom'][0]: #assuming ',' is the delimiter\n",
    "        print(f\"Example value: {df_species['Kingdom'][0]}\")\n",
    "        print(f\"Number of commas in the first value: {df_species['Kingdom'][0].count(',')}\")\n",
    "\n",
    "    # Check for multiple columns representing kingdoms\n",
    "    for col in df_species.columns:\n",
    "        if col in ['Animalia', 'Bacteria', 'Plantae']:  # Add all possible kingdom column names\n",
    "            print(f\"Column '{col}': Data type = {df_species[col].dtype}, Unique values = {df_species[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set()\n",
    "duplicates_count = 0\n",
    "\n",
    "for item in train_x:\n",
    "    if item in seen:\n",
    "        duplicates_count += 1\n",
    "    else:\n",
    "        seen.add(item)\n",
    "\n",
    "print(f\"Number of duplicates: {duplicates_count}\")\n",
    "print(f\"Number of unique items: {len(seen)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e20752",
   "metadata": {},
   "source": [
    "## Time to train a multi-class multi-label Sweetnet on our properly loaded data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b59cf29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 1.9418 LRAP: 0.1018 NDCG: 0.5364\n",
      "val Loss: 1.3059 LRAP: 0.4947 NDCG: 0.6652\n",
      "Validation loss decreased (0.000000 --> 1.305928).  Saving model ...\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 1.2915 LRAP: 0.4963 NDCG: 0.6864\n",
      "val Loss: 1.1568 LRAP: 0.6066 NDCG: 0.7330\n",
      "Validation loss decreased (1.305928 --> 1.156826).  Saving model ...\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 1.1996 LRAP: 0.5826 NDCG: 0.7301\n",
      "val Loss: 1.1165 LRAP: 0.6395 NDCG: 0.7663\n",
      "Validation loss decreased (1.156826 --> 1.116533).  Saving model ...\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 1.1480 LRAP: 0.6077 NDCG: 0.7524\n",
      "val Loss: 1.0790 LRAP: 0.6740 NDCG: 0.7806\n",
      "Validation loss decreased (1.116533 --> 1.078970).  Saving model ...\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 1.1096 LRAP: 0.6380 NDCG: 0.7662\n",
      "val Loss: 1.0661 LRAP: 0.6886 NDCG: 0.7969\n",
      "Validation loss decreased (1.078970 --> 1.066058).  Saving model ...\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 1.0867 LRAP: 0.6491 NDCG: 0.7792\n",
      "val Loss: 1.0446 LRAP: 0.7051 NDCG: 0.8103\n",
      "Validation loss decreased (1.066058 --> 1.044552).  Saving model ...\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 1.0639 LRAP: 0.6761 NDCG: 0.7935\n",
      "val Loss: 1.0293 LRAP: 0.7137 NDCG: 0.8202\n",
      "Validation loss decreased (1.044552 --> 1.029283).  Saving model ...\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 1.0403 LRAP: 0.6957 NDCG: 0.8073\n",
      "val Loss: 1.0154 LRAP: 0.7522 NDCG: 0.8348\n",
      "Validation loss decreased (1.029283 --> 1.015370).  Saving model ...\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 1.0247 LRAP: 0.7082 NDCG: 0.8148\n",
      "val Loss: 1.0006 LRAP: 0.7574 NDCG: 0.8377\n",
      "Validation loss decreased (1.015370 --> 1.000597).  Saving model ...\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.9998 LRAP: 0.7231 NDCG: 0.8240\n",
      "val Loss: 0.9893 LRAP: 0.7850 NDCG: 0.8516\n",
      "Validation loss decreased (1.000597 --> 0.989272).  Saving model ...\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.9807 LRAP: 0.7367 NDCG: 0.8327\n",
      "val Loss: 0.9763 LRAP: 0.7859 NDCG: 0.8570\n",
      "Validation loss decreased (0.989272 --> 0.976321).  Saving model ...\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.9708 LRAP: 0.7485 NDCG: 0.8424\n",
      "val Loss: 0.9758 LRAP: 0.7862 NDCG: 0.8586\n",
      "Validation loss decreased (0.976321 --> 0.975804).  Saving model ...\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.9556 LRAP: 0.7633 NDCG: 0.8508\n",
      "val Loss: 0.9677 LRAP: 0.7950 NDCG: 0.8678\n",
      "Validation loss decreased (0.975804 --> 0.967666).  Saving model ...\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.9496 LRAP: 0.7710 NDCG: 0.8554\n",
      "val Loss: 0.9605 LRAP: 0.7936 NDCG: 0.8683\n",
      "Validation loss decreased (0.967666 --> 0.960506).  Saving model ...\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.9343 LRAP: 0.7803 NDCG: 0.8600\n",
      "val Loss: 0.9624 LRAP: 0.8087 NDCG: 0.8761\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.9270 LRAP: 0.7883 NDCG: 0.8664\n",
      "val Loss: 0.9573 LRAP: 0.8144 NDCG: 0.8771\n",
      "Validation loss decreased (0.960506 --> 0.957319).  Saving model ...\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.9212 LRAP: 0.7963 NDCG: 0.8704\n",
      "val Loss: 0.9518 LRAP: 0.8222 NDCG: 0.8826\n",
      "Validation loss decreased (0.957319 --> 0.951756).  Saving model ...\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.9155 LRAP: 0.8060 NDCG: 0.8749\n",
      "val Loss: 0.9507 LRAP: 0.8319 NDCG: 0.8856\n",
      "Validation loss decreased (0.951756 --> 0.950722).  Saving model ...\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.9066 LRAP: 0.8092 NDCG: 0.8776\n",
      "val Loss: 0.9508 LRAP: 0.8256 NDCG: 0.8828\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.9034 LRAP: 0.8154 NDCG: 0.8819\n",
      "val Loss: 0.9477 LRAP: 0.8316 NDCG: 0.8868\n",
      "Validation loss decreased (0.950722 --> 0.947720).  Saving model ...\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.8993 LRAP: 0.8175 NDCG: 0.8834\n",
      "val Loss: 0.9505 LRAP: 0.8438 NDCG: 0.8910\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.8924 LRAP: 0.8283 NDCG: 0.8884\n",
      "val Loss: 0.9408 LRAP: 0.8301 NDCG: 0.8872\n",
      "Validation loss decreased (0.947720 --> 0.940794).  Saving model ...\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.8841 LRAP: 0.8314 NDCG: 0.8910\n",
      "val Loss: 0.9439 LRAP: 0.8516 NDCG: 0.8970\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.8838 LRAP: 0.8349 NDCG: 0.8928\n",
      "val Loss: 0.9426 LRAP: 0.8578 NDCG: 0.8982\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.8766 LRAP: 0.8402 NDCG: 0.8957\n",
      "val Loss: 0.9418 LRAP: 0.8433 NDCG: 0.8926\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.8719 LRAP: 0.8444 NDCG: 0.8983\n",
      "val Loss: 0.9413 LRAP: 0.8521 NDCG: 0.8979\n",
      "EarlyStopping counter: 4 out of 50\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.8708 LRAP: 0.8466 NDCG: 0.9005\n",
      "val Loss: 0.9400 LRAP: 0.8638 NDCG: 0.9037\n",
      "Validation loss decreased (0.940794 --> 0.940008).  Saving model ...\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.8641 LRAP: 0.8515 NDCG: 0.9035\n",
      "val Loss: 0.9360 LRAP: 0.8516 NDCG: 0.8978\n",
      "Validation loss decreased (0.940008 --> 0.935993).  Saving model ...\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.8610 LRAP: 0.8485 NDCG: 0.9025\n",
      "val Loss: 0.9409 LRAP: 0.8604 NDCG: 0.8999\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.8544 LRAP: 0.8611 NDCG: 0.9079\n",
      "val Loss: 0.9345 LRAP: 0.8744 NDCG: 0.9083\n",
      "Validation loss decreased (0.935993 --> 0.934467).  Saving model ...\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.8526 LRAP: 0.8651 NDCG: 0.9105\n",
      "val Loss: 0.9347 LRAP: 0.8710 NDCG: 0.9069\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.8483 LRAP: 0.8675 NDCG: 0.9137\n",
      "val Loss: 0.9327 LRAP: 0.8693 NDCG: 0.9070\n",
      "Validation loss decreased (0.934467 --> 0.932738).  Saving model ...\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.8442 LRAP: 0.8673 NDCG: 0.9132\n",
      "val Loss: 0.9305 LRAP: 0.8747 NDCG: 0.9076\n",
      "Validation loss decreased (0.932738 --> 0.930503).  Saving model ...\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.8409 LRAP: 0.8631 NDCG: 0.9120\n",
      "val Loss: 0.9265 LRAP: 0.8690 NDCG: 0.9065\n",
      "Validation loss decreased (0.930503 --> 0.926494).  Saving model ...\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.8354 LRAP: 0.8722 NDCG: 0.9162\n",
      "val Loss: 0.9269 LRAP: 0.8801 NDCG: 0.9126\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.8340 LRAP: 0.8737 NDCG: 0.9170\n",
      "val Loss: 0.9327 LRAP: 0.8704 NDCG: 0.9079\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.8281 LRAP: 0.8799 NDCG: 0.9202\n",
      "val Loss: 0.9298 LRAP: 0.8647 NDCG: 0.9052\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.8280 LRAP: 0.8769 NDCG: 0.9199\n",
      "val Loss: 0.9323 LRAP: 0.8738 NDCG: 0.9090\n",
      "EarlyStopping counter: 4 out of 50\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.8242 LRAP: 0.8844 NDCG: 0.9235\n",
      "val Loss: 0.9293 LRAP: 0.8733 NDCG: 0.9098\n",
      "EarlyStopping counter: 5 out of 50\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.8165 LRAP: 0.8848 NDCG: 0.9241\n",
      "val Loss: 0.9274 LRAP: 0.8787 NDCG: 0.9121\n",
      "EarlyStopping counter: 6 out of 50\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.8131 LRAP: 0.8892 NDCG: 0.9275\n",
      "val Loss: 0.9277 LRAP: 0.8804 NDCG: 0.9124\n",
      "EarlyStopping counter: 7 out of 50\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.8107 LRAP: 0.8917 NDCG: 0.9292\n",
      "val Loss: 0.9270 LRAP: 0.8807 NDCG: 0.9139\n",
      "EarlyStopping counter: 8 out of 50\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.8095 LRAP: 0.8948 NDCG: 0.9308\n",
      "val Loss: 0.9255 LRAP: 0.8778 NDCG: 0.9122\n",
      "Validation loss decreased (0.926494 --> 0.925499).  Saving model ...\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.8074 LRAP: 0.8948 NDCG: 0.9313\n",
      "val Loss: 0.9263 LRAP: 0.8835 NDCG: 0.9151\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.8091 LRAP: 0.8948 NDCG: 0.9307\n",
      "val Loss: 0.9260 LRAP: 0.8770 NDCG: 0.9131\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.8063 LRAP: 0.8958 NDCG: 0.9319\n",
      "val Loss: 0.9230 LRAP: 0.8844 NDCG: 0.9149\n",
      "Validation loss decreased (0.925499 --> 0.923024).  Saving model ...\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.8059 LRAP: 0.8955 NDCG: 0.9313\n",
      "val Loss: 0.9238 LRAP: 0.8824 NDCG: 0.9141\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.8032 LRAP: 0.8963 NDCG: 0.9325\n",
      "val Loss: 0.9278 LRAP: 0.8835 NDCG: 0.9148\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.8014 LRAP: 0.8978 NDCG: 0.9329\n",
      "val Loss: 0.9267 LRAP: 0.8832 NDCG: 0.9154\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.8042 LRAP: 0.8966 NDCG: 0.9329\n",
      "val Loss: 0.9254 LRAP: 0.8810 NDCG: 0.9151\n",
      "EarlyStopping counter: 4 out of 50\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.8010 LRAP: 0.8983 NDCG: 0.9332\n",
      "val Loss: 0.9236 LRAP: 0.8815 NDCG: 0.9155\n",
      "EarlyStopping counter: 5 out of 50\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.8004 LRAP: 0.9007 NDCG: 0.9344\n",
      "val Loss: 0.9272 LRAP: 0.8824 NDCG: 0.9154\n",
      "EarlyStopping counter: 6 out of 50\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.7992 LRAP: 0.9000 NDCG: 0.9349\n",
      "val Loss: 0.9234 LRAP: 0.8864 NDCG: 0.9163\n",
      "EarlyStopping counter: 7 out of 50\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.8012 LRAP: 0.8999 NDCG: 0.9351\n",
      "val Loss: 0.9241 LRAP: 0.8824 NDCG: 0.9141\n",
      "EarlyStopping counter: 8 out of 50\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.7965 LRAP: 0.9027 NDCG: 0.9361\n",
      "val Loss: 0.9256 LRAP: 0.8841 NDCG: 0.9156\n",
      "EarlyStopping counter: 9 out of 50\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.7965 LRAP: 0.9011 NDCG: 0.9350\n",
      "val Loss: 0.9289 LRAP: 0.8810 NDCG: 0.9147\n",
      "EarlyStopping counter: 10 out of 50\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.7968 LRAP: 0.9016 NDCG: 0.9360\n",
      "val Loss: 0.9272 LRAP: 0.8832 NDCG: 0.9155\n",
      "EarlyStopping counter: 11 out of 50\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.7980 LRAP: 0.9000 NDCG: 0.9354\n",
      "val Loss: 0.9267 LRAP: 0.8787 NDCG: 0.9130\n",
      "EarlyStopping counter: 12 out of 50\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.7976 LRAP: 0.9023 NDCG: 0.9356\n",
      "val Loss: 0.9239 LRAP: 0.8844 NDCG: 0.9162\n",
      "EarlyStopping counter: 13 out of 50\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.7980 LRAP: 0.9008 NDCG: 0.9350\n",
      "val Loss: 0.9255 LRAP: 0.8852 NDCG: 0.9156\n",
      "EarlyStopping counter: 14 out of 50\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.7986 LRAP: 0.9032 NDCG: 0.9369\n",
      "val Loss: 0.9243 LRAP: 0.8850 NDCG: 0.9156\n",
      "EarlyStopping counter: 15 out of 50\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.7970 LRAP: 0.9017 NDCG: 0.9356\n",
      "val Loss: 0.9277 LRAP: 0.8835 NDCG: 0.9158\n",
      "EarlyStopping counter: 16 out of 50\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.7970 LRAP: 0.9020 NDCG: 0.9358\n",
      "val Loss: 0.9249 LRAP: 0.8858 NDCG: 0.9159\n",
      "EarlyStopping counter: 17 out of 50\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.7974 LRAP: 0.8993 NDCG: 0.9346\n",
      "val Loss: 0.9252 LRAP: 0.8810 NDCG: 0.9141\n",
      "EarlyStopping counter: 18 out of 50\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.7976 LRAP: 0.9012 NDCG: 0.9353\n",
      "val Loss: 0.9271 LRAP: 0.8827 NDCG: 0.9144\n",
      "EarlyStopping counter: 19 out of 50\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.7951 LRAP: 0.9015 NDCG: 0.9359\n",
      "val Loss: 0.9247 LRAP: 0.8818 NDCG: 0.9141\n",
      "EarlyStopping counter: 20 out of 50\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.7974 LRAP: 0.9023 NDCG: 0.9359\n",
      "val Loss: 0.9245 LRAP: 0.8804 NDCG: 0.9135\n",
      "EarlyStopping counter: 21 out of 50\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.7978 LRAP: 0.9005 NDCG: 0.9347\n",
      "val Loss: 0.9241 LRAP: 0.8821 NDCG: 0.9146\n",
      "EarlyStopping counter: 22 out of 50\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.7979 LRAP: 0.9006 NDCG: 0.9352\n",
      "val Loss: 0.9238 LRAP: 0.8824 NDCG: 0.9147\n",
      "EarlyStopping counter: 23 out of 50\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.7978 LRAP: 0.9030 NDCG: 0.9364\n",
      "val Loss: 0.9244 LRAP: 0.8818 NDCG: 0.9150\n",
      "EarlyStopping counter: 24 out of 50\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.7972 LRAP: 0.9013 NDCG: 0.9354\n",
      "val Loss: 0.9237 LRAP: 0.8838 NDCG: 0.9159\n",
      "EarlyStopping counter: 25 out of 50\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.7969 LRAP: 0.9013 NDCG: 0.9356\n",
      "val Loss: 0.9246 LRAP: 0.8841 NDCG: 0.9156\n",
      "EarlyStopping counter: 26 out of 50\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.7967 LRAP: 0.9022 NDCG: 0.9356\n",
      "val Loss: 0.9295 LRAP: 0.8827 NDCG: 0.9141\n",
      "EarlyStopping counter: 27 out of 50\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.7980 LRAP: 0.9035 NDCG: 0.9365\n",
      "val Loss: 0.9265 LRAP: 0.8858 NDCG: 0.9162\n",
      "EarlyStopping counter: 28 out of 50\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.7968 LRAP: 0.8996 NDCG: 0.9357\n",
      "val Loss: 0.9244 LRAP: 0.8852 NDCG: 0.9161\n",
      "EarlyStopping counter: 29 out of 50\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.7984 LRAP: 0.9008 NDCG: 0.9347\n",
      "val Loss: 0.9236 LRAP: 0.8852 NDCG: 0.9156\n",
      "EarlyStopping counter: 30 out of 50\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.7968 LRAP: 0.9047 NDCG: 0.9366\n",
      "val Loss: 0.9276 LRAP: 0.8830 NDCG: 0.9152\n",
      "EarlyStopping counter: 31 out of 50\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.7979 LRAP: 0.9004 NDCG: 0.9348\n",
      "val Loss: 0.9261 LRAP: 0.8847 NDCG: 0.9157\n",
      "EarlyStopping counter: 32 out of 50\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.7967 LRAP: 0.9024 NDCG: 0.9356\n",
      "val Loss: 0.9282 LRAP: 0.8821 NDCG: 0.9152\n",
      "EarlyStopping counter: 33 out of 50\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.7962 LRAP: 0.9016 NDCG: 0.9363\n",
      "val Loss: 0.9260 LRAP: 0.8824 NDCG: 0.9150\n",
      "EarlyStopping counter: 34 out of 50\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.7983 LRAP: 0.9014 NDCG: 0.9355\n",
      "val Loss: 0.9253 LRAP: 0.8810 NDCG: 0.9140\n",
      "EarlyStopping counter: 35 out of 50\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.7982 LRAP: 0.9007 NDCG: 0.9353\n",
      "val Loss: 0.9257 LRAP: 0.8844 NDCG: 0.9154\n",
      "EarlyStopping counter: 36 out of 50\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.7979 LRAP: 0.9035 NDCG: 0.9360\n",
      "val Loss: 0.9259 LRAP: 0.8815 NDCG: 0.9142\n",
      "EarlyStopping counter: 37 out of 50\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.7976 LRAP: 0.9029 NDCG: 0.9362\n",
      "val Loss: 0.9254 LRAP: 0.8818 NDCG: 0.9147\n",
      "EarlyStopping counter: 38 out of 50\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.7978 LRAP: 0.9019 NDCG: 0.9351\n",
      "val Loss: 0.9263 LRAP: 0.8807 NDCG: 0.9144\n",
      "EarlyStopping counter: 39 out of 50\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.7964 LRAP: 0.9002 NDCG: 0.9357\n",
      "val Loss: 0.9252 LRAP: 0.8821 NDCG: 0.9140\n",
      "EarlyStopping counter: 40 out of 50\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.7957 LRAP: 0.9023 NDCG: 0.9361\n",
      "val Loss: 0.9267 LRAP: 0.8844 NDCG: 0.9152\n",
      "EarlyStopping counter: 41 out of 50\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.7970 LRAP: 0.9014 NDCG: 0.9354\n",
      "val Loss: 0.9244 LRAP: 0.8815 NDCG: 0.9141\n",
      "EarlyStopping counter: 42 out of 50\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.7981 LRAP: 0.9020 NDCG: 0.9354\n",
      "val Loss: 0.9247 LRAP: 0.8824 NDCG: 0.9143\n",
      "EarlyStopping counter: 43 out of 50\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.7990 LRAP: 0.8995 NDCG: 0.9350\n",
      "val Loss: 0.9270 LRAP: 0.8830 NDCG: 0.9148\n",
      "EarlyStopping counter: 44 out of 50\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.7968 LRAP: 0.9004 NDCG: 0.9349\n",
      "val Loss: 0.9267 LRAP: 0.8855 NDCG: 0.9166\n",
      "EarlyStopping counter: 45 out of 50\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.7970 LRAP: 0.9015 NDCG: 0.9356\n",
      "val Loss: 0.9231 LRAP: 0.8818 NDCG: 0.9157\n",
      "EarlyStopping counter: 46 out of 50\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.7967 LRAP: 0.8992 NDCG: 0.9342\n",
      "val Loss: 0.9243 LRAP: 0.8850 NDCG: 0.9163\n",
      "EarlyStopping counter: 47 out of 50\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.7975 LRAP: 0.9023 NDCG: 0.9357\n",
      "val Loss: 0.9255 LRAP: 0.8844 NDCG: 0.9163\n",
      "EarlyStopping counter: 48 out of 50\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.7979 LRAP: 0.9015 NDCG: 0.9360\n",
      "val Loss: 0.9254 LRAP: 0.8858 NDCG: 0.9157\n",
      "EarlyStopping counter: 49 out of 50\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.7983 LRAP: 0.8996 NDCG: 0.9344\n",
      "val Loss: 0.9256 LRAP: 0.8807 NDCG: 0.9146\n",
      "EarlyStopping counter: 50 out of 50\n",
      "Early stopping\n",
      "Training complete in 24m 20s\n",
      "Best val loss: 0.923024, best LRAP score: 0.9631\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders\n",
    "model = prep_model('SweetNet', classes) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes)\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'multilabel',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Try with my new glm-infused data\n",
    "\n",
    "classes = 15 # 15 kingdoms in the dataset, should read that from the data instead\n",
    "dataloaders = multilabel_kingdom_loaders_emb\n",
    "model = prep_model('SweetNet', classes, use_external_embeddings = True) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes) #changed to 0.01 from 0.005 from 0.0005\n",
    "model_ft = train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 1, mode = 'multilabel', use_external_embeddings = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5025834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweetnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
