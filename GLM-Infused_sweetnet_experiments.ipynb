{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2416a1b",
   "metadata": {},
   "source": [
    "# GLM-Infused SweetNet Experiments\n",
    "\n",
    "Experimenting with a modified version of SweetNet that allows it to take pre-trained embeddings as input. To get there I need a way to take the embeddings Iâ€™ve gotten from roman and transform them into nice inputs for the model, and a way to set the initial features using these inputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652c8f8",
   "metadata": {},
   "source": [
    "## Importing and exploring the GLM Embedding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: glm_embeddings_1.pkl\n",
      "Embeddings loaded successfully!\n",
      "Type of loaded object: <class 'dict'>\n",
      "Number of items (if dictionary): 2565\n",
      "Example keys (first 5): ['!GlcNAc', '-10', '-12', '-2', '-4']\n"
     ]
    }
   ],
   "source": [
    "# quick thing to load a pickle file\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            loaded_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2140bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of loaded object: <class 'dict'>\n",
      "Number of items (if dictionary): 2565\n",
      "Example keys (first 5): ['!GlcNAc', '-10', '-12', '-2', '-4']\n"
     ]
    }
   ],
   "source": [
    "# lets do some quick exploration\n",
    "\n",
    "# --- Explore the loaded data ---\n",
    "print(f\"Type of loaded object: {type(loaded_embeddings)}\")\n",
    "\n",
    "# Common formats for embeddings: dictionary or numpy array\n",
    "if isinstance(loaded_embeddings, dict):\n",
    "    print(f\"Number of items (if dictionary): {len(loaded_embeddings)}\")\n",
    "    # print some keys to see what they look like\n",
    "    print(f\"Example keys (first 5): {list(loaded_embeddings.keys())[:5]}\")\n",
    "elif hasattr(loaded_embeddings, 'shape'):\n",
    "    print(f\"Shape (if array/tensor): {loaded_embeddings.shape}\")\n",
    "    if hasattr(loaded_embeddings, 'dtype'):\n",
    "         print(f\"Data type (if array/tensor): {loaded_embeddings.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9e6aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-6', '-8', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic', '1,5-Anhydro-Man-ol', '1,5-Anhydro-ManNAc-ol', '1,5-Anhydro-Xyl-ol', '1,5-Anhydro-Xyl2F-ol', '1-1', '1-2', '1-3', '1-4', '1-5', '1-6']\n"
     ]
    }
   ],
   "source": [
    "print(list(loaded_embeddings.keys())[5:30]) # Print more keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1904e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of value for '!GlcNAc': <class 'numpy.ndarray'>\n",
      "Shape of value: (320,)\n",
      "Dtype of value: float32\n",
      "[ 9.33886290e-01 -7.57189512e-01 -5.22765040e-01  4.93726492e-01\n",
      "  3.03156078e-01 -1.72754931e+00  2.03015614e+00 -1.13539708e+00\n",
      " -8.32044244e-01 -6.09763384e-01 -5.63947335e-02 -2.68140852e-01\n",
      " -6.37493312e-01  1.45667583e-01 -7.75620103e-01 -1.39048725e-01\n",
      "  1.06042847e-01 -3.74972522e-01  7.91566074e-01 -1.03034627e+00\n",
      " -1.12639211e-01 -3.78986076e-03  5.92547238e-01  2.81559825e-01\n",
      " -5.21002829e-01  9.35327411e-01  2.56601274e-01 -3.91364455e-01\n",
      "  2.72188634e-02  5.00928342e-01 -5.55309415e-01  1.28289807e+00\n",
      " -6.45282388e-01  5.19899249e-01  6.10100806e-01  1.84122849e+00\n",
      "  3.11432898e-01 -7.64928609e-02 -1.05589128e+00  6.50653005e-01\n",
      "  9.70111132e-01  7.40227938e-01  8.39829683e-01 -3.04328918e-01\n",
      " -1.06630003e+00  4.53770608e-01  4.27673876e-01 -6.02427721e-01\n",
      "  4.39536482e-01 -1.16493046e+00 -2.04154789e-01  1.13036299e+00\n",
      "  2.51586974e-01  1.04393315e+00  2.60879964e-01  4.63881493e-02\n",
      "  8.49927664e-01 -1.21275747e+00 -5.25301337e-01 -7.54553556e-01\n",
      " -5.36846638e-01  1.71898973e+00  1.07118464e+00  1.25938666e+00\n",
      "  7.28268623e-01  2.50012755e-01 -8.84264708e-01  3.54878515e-01\n",
      " -9.51814711e-01  1.92197442e-01  6.22674108e-01 -7.19715357e-02\n",
      " -2.53418744e-01  6.10054433e-01 -1.37844992e+00  1.10613918e+00\n",
      " -7.89550483e-01  4.11728621e-01 -1.39660871e+00 -2.74130464e-01\n",
      " -4.85218346e-01 -1.64008796e+00 -2.54515797e-01 -4.76354361e-02\n",
      "  1.70321250e+00  1.37953115e+00  6.62403643e-01  1.23904690e-01\n",
      " -2.03382596e-02  2.49572158e-01 -1.19476050e-01  1.01610112e+00\n",
      "  1.54832602e-01 -3.18885893e-01  1.02479362e+00  2.19304472e-01\n",
      " -1.77515924e-01 -2.96848416e-01 -1.51161349e+00 -1.55658543e+00\n",
      "  6.01615787e-01 -1.18876457e+00  6.75462842e-01 -1.21065450e+00\n",
      "  1.00956786e+00  5.41580915e-01  4.89682317e-01 -4.31063682e-01\n",
      " -6.99561596e-01 -9.50598717e-01 -4.71236914e-01  8.96337509e-01\n",
      "  1.97975963e-01  6.51351273e-01 -1.65811467e+00 -2.37476051e-01\n",
      "  1.22424424e+00  3.85935336e-01  1.74970782e+00  1.08295810e+00\n",
      " -2.08416104e-01 -1.44780791e+00 -3.18115175e-01 -2.69204080e-02\n",
      " -7.30906725e-01  3.65380794e-01 -5.23220778e-01 -1.59638667e+00\n",
      "  9.76120412e-01  4.75375116e-01  1.10794783e+00 -9.16275680e-01\n",
      "  8.67535770e-01 -2.21260801e-01  3.58714461e-02 -1.62487292e+00\n",
      "  9.47338939e-01  2.52621353e-01 -2.44861484e-01  4.85217899e-01\n",
      " -1.72671735e-01  1.49431840e-01 -9.26872373e-01 -6.38668120e-01\n",
      " -1.37115136e-01  1.30791855e+00  1.25448748e-01  3.05962026e-01\n",
      " -2.51638025e-01  6.88706279e-01 -6.43941760e-01  6.10008895e-01\n",
      "  2.45932966e-01  1.53176570e+00 -2.05617994e-01  5.01646757e-01\n",
      " -4.11370814e-01 -5.36742508e-01 -1.23477876e-02  6.50121808e-01\n",
      " -3.78578186e-01  6.62264466e-01  1.53327346e-01 -9.97333288e-01\n",
      "  2.86916673e-01 -3.98133188e-01  1.19174033e-01 -1.07086766e+00\n",
      "  5.68605885e-02  8.55352730e-02 -2.43456244e-01 -5.13940752e-01\n",
      "  9.52608764e-01 -3.56329709e-01 -9.76832956e-02  1.55454218e-01\n",
      "  1.07665420e-01  7.78901517e-01  1.94103813e+00  5.98729789e-01\n",
      "  1.49250478e-01  6.60319090e-01 -9.16693985e-01 -1.80390513e+00\n",
      " -1.08837974e+00 -5.85823774e-01 -5.17625034e-01  1.13187218e+00\n",
      " -3.11186165e-01 -1.56313211e-01  4.89638031e-01  6.32191420e-01\n",
      " -9.01452422e-01  3.40963513e-01  3.77618819e-01  4.78747129e-01\n",
      " -1.26142776e+00  1.63014054e+00 -7.38181099e-02 -8.88819635e-01\n",
      " -9.81908560e-01 -3.11309278e-01 -2.87041283e+00 -6.68797910e-01\n",
      "  1.15292573e+00  1.82262063e+00  6.86679184e-01  3.54639411e-01\n",
      "  1.14279723e+00  1.23592412e+00 -4.26488072e-01  5.78116417e-01\n",
      "  2.67315298e-01  1.73516899e-01 -6.95198655e-01 -7.84443021e-01\n",
      "  1.87699527e-01  7.76465774e-01  1.17747712e+00  2.98208922e-01\n",
      "  1.80739570e+00 -6.55146241e-02  2.10267353e+00 -1.49224257e+00\n",
      "  1.67633876e-01 -5.96812427e-01  4.02143002e-01 -5.80711842e-01\n",
      " -6.86030865e-01  2.82077312e-01  4.62324202e-01 -8.51680398e-01\n",
      " -6.37305975e-01 -1.97909772e-01  8.27008903e-01 -2.47440666e-01\n",
      "  5.40550411e-01  2.20697820e-02 -3.67172241e-01  1.37753654e+00\n",
      "  2.57560164e-01  1.12044883e+00  1.47008979e+00 -3.09366286e-01\n",
      "  1.41206241e+00 -1.07911384e+00 -3.82883579e-01  1.15288660e-01\n",
      "  6.46931171e-01 -1.63524508e+00 -4.82143342e-01 -2.22676694e-02\n",
      " -2.94011176e-01  1.76649165e+00 -1.42879653e+00 -1.01673603e+00\n",
      "  6.92535341e-01  1.08943865e-01 -1.51619220e+00 -1.31418991e+00\n",
      " -5.36556542e-01 -9.08092409e-02 -3.43192220e-02 -5.01663029e-01\n",
      " -4.27816272e-01  5.04320741e-01 -8.19638968e-01  1.27975166e-01\n",
      "  6.98855758e-01  4.11748588e-01 -2.63869703e-01 -1.72789741e+00\n",
      "  2.40177006e-01 -3.30802739e-01  1.47785515e-01  4.70187128e-01\n",
      "  3.38367313e-01 -1.54152012e+00  3.17173868e-01 -1.70832485e-01\n",
      "  9.85031009e-01 -1.51257575e+00  7.86181986e-01  2.95546353e-01\n",
      "  4.57608998e-02 -6.43859148e-01  4.83155847e-01 -1.51108074e+00\n",
      " -1.82736918e-01 -3.47120881e-01 -5.70403397e-01 -1.21720120e-01\n",
      " -1.61197579e+00  1.02913380e-03 -4.93016541e-02 -1.70051694e+00\n",
      " -4.81017500e-01 -9.90746021e-01  3.51191968e-01 -6.38143182e-01\n",
      "  8.80924284e-01  1.06428635e+00 -1.31740403e+00 -1.46576715e+00\n",
      " -8.72395873e-01  1.48068953e+00 -2.76599586e-01 -1.15330029e+00\n",
      "  1.45732999e-01 -1.63671541e+00  2.22910285e-01 -3.31862628e-01\n",
      "  5.65533102e-01 -4.64938819e-01  1.83547580e+00 -7.03186333e-01\n",
      "  2.57217407e-01  1.83000445e+00  1.64521456e-01  1.26764941e+00]\n"
     ]
    }
   ],
   "source": [
    "example_key = '!GlcNAc' \n",
    "if example_key in loaded_embeddings:\n",
    "    embedding_vector = loaded_embeddings[example_key]\n",
    "    print(f\"Type of value for '{example_key}': {type(embedding_vector)}\")\n",
    "    if hasattr(embedding_vector, 'shape'):\n",
    "        print(f\"Shape of value: {embedding_vector.shape}\") # This gives dimensionality!\n",
    "        print(f\"Dtype of value: {embedding_vector.dtype}\")\n",
    "    print(embedding_vector) # Print the vector itself if it's not too long\n",
    "else:\n",
    "    print(f\"Key '{example_key}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ed7f66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'other': 122, 'linkage_or_modification': 36, 'monosaccharide': 2407})\n"
     ]
    }
   ],
   "source": [
    "# let's look at the keys a bit more closely\n",
    "\n",
    "import collections\n",
    "\n",
    "key_types = collections.defaultdict(int)\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        key_types['linkage_or_modification'] += 1\n",
    "    elif key[0].isalpha():\n",
    "        key_types['monosaccharide'] += 1\n",
    "    else:\n",
    "        key_types['other'] += 1\n",
    "\n",
    "print(key_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd6a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'other' keys: 122\n",
      "Examples of 'other' keys: ['!GlcNAc', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic', '1,5-Anhydro-Man-ol', '1,5-Anhydro-ManNAc-ol', '1,5-Anhydro-Xyl-ol', '1,5-Anhydro-Xyl2F-ol', '1b-4', '1dAlt-ol']\n"
     ]
    }
   ],
   "source": [
    "# Let's explore those Other keys \n",
    "\n",
    "other_keys = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        other_keys.append(key)\n",
    "\n",
    "print(f\"Number of 'other' keys: {len(other_keys)}\")\n",
    "print(f\"Examples of 'other' keys: {other_keys[:20]}\") # Print the first 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "398bd9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More Examples of 'other' keys: ['1dEry-ol', '2,3-Anhydro-All', '2,3-Anhydro-Man', '2,3-Anhydro-Rib', '2,5-Anhydro-D-Alt-ol', '2,5-Anhydro-D-Alt3S-ol', '2,5-Anhydro-D-Tal', '2,5-Anhydro-Glc', '2,5-Anhydro-L-Man-ol', '2,5-Anhydro-Man', '2,5-Anhydro-Man-ol', '2,5-Anhydro-Man1S-ol', '2,5-Anhydro-Man3S-ol', '2,5-Anhydro-Man6S', '2,5-Anhydro-Tal-ol', '2,5-Anhydro-Tal6P', '2,6-Anhydro-Glc5NAc-ol', '2,6-Anhydro-L-Gul-ol', '2,6-Anhydro-L-Gul-onic', '2,6-Anhydro-Man-ol', '2,6-Anhydro-Tal5NAc-ol', '2,7-Anhydro-Kdo', '2,7-Anhydro-Kdof', '2dAraHexA', '3,6-Anhydro-Fruf', '3,6-Anhydro-Gal', '3,6-Anhydro-Gal2S', '3,6-Anhydro-Glc', '3,6-Anhydro-L-Gal', '3,6-Anhydro-L-Gal2Me', '3-Anhydro-Gal', '3-Anhydro-Gal2S', '3dFuc', '3dGal', '3dLyxHep-ulosaric', '4,7-Anhydro-Kdo', '4,7-Anhydro-KdoOPEtN', '4,8-Anhydro-Kdo', '4d8dNeu5Ac', '4dAraHex', '4dEry-ol', '4dFuc', '4dGal', '4dNeu5Ac', '4dThrHexNAcA4en', '4eLeg5Ac7Ac', '5dAraf', '5dAraf3Me', '5dLyxf3CFo', '5dLyxf3CMe']\n"
     ]
    }
   ],
   "source": [
    "# Let's look at 50 more keys\n",
    "\n",
    "print(f\"More Examples of 'other' keys: {other_keys[20:70]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ac3a766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'monosaccharide' keys: 2407\n",
      "Examples of 'monosaccharide' keys: ['Abe', 'Abe1PP', 'Abe2Ac', 'AbeOAc', 'Acarbose', 'AcefA', 'Aci5Ac7Ac', 'AcoNAc', 'All', 'All-ol', 'All1S2S3S4S', 'All2Ac3Ac', 'All2S3S4S', 'All3Ac', 'All6Ac', 'AllN', 'AllN1P', 'AllNAc', 'AllNAc6Me', 'AllOMe', 'Alt', 'AltA', 'AltA2N', 'AltA2S', 'AltAN', 'AltNAc', 'AltNAcA', 'AltNAcA1Prop', 'Altf', 'AltfOAc', 'Amikacin', 'Api', 'ApiOAc', 'ApiOMe-ol', 'Apif', 'Ara', 'Ara-ol', 'Ara1Cer2Ac', 'Ara1Me', 'Ara1N4P', 'Ara1P4N', 'Ara1PP', 'Ara1PP2NAc', 'Ara1PP4N', 'Ara1PP4NFo', 'Ara2Ac', 'Ara2Ac3Ac', 'Ara2Ac3Ac4Ac', 'Ara2Ac4Ac', 'Ara2Ac5P-ol']\n"
     ]
    }
   ],
   "source": [
    "# Let's explore those monosaccharide keys\n",
    "monosaccharide = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        monosaccharide.append(key)\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'monosaccharide' keys: {len(monosaccharide)}\")\n",
    "print(f\"Examples of 'monosaccharide' keys: {monosaccharide[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d2ddd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'linkage_or_modification' keys: 36\n",
      "Examples of 'linkage_or_modification' keys: ['-10', '-12', '-2', '-4', '-6', '-8', '1-1', '1-2', '1-3', '1-4', '1-5', '1-6', '1-?', '2-3', '2-4', '2-5', '2-6', '3-1', '3-5', '4-1', '4-5', '5-1', '5-2', '5-3', '5-4', '5-5', '5-6', '6-1', '6-3', '6-4', '?1-2', '?1-3', '?1-4', '?1-6', '?1-?', '?2-?']\n"
     ]
    }
   ],
   "source": [
    "# To be throughough, let's look at 50 Linkage or Modification keys as well\n",
    "linkage_or_modification = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        linkage_or_modification.append(key)\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'linkage_or_modification' keys: {len(linkage_or_modification)}\")\n",
    "print(f\"Examples of 'linkage_or_modification' keys: {linkage_or_modification[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9ad5a",
   "metadata": {},
   "source": [
    "### Load the glycowork library\n",
    "\n",
    "I'll load the glycowork library and compare it to the keys in the embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84b14506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in glycowork vocabulary: 2565\n",
      "Example keys from glycowork vocabulary (first 20): ['!GlcNAc', '-10', '-12', '-2', '-4', '-6', '-8', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic']\n"
     ]
    }
   ],
   "source": [
    "from glycowork.glycan_data import loader\n",
    "\n",
    "glycowork_vocabulary = loader.lib\n",
    "\n",
    "print(f\"Number of items in glycowork vocabulary: {len(glycowork_vocabulary)}\")\n",
    "print(f\"Example keys from glycowork vocabulary (first 20): {list(glycowork_vocabulary.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e1f3d",
   "metadata": {},
   "source": [
    "Nice, they seem to correspond one to one!\n",
    "\n",
    "That saves me a lot of work down the line (Thanks Roman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8434257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of value for '-10': <class 'int'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# let's look at one of the keys in the glycowork vocabulary to see what they return\n",
    "example_glycowork_key = '-10'\n",
    "if example_glycowork_key in glycowork_vocabulary:\n",
    "    glycowork_value = glycowork_vocabulary[example_glycowork_key]\n",
    "    print(f\"Type of value for '{example_glycowork_key}': {type(glycowork_value)}\")\n",
    "    print(glycowork_value)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a5ce2",
   "metadata": {},
   "source": [
    "### Filter and Transform embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fa117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59b56aa7",
   "metadata": {},
   "source": [
    "## SweetNet copy from models.py for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5658e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SweetNet class\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Union, Literal\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.nn import GraphConv\n",
    "    from torch_geometric.nn import global_mean_pool as gap\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "  raise ImportError(\"<torch or torch_geometric missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from glycowork.glycan_data.loader import lib, download_model \n",
    "\n",
    "class SweetNet(torch.nn.Module):\n",
    "    def __init__(self, lib_size: int, # number of unique tokens for graph nodes\n",
    "                 num_classes: int = 1, # number of output classes (>1 for multilabel)\n",
    "                 hidden_dim: int = 128 # dimension of hidden layers\n",
    "                ) -> None:\n",
    "        \"given glycan graphs as input, predicts properties via a graph neural network\"\n",
    "        print(\"Using SweetNet from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "        super(SweetNet, self).__init__()\n",
    "        # Convolution operations on the graph\n",
    "        self.conv1 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GraphConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Node embedding\n",
    "        self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "        # Fully connected part\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, 1024)\n",
    "        self.lin2 = torch.nn.Linear(1024, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, num_classes)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(1024)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.act1 = torch.nn.LeakyReLU()\n",
    "        self.act2 = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor,\n",
    "                inference: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        # Getting node features\n",
    "        x = self.item_embedding(x)\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        # Graph convolution operations\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = gap(x, batch)\n",
    "\n",
    "        # Fully connected part\n",
    "        x = self.act1(self.bn1(self.lin1(x)))\n",
    "        x_out = self.bn2(self.lin2(x))\n",
    "        x = F.dropout(self.act2(x_out), p = 0.5, training = self.training)\n",
    "\n",
    "        x = self.lin3(x).squeeze(1)\n",
    "\n",
    "        if inference:\n",
    "          return x, x_out\n",
    "        else:\n",
    "          return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a375b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init_weights function\n",
    "\n",
    "def init_weights(model: torch.nn.Module, # neural network for analyzing glycans\n",
    "                mode: str = 'sparse', # initialization algorithm: 'sparse', 'kaiming', 'xavier'\n",
    "                sparsity: float = 0.1 # proportion of sparsity after initialization\n",
    "               ) -> None:\n",
    "    \"initializes linear layers of PyTorch model with a weight initialization\"\n",
    "    print(\"Using init_weights from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if isinstance(model, torch.nn.Linear):\n",
    "        if mode == 'sparse':\n",
    "            torch.nn.init.sparse_(model.weight, sparsity = sparsity)\n",
    "        elif mode == 'kaiming':\n",
    "            torch.nn.init.kaiming_uniform_(model.weight)\n",
    "        elif mode == 'xavier':\n",
    "            torch.nn.init.xavier_uniform_(model.weight)\n",
    "        else:\n",
    "            print(\"This initialization option is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a059da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_model function\n",
    "\n",
    "def prep_model(model_type: Literal[\"SweetNet\", \"LectinOracle\", \"LectinOracle_flex\", \"NSequonPred\"], # type of model to create\n",
    "              num_classes: int, # number of unique classes for classification\n",
    "              libr: Optional[Dict[str, int]] = None, # dictionary of form glycoletter:index\n",
    "              trained: bool = False, # whether to use pretrained model\n",
    "              hidden_dim: int = 128 # hidden dimension for the model (SweetNet/LectinOracle only)\n",
    "             ) -> torch.nn.Module: # initialized PyTorch model\n",
    "    \"wrapper to instantiate model, initialize it, and put it on the GPU\"\n",
    "    print(\"Using prep_model from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if libr is None:\n",
    "      libr = lib\n",
    "    if model_type == 'SweetNet':\n",
    "      model = SweetNet(len(libr), num_classes = num_classes, hidden_dim = hidden_dim)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'sparse'))\n",
    "      if trained:\n",
    "        if hidden_dim != 128:\n",
    "          raise ValueError(\"Hidden dimension must be 128 for pretrained model\")\n",
    "        model_path = download_model(\"glycowork_sweetnet_species.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle':\n",
    "      model = LectinOracle(len(libr), num_classes = num_classes, input_size_prot = int(10*hidden_dim))\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle_flex':\n",
    "      model = LectinOracle_flex(len(libr), num_classes = num_classes)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle_flex.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'NSequonPred':\n",
    "      model = NSequonPred()\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"NSequonPred_batch32.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    else:\n",
    "      print(\"Invalid Model Type\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbbca33",
   "metadata": {},
   "source": [
    "## Testing using same framework as iteration 0 (basic kingdom sweetnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300d8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# testing the modified SweetNet model on the GlycoWork dataset \n",
    "from glycowork.glycan_data.loader import df_species\n",
    "from glycowork.ml.train_test_split import hierarchy_filter\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "from glycowork.ml import model_training\n",
    "\n",
    "# silence the avalanche of \"undefined\" warnings\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "train_x, val_x, train_y, val_y, id_val, class_list, class_converter = hierarchy_filter(df_species,\n",
    "                                                                                       rank = 'Kingdom')\n",
    "\n",
    "dataloaders = split_data_to_train(train_x, val_x, train_y, val_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812c197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prep_model from notebook cell!\n",
      "Using SweetNet from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Using init_weights from notebook cell!\n",
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.9425 Accuracy: 0.6660 MCC: 0.4531\n",
      "val Loss: 1.4610 Accuracy: 0.7855 MCC: 0.6738\n",
      "Validation loss decreased (0.000000 --> 1.461049).  Saving model ...\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.4484 Accuracy: 0.8158 MCC: 0.7211\n",
      "val Loss: 1.3235 Accuracy: 0.8331 MCC: 0.7454\n",
      "Validation loss decreased (1.461049 --> 1.323499).  Saving model ...\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.3499 Accuracy: 0.8400 MCC: 0.7581\n",
      "val Loss: 1.2732 Accuracy: 0.8474 MCC: 0.7664\n",
      "Validation loss decreased (1.323499 --> 1.273244).  Saving model ...\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.3025 Accuracy: 0.8500 MCC: 0.7737\n",
      "val Loss: 1.2279 Accuracy: 0.8573 MCC: 0.7830\n",
      "Validation loss decreased (1.273244 --> 1.227869).  Saving model ...\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.2616 Accuracy: 0.8596 MCC: 0.7883\n",
      "val Loss: 1.2020 Accuracy: 0.8595 MCC: 0.7864\n",
      "Validation loss decreased (1.227869 --> 1.202015).  Saving model ...\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.2356 Accuracy: 0.8642 MCC: 0.7948\n",
      "val Loss: 1.1934 Accuracy: 0.8642 MCC: 0.7932\n",
      "Validation loss decreased (1.202015 --> 1.193445).  Saving model ...\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.2150 Accuracy: 0.8725 MCC: 0.8075\n",
      "val Loss: 1.1804 Accuracy: 0.8685 MCC: 0.8011\n",
      "Validation loss decreased (1.193445 --> 1.180360).  Saving model ...\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.1936 Accuracy: 0.8739 MCC: 0.8096\n",
      "val Loss: 1.1565 Accuracy: 0.8726 MCC: 0.8062\n",
      "Validation loss decreased (1.180360 --> 1.156524).  Saving model ...\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.1757 Accuracy: 0.8770 MCC: 0.8148\n",
      "val Loss: 1.1470 Accuracy: 0.8742 MCC: 0.8092\n",
      "Validation loss decreased (1.156524 --> 1.147003).  Saving model ...\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.1547 Accuracy: 0.8828 MCC: 0.8243\n",
      "val Loss: 1.1407 Accuracy: 0.8812 MCC: 0.8198\n",
      "Validation loss decreased (1.147003 --> 1.140727).  Saving model ...\n",
      "\n",
      "Training complete in 1m 34s\n",
      "Best val loss: 1.140727, best Accuracy score: 0.8812\n"
     ]
    }
   ],
   "source": [
    "# Lets split out the training function so I don't have to load the data each time\n",
    "\n",
    "model = prep_model('SweetNet', len(class_list))\n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = len(class_list))\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 10, return_metrics = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdade799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweetnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
