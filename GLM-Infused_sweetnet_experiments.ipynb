{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2416a1b",
   "metadata": {},
   "source": [
    "# GLM-Infused SweetNet Experiments\n",
    "\n",
    "Experimenting with a modified version of SweetNet that allows it to take pre-trained embeddings as input. To get there I need a way to take the embeddings Iâ€™ve gotten from roman and transform them into nice inputs for the model, and a way to set the initial features using these inputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652c8f8",
   "metadata": {},
   "source": [
    "## Importing and exploring the GLM Embedding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0485fe3",
   "metadata": {},
   "source": [
    "### Exploring embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: glm_embeddings_1.pkl\n",
      "Embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# quick thing to load a pickle file\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            loaded_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of loaded object: <class 'dict'>\n",
      "Number of items (if dictionary): 2565\n",
      "Example keys (first 5): ['!GlcNAc', '-10', '-12', '-2', '-4']\n"
     ]
    }
   ],
   "source": [
    "# lets do some quick exploration\n",
    "\n",
    "# --- Explore the loaded data ---\n",
    "print(f\"Type of loaded object: {type(loaded_embeddings)}\")\n",
    "\n",
    "# Common formats for embeddings: dictionary or numpy array\n",
    "if isinstance(loaded_embeddings, dict):\n",
    "    print(f\"Number of items (if dictionary): {len(loaded_embeddings)}\")\n",
    "    # print some keys to see what they look like\n",
    "    print(f\"Example keys (first 5): {list(loaded_embeddings.keys())[:5]}\")\n",
    "elif hasattr(loaded_embeddings, 'shape'):\n",
    "    print(f\"Shape (if array/tensor): {loaded_embeddings.shape}\")\n",
    "    if hasattr(loaded_embeddings, 'dtype'):\n",
    "         print(f\"Data type (if array/tensor): {loaded_embeddings.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e6aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-6', '-8', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic', '1,5-Anhydro-Man-ol', '1,5-Anhydro-ManNAc-ol', '1,5-Anhydro-Xyl-ol', '1,5-Anhydro-Xyl2F-ol', '1-1', '1-2', '1-3', '1-4', '1-5', '1-6']\n"
     ]
    }
   ],
   "source": [
    "print(list(loaded_embeddings.keys())[5:30]) # Print more keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of value for '-8': <class 'numpy.ndarray'>\n",
      "Shape of value: (320,)\n",
      "Dtype of value: float32\n",
      "[ 9.33886290e-01 -7.57189512e-01 -5.22765040e-01  4.93726492e-01\n",
      "  3.03156078e-01 -1.72754931e+00  2.03015614e+00 -1.13539708e+00\n",
      " -8.32044244e-01 -6.09763384e-01 -5.63947335e-02 -2.68140852e-01\n",
      " -6.37493312e-01  1.45667583e-01 -7.75620103e-01 -1.39048725e-01\n",
      "  1.06042847e-01 -3.74972522e-01  7.91566074e-01 -1.03034627e+00\n",
      " -1.12639211e-01 -3.78986076e-03  5.92547238e-01  2.81559825e-01\n",
      " -5.21002829e-01  9.35327411e-01  2.56601274e-01 -3.91364455e-01\n",
      "  2.72188634e-02  5.00928342e-01 -5.55309415e-01  1.28289807e+00\n",
      " -6.45282388e-01  5.19899249e-01  6.10100806e-01  1.84122849e+00\n",
      "  3.11432898e-01 -7.64928609e-02 -1.05589128e+00  6.50653005e-01\n",
      "  9.70111132e-01  7.40227938e-01  8.39829683e-01 -3.04328918e-01\n",
      " -1.06630003e+00  4.53770608e-01  4.27673876e-01 -6.02427721e-01\n",
      "  4.39536482e-01 -1.16493046e+00 -2.04154789e-01  1.13036299e+00\n",
      "  2.51586974e-01  1.04393315e+00  2.60879964e-01  4.63881493e-02\n",
      "  8.49927664e-01 -1.21275747e+00 -5.25301337e-01 -7.54553556e-01\n",
      " -5.36846638e-01  1.71898973e+00  1.07118464e+00  1.25938666e+00\n",
      "  7.28268623e-01  2.50012755e-01 -8.84264708e-01  3.54878515e-01\n",
      " -9.51814711e-01  1.92197442e-01  6.22674108e-01 -7.19715357e-02\n",
      " -2.53418744e-01  6.10054433e-01 -1.37844992e+00  1.10613918e+00\n",
      " -7.89550483e-01  4.11728621e-01 -1.39660871e+00 -2.74130464e-01\n",
      " -4.85218346e-01 -1.64008796e+00 -2.54515797e-01 -4.76354361e-02\n",
      "  1.70321250e+00  1.37953115e+00  6.62403643e-01  1.23904690e-01\n",
      " -2.03382596e-02  2.49572158e-01 -1.19476050e-01  1.01610112e+00\n",
      "  1.54832602e-01 -3.18885893e-01  1.02479362e+00  2.19304472e-01\n",
      " -1.77515924e-01 -2.96848416e-01 -1.51161349e+00 -1.55658543e+00\n",
      "  6.01615787e-01 -1.18876457e+00  6.75462842e-01 -1.21065450e+00\n",
      "  1.00956786e+00  5.41580915e-01  4.89682317e-01 -4.31063682e-01\n",
      " -6.99561596e-01 -9.50598717e-01 -4.71236914e-01  8.96337509e-01\n",
      "  1.97975963e-01  6.51351273e-01 -1.65811467e+00 -2.37476051e-01\n",
      "  1.22424424e+00  3.85935336e-01  1.74970782e+00  1.08295810e+00\n",
      " -2.08416104e-01 -1.44780791e+00 -3.18115175e-01 -2.69204080e-02\n",
      " -7.30906725e-01  3.65380794e-01 -5.23220778e-01 -1.59638667e+00\n",
      "  9.76120412e-01  4.75375116e-01  1.10794783e+00 -9.16275680e-01\n",
      "  8.67535770e-01 -2.21260801e-01  3.58714461e-02 -1.62487292e+00\n",
      "  9.47338939e-01  2.52621353e-01 -2.44861484e-01  4.85217899e-01\n",
      " -1.72671735e-01  1.49431840e-01 -9.26872373e-01 -6.38668120e-01\n",
      " -1.37115136e-01  1.30791855e+00  1.25448748e-01  3.05962026e-01\n",
      " -2.51638025e-01  6.88706279e-01 -6.43941760e-01  6.10008895e-01\n",
      "  2.45932966e-01  1.53176570e+00 -2.05617994e-01  5.01646757e-01\n",
      " -4.11370814e-01 -5.36742508e-01 -1.23477876e-02  6.50121808e-01\n",
      " -3.78578186e-01  6.62264466e-01  1.53327346e-01 -9.97333288e-01\n",
      "  2.86916673e-01 -3.98133188e-01  1.19174033e-01 -1.07086766e+00\n",
      "  5.68605885e-02  8.55352730e-02 -2.43456244e-01 -5.13940752e-01\n",
      "  9.52608764e-01 -3.56329709e-01 -9.76832956e-02  1.55454218e-01\n",
      "  1.07665420e-01  7.78901517e-01  1.94103813e+00  5.98729789e-01\n",
      "  1.49250478e-01  6.60319090e-01 -9.16693985e-01 -1.80390513e+00\n",
      " -1.08837974e+00 -5.85823774e-01 -5.17625034e-01  1.13187218e+00\n",
      " -3.11186165e-01 -1.56313211e-01  4.89638031e-01  6.32191420e-01\n",
      " -9.01452422e-01  3.40963513e-01  3.77618819e-01  4.78747129e-01\n",
      " -1.26142776e+00  1.63014054e+00 -7.38181099e-02 -8.88819635e-01\n",
      " -9.81908560e-01 -3.11309278e-01 -2.87041283e+00 -6.68797910e-01\n",
      "  1.15292573e+00  1.82262063e+00  6.86679184e-01  3.54639411e-01\n",
      "  1.14279723e+00  1.23592412e+00 -4.26488072e-01  5.78116417e-01\n",
      "  2.67315298e-01  1.73516899e-01 -6.95198655e-01 -7.84443021e-01\n",
      "  1.87699527e-01  7.76465774e-01  1.17747712e+00  2.98208922e-01\n",
      "  1.80739570e+00 -6.55146241e-02  2.10267353e+00 -1.49224257e+00\n",
      "  1.67633876e-01 -5.96812427e-01  4.02143002e-01 -5.80711842e-01\n",
      " -6.86030865e-01  2.82077312e-01  4.62324202e-01 -8.51680398e-01\n",
      " -6.37305975e-01 -1.97909772e-01  8.27008903e-01 -2.47440666e-01\n",
      "  5.40550411e-01  2.20697820e-02 -3.67172241e-01  1.37753654e+00\n",
      "  2.57560164e-01  1.12044883e+00  1.47008979e+00 -3.09366286e-01\n",
      "  1.41206241e+00 -1.07911384e+00 -3.82883579e-01  1.15288660e-01\n",
      "  6.46931171e-01 -1.63524508e+00 -4.82143342e-01 -2.22676694e-02\n",
      " -2.94011176e-01  1.76649165e+00 -1.42879653e+00 -1.01673603e+00\n",
      "  6.92535341e-01  1.08943865e-01 -1.51619220e+00 -1.31418991e+00\n",
      " -5.36556542e-01 -9.08092409e-02 -3.43192220e-02 -5.01663029e-01\n",
      " -4.27816272e-01  5.04320741e-01 -8.19638968e-01  1.27975166e-01\n",
      "  6.98855758e-01  4.11748588e-01 -2.63869703e-01 -1.72789741e+00\n",
      "  2.40177006e-01 -3.30802739e-01  1.47785515e-01  4.70187128e-01\n",
      "  3.38367313e-01 -1.54152012e+00  3.17173868e-01 -1.70832485e-01\n",
      "  9.85031009e-01 -1.51257575e+00  7.86181986e-01  2.95546353e-01\n",
      "  4.57608998e-02 -6.43859148e-01  4.83155847e-01 -1.51108074e+00\n",
      " -1.82736918e-01 -3.47120881e-01 -5.70403397e-01 -1.21720120e-01\n",
      " -1.61197579e+00  1.02913380e-03 -4.93016541e-02 -1.70051694e+00\n",
      " -4.81017500e-01 -9.90746021e-01  3.51191968e-01 -6.38143182e-01\n",
      "  8.80924284e-01  1.06428635e+00 -1.31740403e+00 -1.46576715e+00\n",
      " -8.72395873e-01  1.48068953e+00 -2.76599586e-01 -1.15330029e+00\n",
      "  1.45732999e-01 -1.63671541e+00  2.22910285e-01 -3.31862628e-01\n",
      "  5.65533102e-01 -4.64938819e-01  1.83547580e+00 -7.03186333e-01\n",
      "  2.57217407e-01  1.83000445e+00  1.64521456e-01  1.26764941e+00]\n"
     ]
    }
   ],
   "source": [
    "example_key = '!GlcNAc' \n",
    "if example_key in loaded_embeddings:\n",
    "    embedding_vector = loaded_embeddings[example_key]\n",
    "    print(f\"Type of value for '{example_key}': {type(embedding_vector)}\")\n",
    "    if hasattr(embedding_vector, 'shape'):\n",
    "        print(f\"Shape of value: {embedding_vector.shape}\") # This gives dimensionality!\n",
    "        print(f\"Dtype of value: {embedding_vector.dtype}\")\n",
    "    print(embedding_vector) # Print the vector itself if it's not too long\n",
    "else:\n",
    "    print(f\"Key '{example_key}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7f66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'other': 122, 'linkage_or_modification': 36, 'monosaccharide': 2407})\n"
     ]
    }
   ],
   "source": [
    "# let's look at the keys a bit more closely\n",
    "\n",
    "import collections\n",
    "\n",
    "key_types = collections.defaultdict(int)\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        key_types['linkage_or_modification'] += 1\n",
    "    elif key[0].isalpha():\n",
    "        key_types['monosaccharide'] += 1\n",
    "    else:\n",
    "        key_types['other'] += 1\n",
    "\n",
    "print(key_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd6a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'other' keys: 122\n",
      "Examples of 'other' keys: ['!GlcNAc', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic', '1,5-Anhydro-Man-ol', '1,5-Anhydro-ManNAc-ol', '1,5-Anhydro-Xyl-ol', '1,5-Anhydro-Xyl2F-ol', '1b-4', '1dAlt-ol']\n"
     ]
    }
   ],
   "source": [
    "# Let's explore those Other keys \n",
    "\n",
    "other_keys = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        other_keys.append(key)\n",
    "\n",
    "print(f\"Number of 'other' keys: {len(other_keys)}\")\n",
    "print(f\"Examples of 'other' keys: {other_keys[:20]}\") # Print the first 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398bd9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More Examples of 'other' keys: ['1dEry-ol', '2,3-Anhydro-All', '2,3-Anhydro-Man', '2,3-Anhydro-Rib', '2,5-Anhydro-D-Alt-ol', '2,5-Anhydro-D-Alt3S-ol', '2,5-Anhydro-D-Tal', '2,5-Anhydro-Glc', '2,5-Anhydro-L-Man-ol', '2,5-Anhydro-Man', '2,5-Anhydro-Man-ol', '2,5-Anhydro-Man1S-ol', '2,5-Anhydro-Man3S-ol', '2,5-Anhydro-Man6S', '2,5-Anhydro-Tal-ol', '2,5-Anhydro-Tal6P', '2,6-Anhydro-Glc5NAc-ol', '2,6-Anhydro-L-Gul-ol', '2,6-Anhydro-L-Gul-onic', '2,6-Anhydro-Man-ol', '2,6-Anhydro-Tal5NAc-ol', '2,7-Anhydro-Kdo', '2,7-Anhydro-Kdof', '2dAraHexA', '3,6-Anhydro-Fruf', '3,6-Anhydro-Gal', '3,6-Anhydro-Gal2S', '3,6-Anhydro-Glc', '3,6-Anhydro-L-Gal', '3,6-Anhydro-L-Gal2Me', '3-Anhydro-Gal', '3-Anhydro-Gal2S', '3dFuc', '3dGal', '3dLyxHep-ulosaric', '4,7-Anhydro-Kdo', '4,7-Anhydro-KdoOPEtN', '4,8-Anhydro-Kdo', '4d8dNeu5Ac', '4dAraHex', '4dEry-ol', '4dFuc', '4dGal', '4dNeu5Ac', '4dThrHexNAcA4en', '4eLeg5Ac7Ac', '5dAraf', '5dAraf3Me', '5dLyxf3CFo', '5dLyxf3CMe']\n"
     ]
    }
   ],
   "source": [
    "# Let's look at 50 more keys\n",
    "\n",
    "print(f\"More Examples of 'other' keys: {other_keys[20:70]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3a766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'monosaccharide' keys: 2407\n",
      "Examples of 'monosaccharide' keys: ['Abe', 'Abe1PP', 'Abe2Ac', 'AbeOAc', 'Acarbose', 'AcefA', 'Aci5Ac7Ac', 'AcoNAc', 'All', 'All-ol', 'All1S2S3S4S', 'All2Ac3Ac', 'All2S3S4S', 'All3Ac', 'All6Ac', 'AllN', 'AllN1P', 'AllNAc', 'AllNAc6Me', 'AllOMe', 'Alt', 'AltA', 'AltA2N', 'AltA2S', 'AltAN', 'AltNAc', 'AltNAcA', 'AltNAcA1Prop', 'Altf', 'AltfOAc', 'Amikacin', 'Api', 'ApiOAc', 'ApiOMe-ol', 'Apif', 'Ara', 'Ara-ol', 'Ara1Cer2Ac', 'Ara1Me', 'Ara1N4P', 'Ara1P4N', 'Ara1PP', 'Ara1PP2NAc', 'Ara1PP4N', 'Ara1PP4NFo', 'Ara2Ac', 'Ara2Ac3Ac', 'Ara2Ac3Ac4Ac', 'Ara2Ac4Ac', 'Ara2Ac5P-ol']\n"
     ]
    }
   ],
   "source": [
    "# Let's explore those monosaccharide keys\n",
    "monosaccharide = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        monosaccharide.append(key)\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'monosaccharide' keys: {len(monosaccharide)}\")\n",
    "print(f\"Examples of 'monosaccharide' keys: {monosaccharide[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ddd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'linkage_or_modification' keys: 36\n",
      "Examples of 'linkage_or_modification' keys: ['-10', '-12', '-2', '-4', '-6', '-8', '1-1', '1-2', '1-3', '1-4', '1-5', '1-6', '1-?', '2-3', '2-4', '2-5', '2-6', '3-1', '3-5', '4-1', '4-5', '5-1', '5-2', '5-3', '5-4', '5-5', '5-6', '6-1', '6-3', '6-4', '?1-2', '?1-3', '?1-4', '?1-6', '?1-?', '?2-?']\n"
     ]
    }
   ],
   "source": [
    "# To be throughough, let's look at 50 Linkage or Modification keys as well\n",
    "linkage_or_modification = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        linkage_or_modification.append(key)\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'linkage_or_modification' keys: {len(linkage_or_modification)}\")\n",
    "print(f\"Examples of 'linkage_or_modification' keys: {linkage_or_modification[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "df823d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All embeddings in the loaded dictionary appear to be the same.\n",
      "Number of embeddings in the dictionary: 2565\n",
      "First embedding:\n",
      "[ 9.33886290e-01 -7.57189512e-01 -5.22765040e-01  4.93726492e-01\n",
      "  3.03156078e-01 -1.72754931e+00  2.03015614e+00 -1.13539708e+00\n",
      " -8.32044244e-01 -6.09763384e-01 -5.63947335e-02 -2.68140852e-01\n",
      " -6.37493312e-01  1.45667583e-01 -7.75620103e-01 -1.39048725e-01\n",
      "  1.06042847e-01 -3.74972522e-01  7.91566074e-01 -1.03034627e+00\n",
      " -1.12639211e-01 -3.78986076e-03  5.92547238e-01  2.81559825e-01\n",
      " -5.21002829e-01  9.35327411e-01  2.56601274e-01 -3.91364455e-01\n",
      "  2.72188634e-02  5.00928342e-01 -5.55309415e-01  1.28289807e+00\n",
      " -6.45282388e-01  5.19899249e-01  6.10100806e-01  1.84122849e+00\n",
      "  3.11432898e-01 -7.64928609e-02 -1.05589128e+00  6.50653005e-01\n",
      "  9.70111132e-01  7.40227938e-01  8.39829683e-01 -3.04328918e-01\n",
      " -1.06630003e+00  4.53770608e-01  4.27673876e-01 -6.02427721e-01\n",
      "  4.39536482e-01 -1.16493046e+00 -2.04154789e-01  1.13036299e+00\n",
      "  2.51586974e-01  1.04393315e+00  2.60879964e-01  4.63881493e-02\n",
      "  8.49927664e-01 -1.21275747e+00 -5.25301337e-01 -7.54553556e-01\n",
      " -5.36846638e-01  1.71898973e+00  1.07118464e+00  1.25938666e+00\n",
      "  7.28268623e-01  2.50012755e-01 -8.84264708e-01  3.54878515e-01\n",
      " -9.51814711e-01  1.92197442e-01  6.22674108e-01 -7.19715357e-02\n",
      " -2.53418744e-01  6.10054433e-01 -1.37844992e+00  1.10613918e+00\n",
      " -7.89550483e-01  4.11728621e-01 -1.39660871e+00 -2.74130464e-01\n",
      " -4.85218346e-01 -1.64008796e+00 -2.54515797e-01 -4.76354361e-02\n",
      "  1.70321250e+00  1.37953115e+00  6.62403643e-01  1.23904690e-01\n",
      " -2.03382596e-02  2.49572158e-01 -1.19476050e-01  1.01610112e+00\n",
      "  1.54832602e-01 -3.18885893e-01  1.02479362e+00  2.19304472e-01\n",
      " -1.77515924e-01 -2.96848416e-01 -1.51161349e+00 -1.55658543e+00\n",
      "  6.01615787e-01 -1.18876457e+00  6.75462842e-01 -1.21065450e+00\n",
      "  1.00956786e+00  5.41580915e-01  4.89682317e-01 -4.31063682e-01\n",
      " -6.99561596e-01 -9.50598717e-01 -4.71236914e-01  8.96337509e-01\n",
      "  1.97975963e-01  6.51351273e-01 -1.65811467e+00 -2.37476051e-01\n",
      "  1.22424424e+00  3.85935336e-01  1.74970782e+00  1.08295810e+00\n",
      " -2.08416104e-01 -1.44780791e+00 -3.18115175e-01 -2.69204080e-02\n",
      " -7.30906725e-01  3.65380794e-01 -5.23220778e-01 -1.59638667e+00\n",
      "  9.76120412e-01  4.75375116e-01  1.10794783e+00 -9.16275680e-01\n",
      "  8.67535770e-01 -2.21260801e-01  3.58714461e-02 -1.62487292e+00\n",
      "  9.47338939e-01  2.52621353e-01 -2.44861484e-01  4.85217899e-01\n",
      " -1.72671735e-01  1.49431840e-01 -9.26872373e-01 -6.38668120e-01\n",
      " -1.37115136e-01  1.30791855e+00  1.25448748e-01  3.05962026e-01\n",
      " -2.51638025e-01  6.88706279e-01 -6.43941760e-01  6.10008895e-01\n",
      "  2.45932966e-01  1.53176570e+00 -2.05617994e-01  5.01646757e-01\n",
      " -4.11370814e-01 -5.36742508e-01 -1.23477876e-02  6.50121808e-01\n",
      " -3.78578186e-01  6.62264466e-01  1.53327346e-01 -9.97333288e-01\n",
      "  2.86916673e-01 -3.98133188e-01  1.19174033e-01 -1.07086766e+00\n",
      "  5.68605885e-02  8.55352730e-02 -2.43456244e-01 -5.13940752e-01\n",
      "  9.52608764e-01 -3.56329709e-01 -9.76832956e-02  1.55454218e-01\n",
      "  1.07665420e-01  7.78901517e-01  1.94103813e+00  5.98729789e-01\n",
      "  1.49250478e-01  6.60319090e-01 -9.16693985e-01 -1.80390513e+00\n",
      " -1.08837974e+00 -5.85823774e-01 -5.17625034e-01  1.13187218e+00\n",
      " -3.11186165e-01 -1.56313211e-01  4.89638031e-01  6.32191420e-01\n",
      " -9.01452422e-01  3.40963513e-01  3.77618819e-01  4.78747129e-01\n",
      " -1.26142776e+00  1.63014054e+00 -7.38181099e-02 -8.88819635e-01\n",
      " -9.81908560e-01 -3.11309278e-01 -2.87041283e+00 -6.68797910e-01\n",
      "  1.15292573e+00  1.82262063e+00  6.86679184e-01  3.54639411e-01\n",
      "  1.14279723e+00  1.23592412e+00 -4.26488072e-01  5.78116417e-01\n",
      "  2.67315298e-01  1.73516899e-01 -6.95198655e-01 -7.84443021e-01\n",
      "  1.87699527e-01  7.76465774e-01  1.17747712e+00  2.98208922e-01\n",
      "  1.80739570e+00 -6.55146241e-02  2.10267353e+00 -1.49224257e+00\n",
      "  1.67633876e-01 -5.96812427e-01  4.02143002e-01 -5.80711842e-01\n",
      " -6.86030865e-01  2.82077312e-01  4.62324202e-01 -8.51680398e-01\n",
      " -6.37305975e-01 -1.97909772e-01  8.27008903e-01 -2.47440666e-01\n",
      "  5.40550411e-01  2.20697820e-02 -3.67172241e-01  1.37753654e+00\n",
      "  2.57560164e-01  1.12044883e+00  1.47008979e+00 -3.09366286e-01\n",
      "  1.41206241e+00 -1.07911384e+00 -3.82883579e-01  1.15288660e-01\n",
      "  6.46931171e-01 -1.63524508e+00 -4.82143342e-01 -2.22676694e-02\n",
      " -2.94011176e-01  1.76649165e+00 -1.42879653e+00 -1.01673603e+00\n",
      "  6.92535341e-01  1.08943865e-01 -1.51619220e+00 -1.31418991e+00\n",
      " -5.36556542e-01 -9.08092409e-02 -3.43192220e-02 -5.01663029e-01\n",
      " -4.27816272e-01  5.04320741e-01 -8.19638968e-01  1.27975166e-01\n",
      "  6.98855758e-01  4.11748588e-01 -2.63869703e-01 -1.72789741e+00\n",
      "  2.40177006e-01 -3.30802739e-01  1.47785515e-01  4.70187128e-01\n",
      "  3.38367313e-01 -1.54152012e+00  3.17173868e-01 -1.70832485e-01\n",
      "  9.85031009e-01 -1.51257575e+00  7.86181986e-01  2.95546353e-01\n",
      "  4.57608998e-02 -6.43859148e-01  4.83155847e-01 -1.51108074e+00\n",
      " -1.82736918e-01 -3.47120881e-01 -5.70403397e-01 -1.21720120e-01\n",
      " -1.61197579e+00  1.02913380e-03 -4.93016541e-02 -1.70051694e+00\n",
      " -4.81017500e-01 -9.90746021e-01  3.51191968e-01 -6.38143182e-01\n",
      "  8.80924284e-01  1.06428635e+00 -1.31740403e+00 -1.46576715e+00\n",
      " -8.72395873e-01  1.48068953e+00 -2.76599586e-01 -1.15330029e+00\n",
      "  1.45732999e-01 -1.63671541e+00  2.22910285e-01 -3.31862628e-01\n",
      "  5.65533102e-01 -4.64938819e-01  1.83547580e+00 -7.03186333e-01\n",
      "  2.57217407e-01  1.83000445e+00  1.64521456e-01  1.26764941e+00]\n"
     ]
    }
   ],
   "source": [
    "# wait, a couple of weeks later and I couldn't figure out why my glm-infused model wasn't converging, \n",
    "# and I just realised that the embeddings were all the same, no wonder it didn't work. \n",
    "# I should have looked at the embeddings themselves, not just the keys.\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    loaded_embeddings = pickle.load(f)\n",
    "\n",
    "first_embedding = None\n",
    "all_same = True\n",
    "if loaded_embeddings:\n",
    "    first_key = next(iter(loaded_embeddings))\n",
    "    first_embedding = loaded_embeddings[first_key]\n",
    "    for key, embedding in loaded_embeddings.items():\n",
    "        if not (embedding == first_embedding).all():\n",
    "            all_same = False\n",
    "            print(f\"Found a different embedding for key: {key}\")\n",
    "            break\n",
    "\n",
    "if all_same and first_embedding is not None:\n",
    "    print(\"All embeddings in the loaded dictionary appear to be the same.\")\n",
    "elif first_embedding is None:\n",
    "    print(\"The embedding dictionary is empty.\")\n",
    "else:\n",
    "    print(\"Embeddings in the dictionary are not all the same.\")\n",
    "\n",
    "print(f\"Number of embeddings in the dictionary: {len(loaded_embeddings)}\")\n",
    "if first_embedding is not None:\n",
    "    print(f\"First embedding:\")\n",
    "    print(first_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9ad5a",
   "metadata": {},
   "source": [
    "### Load the glycowork libr\n",
    "\n",
    "I'll load the glycowork library and compare it to the keys in the embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b14506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in glycowork vocabulary: 2565\n",
      "Example keys from glycowork vocabulary (first 20): ['!GlcNAc', '-10', '-12', '-2', '-4', '-6', '-8', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic']\n"
     ]
    }
   ],
   "source": [
    "from glycowork.glycan_data import loader\n",
    "\n",
    "glycowork_vocabulary = loader.lib\n",
    "\n",
    "print(f\"Number of items in glycowork vocabulary: {len(glycowork_vocabulary)}\")\n",
    "print(f\"Example keys from glycowork vocabulary (first 20): {list(glycowork_vocabulary.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e1f3d",
   "metadata": {},
   "source": [
    "Nice, they seem to correspond one to one!\n",
    "\n",
    "That saves me a lot of work down the line (Thanks Roman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of value for '-10': <class 'int'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# let's look at one of the keys in the glycowork vocabulary to see what they return\n",
    "example_glycowork_key = '-10'\n",
    "if example_glycowork_key in glycowork_vocabulary:\n",
    "    glycowork_value = glycowork_vocabulary[example_glycowork_key]\n",
    "    print(f\"Type of value for '{example_glycowork_key}': {type(glycowork_value)}\")\n",
    "    print(glycowork_value)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f827a0",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54413ae2",
   "metadata": {},
   "source": [
    "### Load, filter, and transform glycowork data into glycan_loaders ||||run on kernel restart||||\n",
    "This is used to load and filter glycowork data for a specific prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b4d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the full sugarbase dataset and make a custom dataframe for the problem you want to tackle\n",
    "\n",
    "from glycowork.glycan_data.loader import build_custom_df, df_glycan\n",
    "from glycowork.ml.train_test_split import prepare_multilabel\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "def build_multilabel_dataset(glycan_dataset: str = 'df_species',\n",
    "                          glycan_class: str = 'Kingdom',\n",
    "                          min_class_size: int = 6) -> Tuple[List[str], List[List[float]], List[str]]:\n",
    "    \"\"\"\n",
    "    Loads glycan data, prepares it for multi-label classification,\n",
    "    and removes classes with fewer than min_class_size samples.\n",
    "\n",
    "    Args:\n",
    "        glycan_dataset: The glycowork dataset to use. Options include: \n",
    "            'df_species', 'df_tissue', and 'df_disease'.            \n",
    "        glycan_class: The class to predict. Options include:\n",
    "            df_species: 'Species', 'Genus', 'Family', 'Order', 'Class', 'Phylum', 'Kingdom', 'Domain', 'ref'\n",
    "            df_tissue:  'tissue_sample', 'tissue_species', 'tissue_id', 'tissue_ref'\n",
    "            df_disease: 'disease_association', 'disease_sample', 'disease_direction', 'disease_species', 'disease_id', 'disease_ref'\n",
    "        min_class_size: Minimum number of samples required for a class to be included. Set to 1 to include all classes.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - glycan_sequences: List of glycan strings after filtering rare classes.\n",
    "        - binary_labels: List of corresponding multi-label binary vectors.\n",
    "        - label_names: The ordered list of names for each position in the binary vectors.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    all_glycan_data = df_glycan\n",
    "\n",
    "    # Build custom dataframe\n",
    "    custom_glycan_df = build_custom_df(all_glycan_data, glycan_dataset)\n",
    "\n",
    "    # Extract the list of unique individual labels from the chosen class from the custom_glycan_df\n",
    "    # These are used to dechipher the labels when the model is used for prediction\n",
    "    all_possible_label_names = sorted(list(custom_glycan_df[glycan_class].unique()))\n",
    "    print(f\"Found {len(all_possible_label_names)} unique individual classes/labels.\")\n",
    "\n",
    "\n",
    "    # Prepare for multi-label prediction\n",
    "    glycans, labels = prepare_multilabel(custom_glycan_df, glycan_class)\n",
    "\n",
    "    # if needed, removes classes with fewer than min_class_size samples.\n",
    "    if(min_class_size > 1):\n",
    "\n",
    "        # Convert labels to string representation for counting\n",
    "        label_strings = [''.join(map(str, label)) for label in labels]\n",
    "\n",
    "        # Count occurrences of each label combination\n",
    "        label_counts = Counter(label_strings)\n",
    "\n",
    "        # Filter glycans and labels based on class size\n",
    "        glycan_sequences = [glycans[i] for i, label_str in enumerate(label_strings) if label_counts[label_str] >= min_class_size]\n",
    "        binary_labels_unfiltered = [labels[i] for i, label_str in enumerate(label_strings) if label_counts[label_str] >= min_class_size]\n",
    "        print(f\"Number of unique glycans left after filtering rare classes (size >= {min_class_size}): {len(glycan_sequences)}/{len(glycans)}\")\n",
    "        \n",
    "        # Filter out individual labels with no positive examples after glycan filtering\n",
    "\n",
    "        # Convert binary_labels to numpy array for easier column manipulation\n",
    "        binary_labels_np = np.array(binary_labels_unfiltered)\n",
    "\n",
    "        # Find indices of labels with at least one positive example\n",
    "        # Sum across rows (axis=0) to get count for each label\n",
    "        label_sums = binary_labels_np.sum(axis=0)\n",
    "        active_label_indices = np.where(label_sums > 0)[0]\n",
    "\n",
    "        # Create the final list of label names using the active indices\n",
    "        # Use the initially generated sorted list (all_possible_label_names)\n",
    "        # because its order matches the columns of binary_labels after prepare_multilabel\n",
    "        label_names = [all_possible_label_names[i] for i in active_label_indices]\n",
    "\n",
    "        # Create the final filtered binary label vectors, keeping only the active columns\n",
    "        binary_labels = binary_labels_np[:, active_label_indices].tolist() # Convert back to list of lists\n",
    "\n",
    "        print(f\"Number of unique labels left after filtering: {len(binary_labels[0])}\")\n",
    "\n",
    "    else:\n",
    "        glycan_sequences = glycans\n",
    "        binary_labels = labels\n",
    "        print(f\"Number of unique glycans: {len(glycan_sequences)}\")\n",
    "\n",
    "    return glycan_sequences, binary_labels, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd78302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 unique individual classes/labels.\n",
      "Number of unique glycans left after filtering rare classes (size >= 6): 1458/1648\n",
      "Number of unique labels left after filtering: 18\n"
     ]
    }
   ],
   "source": [
    "glycans, labels, label_names = build_multilabel_dataset(glycan_dataset='df_disease', glycan_class='disease_association', min_class_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b77c4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Crohn_disease', 'Parkinson_disease', 'Toxoplasma_gondii_infection', 'autoimmune_pancreatitis', 'cholangiocarcinoma', 'colon_adenocarcinoma', 'colorectal_cancer', 'diabetic_kidney_disease', 'esophageal_cancer', 'female_breast_cancer', 'liver_cancer', 'lung_non_small_cell_carcinoma', 'melanoma', 'pancreatic_cancer', 'prostate_cancer', 'restless_legs_syndrome', 'stomach_cancer']\n"
     ]
    }
   ],
   "source": [
    "print (label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c440705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique glycans: 1458\n",
      "Number of label vectors: 1458\n",
      "Shape of first label vector (number of members in class): 18\n",
      "\n",
      "First 5 glycans:\n",
      "['Neu5Ac(a2-?)Gal(b1-4)GlcNAc(b1-3)Gal(b1-4)GlcNAc(b1-?)[Neu5Ac(a2-?)Gal(b1-4)GlcNAc(b1-?)]Man(a1-3)[Neu5Ac(a2-?)Gal(b1-4)GlcNAc(b1-3)Gal(b1-4)GlcNAc(b1-2)[Neu5Ac(a2-?)Gal(b1-4)GlcNAc(b1-3)Gal(b1-4)GlcNAc(b1-6)]Man(a1-6)]Man(b1-4)GlcNAc(b1-4)[Fuc(a1-6)]GlcNAc', 'Neu5Ac(a2-3)Gal(b1-?)[Fuc(a1-?)]GlcNAc(b1-3)Gal(b1-?)GlcNAc(b1-3)Gal(b1-3)[Gal(b1-4)GlcNAc(b1-6)]GalNAc', 'Neu5Ac(a2-?)GalNAc(b1-4)GlcNAc(b1-2)Man(a1-3)[Neu5Ac(a2-?)GalNAc(b1-4)GlcNAc(b1-2)Man(a1-6)]Man(b1-4)GlcNAc(b1-4)[Fuc(a1-6)]GlcNAc', 'Fuc(a1-3)[Gal(b1-4)]GlcNAc(b1-2)[Fuc(a1-3)[Gal(b1-4)]GlcNAc(b1-4)]Man(a1-3)[Fuc(a1-3)[Gal(b1-4)]GlcNAc(b1-2)[GlcNAc(b1-6)]Man(a1-6)][GlcNAc(b1-4)]Man(b1-4)GlcNAc(b1-4)[Fuc(a1-6)]GlcNAc', 'Fuc(a1-2)[GalNAc(a1-3)]Gal(b1-4)GlcNAc(b1-3)Gal(b1-3)[Fuc(a1-2)Gal(b1-4)GlcNAc(b1-6)]GalNAc']\n",
      "\n",
      "First 5 label vectors:\n",
      "[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "# quick inspection of the data\n",
    "\n",
    "print(f\"Number of unique glycans: {len(glycans)}\")\n",
    "print(f\"Number of label vectors: {len(labels)}\")\n",
    "print(f\"Shape of first label vector (number of members in class): {len(labels[0])}\")\n",
    "print(f\"\\nFirst 5 glycans:\\n{glycans[:5]}\")\n",
    "print(f\"\\nFirst 5 label vectors:\\n{labels[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ff9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and testing sets using StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import List, Union\n",
    "\n",
    "def multilabel_split(glycans: List[str], # list of IUPAC-condensed glycans\n",
    "                 labels: List[Union[float, int, str]], # list of prediction labels\n",
    "                 train_size: float = 0.7, # size of train set, the rest is split into validation and test sets\n",
    "                 random_state: int = 42 # random state for reproducibility\n",
    "                )-> Tuple[List[str], List[str], List[str], List[List[float]], List[List[float]], List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets using StratifiedShuffleSplit.\n",
    "    \n",
    "    Args:\n",
    "        glycans: List of glycans.\n",
    "        labels: List of label vectors.\n",
    "        train_size: Proportion of the dataset to include in the validation and test split.\n",
    "        random_state: Controls the randomness of the split.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - train_glycans: Training set of glycans.\n",
    "            - val_glycans: Validation set of glycans.\n",
    "            - test_glycans: Testing set of glycans.\n",
    "            - train_labels: Training set of labels.\n",
    "            - val_labels: Validation set of labels.\n",
    "            - test_labels: Testing set of labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert labels to a suitable format for stratification (string representation)\n",
    "    label_strings = [''.join(map(str, label)) for label in labels]\n",
    "\n",
    "    # Initial split for train vs. (val + test)\n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = 1 - train_size, random_state = random_state)\n",
    "    train_index, temp_index = next(sss.split(glycans, label_strings))\n",
    "    train_glycans = [glycans[i] for i in train_index]\n",
    "    train_labels = [labels[i] for i in train_index]\n",
    "    temp_glycans = [glycans[i] for i in temp_index]\n",
    "    temp_labels = [labels[i] for i in temp_index]\n",
    "\n",
    "    # Split the remaining (val + test) into validation and test sets\n",
    "    sss_val_test = StratifiedShuffleSplit(n_splits = 1, test_size = 0.5, random_state = random_state)\n",
    "    val_index, test_index = next(sss_val_test.split(temp_glycans, [''.join(map(str, label)) for label in temp_labels]))\n",
    "    val_glycans = [temp_glycans[i] for i in val_index]\n",
    "    val_labels = [temp_labels[i] for i in val_index]\n",
    "    test_glycans = [temp_glycans[i] for i in test_index]\n",
    "    test_labels = [temp_labels[i] for i in test_index]\n",
    "\n",
    "    print(\"Split complete!\")\n",
    "    print(f\"Train set size: {len(train_glycans)}\")\n",
    "    print(f\"Validation set size: {len(val_glycans)}\")\n",
    "    print(f\"Test set size: {len(test_glycans)}\")\n",
    "        \n",
    "    return train_glycans, val_glycans, test_glycans, train_labels, val_labels, test_labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4723022a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete!\n",
      "Train set size: 1020\n",
      "Validation set size: 219\n",
      "Test set size: 219\n"
     ]
    }
   ],
   "source": [
    "# Lets use the split function\n",
    "train_glycans, val_glycans, test_glycans, train_labels, val_labels, test_labels = multilabel_split(glycans, labels, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "068f4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms IUPAC into graphs and makes the data loaders for the training and validation sets\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "\n",
    "glycan_loaders = split_data_to_train(\n",
    "    glycan_list_train = train_glycans,\n",
    "    glycan_list_val = val_glycans,\n",
    "    labels_train = train_labels,\n",
    "    labels_val = val_labels,\n",
    "    batch_size = 128,  # 32 or 128 seem to work well on this system\n",
    "    drop_last = False,\n",
    "    augment_prob = 0.0,  # Adjust if you want augmentation for training\n",
    "    generalization_prob = 0.2  # Adjust if you want generalization for training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae2f77",
   "metadata": {},
   "source": [
    "### GIFFLAR Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a5ce2",
   "metadata": {},
   "source": [
    "#### Load GIFFLAR Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the GIFFLAR dataset for the Taxonomy Kingdom (takes a long time to run(~40m), just use the file it generated)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../GIFFLAR') \n",
    "\n",
    "from gifflar.benchmarks import get_dataset\n",
    "import pathlib\n",
    "\n",
    "data_config_kingdom = {\"name\": \"Taxonomy_Kingdom\"}\n",
    "root_dir = pathlib.Path(\"./data_gifflar\")  # Choose a directory to save the data\n",
    "root_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "kingdom_dataset_config = get_dataset(data_config_kingdom, root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c4b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Taxonomy_Kingdom', 'filepath': WindowsPath('data_gifflar/taxonomy_Kingdom.tsv')}\n"
     ]
    }
   ],
   "source": [
    "# Print the dataset configuration\n",
    "print(kingdom_dataset_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a488a",
   "metadata": {},
   "source": [
    "#### Load and transform GIFFLAR Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "339c831f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               IUPAC  Amoebozoa  Animalia  \\\n",
      "0  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "1  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "2  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "3  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "4  3,6-Anhydro-L-Gal(a1-3)GalOMe(b1-4)3,6-Anhydro...          0         0   \n",
      "\n",
      "   Bacteria  Bamfordvirae  Chromista  Euryarchaeota  Excavata  Fungi  \\\n",
      "0         1             0          0              0         0      0   \n",
      "1         1             0          0              0         0      0   \n",
      "2         1             0          0              0         0      0   \n",
      "3         1             0          0              0         0      0   \n",
      "4         1             0          0              0         0      0   \n",
      "\n",
      "   Heunggongvirae  Metazoa  Orthornavirae  Pararnavirae  Plantae  Protista  \\\n",
      "0               0        0              0             0        0         0   \n",
      "1               0        0              0             0        0         0   \n",
      "2               0        0              0             0        0         0   \n",
      "3               0        0              0             0        0         0   \n",
      "4               0        0              0             0        0         0   \n",
      "\n",
      "   Riboviria  split  \n",
      "0          0  train  \n",
      "1          0  train  \n",
      "2          0  train  \n",
      "3          0    val  \n",
      "4          0   test  \n",
      "Shape of the DataFrame: (16452, 17)\n"
     ]
    }
   ],
   "source": [
    "# Lets load the dataset into a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "filepath = 'data_gifflar/taxonomy_Kingdom.tsv'\n",
    "#filepath = kingdom_dataset_config['filepath'] #If you've loaded it recently, which you shouldn't\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "multilabel_kingdom_df = pd.read_csv(filepath, sep=\"\\t\")\n",
    "# Display the first few rows of the DataFrame\n",
    "print(multilabel_kingdom_df.head())\n",
    "# Display the shape of the DataFrame\n",
    "print(f\"Shape of the DataFrame: {multilabel_kingdom_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b5a99",
   "metadata": {},
   "source": [
    "That looks fine\n",
    "\n",
    "Let's convert them into graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f975f366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['IUPAC', 'Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae',\n",
      "       'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae',\n",
      "       'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista',\n",
      "       'Riboviria', 'split'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(multilabel_kingdom_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f37ce3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's try the prepare multilabel function from the train_test_split module\n",
    "\n",
    "from glycowork.ml.train_test_split import prepare_multilabel\n",
    "\n",
    "# Prepare the multilabel dataset using the melt function\n",
    "# The melt function is used to transform the DataFrame from wide format to long format\n",
    "kingdom_df_melted = multilabel_kingdom_df.melt(\n",
    "    id_vars=['IUPAC', 'split'],\n",
    "    value_vars=['Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae', 'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae', 'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista', 'Riboviria'],\n",
    "    var_name='Kingdom',\n",
    "    value_name='Association'\n",
    ")\n",
    "\n",
    "# Filter for associations where the glycan belongs to the kingdom (Association == 1)\n",
    "kingdom_df_melted = kingdom_df_melted[kingdom_df_melted['Association'] == 1]\n",
    "\n",
    "# Splitting the dataset using the 'split' column\n",
    "train_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'train'].drop(columns=['split'])\n",
    "val_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'val'].drop(columns=['split'])\n",
    "test_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'test'].drop(columns=['split'])\n",
    "\n",
    "# Finally using the prepare_multilabel function to prepare the data for training\n",
    "glycan_train, label_train = prepare_multilabel(train_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "glycan_val, label_val = prepare_multilabel(val_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "glycan_test, label_test = prepare_multilabel(test_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b68ad291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make them into graphs again, but hyper efficiently this time\n",
    "\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "\n",
    "multilabel_kingdom_loaders = split_data_to_train(\n",
    "    glycan_list_train=glycan_train,\n",
    "    glycan_list_val=glycan_val,\n",
    "    labels_train=label_train,\n",
    "    labels_val=label_val,\n",
    "    batch_size=32,  # Adjust as needed\n",
    "    drop_last=False,\n",
    "    augment_prob=0.0,  # Adjust if you want augmentation for training\n",
    "    generalization_prob=0.0  # Adjust if you want generalization for training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26e9af",
   "metadata": {},
   "source": [
    "#### Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2104e79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training glycans: 11593\n",
      "Number of validation glycans: 3213\n",
      "Number of test glycans: 1646\n",
      "Shape of training labels: 11593 x 15\n",
      "Shape of validation labels: 3213 x 15\n",
      "Shape of test labels: 1646 x 15\n",
      "--- Checking example from train set (Direct) ---\n",
      "Glycan: 3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-Gal(a1-3)Gal\n",
      "Split in original data: train\n",
      "Labels in split data: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Labels in original data: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Labels match!\n",
      "--- Checking example from val set (Direct) ---\n",
      "Glycan: 3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-Gal(a1-3)GalOMe(b1-4)3,6-Anhydro-L-Gal(a1-3)Gal\n",
      "Split in original data: val\n",
      "Labels in split data: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Labels in original data: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Labels match!\n",
      "--- Checking example from test set (Direct) ---\n",
      "Glycan: 3,6-Anhydro-L-Gal(a1-3)GalOMe(b1-4)3,6-Anhydro-L-Gal(a1-3)Gal\n",
      "Split in original data: test\n",
      "Labels in split data: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Labels in original data: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Labels match!\n"
     ]
    }
   ],
   "source": [
    "# Let's validate the split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "filepath = 'data_gifflar/taxonomy_Kingdom.tsv'\n",
    "\n",
    "# 1. Load the original DataFrame\n",
    "multilabel_kingdom_df_original = pd.read_csv(filepath, sep=\"\\t\")\n",
    "kingdom_cols = ['Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae', 'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae', 'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista', 'Riboviria']\n",
    "\n",
    "# 2. Split the original DataFrame by 'split'\n",
    "train_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'train']\n",
    "val_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'val']\n",
    "test_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'test']\n",
    "\n",
    "# 3. Extract glycans and labels directly\n",
    "glycan_train_list = train_df['IUPAC'].tolist()\n",
    "label_train_list = [train_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_train_list = list(zip(*label_train_list)) # Transpose\n",
    "\n",
    "glycan_val_list = val_df['IUPAC'].tolist()\n",
    "label_val_list = [val_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_val_list = list(zip(*label_val_list)) # Transpose\n",
    "\n",
    "glycan_test_list = test_df['IUPAC'].tolist()\n",
    "label_test_list = [test_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_test_list = list(zip(*label_test_list)) # Transpose\n",
    "\n",
    "print(f\"Number of training glycans: {len(glycan_train_list)}\")\n",
    "print(f\"Number of validation glycans: {len(glycan_val_list)}\")\n",
    "print(f\"Number of test glycans: {len(glycan_test_list)}\")\n",
    "print(f\"Shape of training labels: {len(label_train_list)} x {len(label_train_list[0]) if label_train_list else 0}\")\n",
    "print(f\"Shape of validation labels: {len(label_val_list)} x {len(label_val_list[0]) if label_val_list else 0}\")\n",
    "print(f\"Shape of test labels: {len(label_test_list)} x {len(label_test_list[0]) if label_test_list else 0}\")\n",
    "\n",
    "# Now re-run the checking function (modified for this new label extraction)\n",
    "def check_example_direct(glycan_list, label_list, split_name, original_df, kingdom_cols):\n",
    "    if glycan_list:\n",
    "        example_index = 0\n",
    "        example_glycan = glycan_list[example_index]\n",
    "        example_labels_split = list(label_list[example_index])\n",
    "\n",
    "        original_row = original_df[original_df['IUPAC'] == example_glycan].iloc[0]\n",
    "        labels_original = np.array([original_row[col] for col in kingdom_cols], dtype=np.float32).tolist()\n",
    "\n",
    "        print(f\"--- Checking example from {split_name} set (Direct) ---\")\n",
    "        print(f\"Glycan: {example_glycan}\")\n",
    "        print(f\"Split in original data: {original_row['split']}\")\n",
    "        print(f\"Labels in split data: {example_labels_split}\")\n",
    "        print(f\"Labels in original data: {labels_original}\")\n",
    "        if labels_original == example_labels_split:\n",
    "            print(\"Labels match!\")\n",
    "        else:\n",
    "            print(\"Labels DO NOT match!\")\n",
    "    else:\n",
    "        print(f\"{split_name} set is empty.\")\n",
    "\n",
    "check_example_direct(glycan_train_list, label_train_list, 'train', multilabel_kingdom_df_original, kingdom_cols)\n",
    "check_example_direct(glycan_val_list, label_val_list, 'val', multilabel_kingdom_df_original, kingdom_cols)\n",
    "check_example_direct(glycan_test_list, label_test_list, 'test', multilabel_kingdom_df_original, kingdom_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0f833",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multilabel_kingdom_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmultilabel_kingdom_loader\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'multilabel_kingdom_loader' is not defined"
     ]
    }
   ],
   "source": [
    "print(multilabel_kingdom_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e0b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: DataBatch(edge_index=[2, 430], labels=[462], string_labels=[32], y=[480], num_nodes=462, x=[462, 320], batch=[462], ptr=[33])\n",
      "Number of graphs in batch: 32\n",
      "\n",
      "First graph data: Data(edge_index=[2, 8], labels=[9], string_labels=[9], y=[15], x=[9, 320], num_nodes=9)\n",
      "Node features (x): tensor([[ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        ...,\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676]])\n",
      "Edge indices (edge_index): tensor([[1, 2, 3, 4, 5, 6, 7, 8],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7]])\n",
      "Labels (y): tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "String labels: ['Rha', 'a1-3', 'Rha', 'a1-4', 'GalNAcA3Ac', 'a1-3', 'QuiNAc', 'b1-2', 'Rha']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check one of the graphs in the dataloader\n",
    "\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataLoader is called 'multilabel_kingdom_loader'\n",
    "inspected_loader = multilabel_kingdom_loaders_emb['train'] # Access the train DataLoader\n",
    "\n",
    "# Get the first batch of data\n",
    "try:\n",
    "    batch = next(iter(inspected_loader))\n",
    "    print(\"Batch:\", batch)\n",
    "    print(\"Number of graphs in batch:\", batch.num_graphs)\n",
    "\n",
    "    # Extract the first graph from the batch\n",
    "    first_graph_data = batch[2] #change to check other graphs\n",
    "    print(\"\\nFirst graph data:\", first_graph_data)\n",
    "    print(\"Node features (x):\", first_graph_data.x)\n",
    "    print(\"Edge indices (edge_index):\", first_graph_data.edge_index)\n",
    "    print(\"Labels (y):\", first_graph_data.y)\n",
    "    print(\"String labels:\", first_graph_data.string_labels)\n",
    "\n",
    "    # Convert the PyG Data object to a NetworkX graph for visualization\n",
    "    nx_graph = to_networkx(first_graph_data) \n",
    "\n",
    "    # Visualize the NetworkX graph\n",
    "    nx_graph = to_networkx(first_graph_data)\n",
    "    pos = nx.spring_layout(nx_graph)\n",
    "    nx.draw(nx_graph, pos, with_labels=True, node_size=500, node_color=\"red\", font_size=10, font_weight=\"bold\", arrows=False)\n",
    "    plt.title(\"First Graph from DataLoader (PyG)\")\n",
    "    plt.show()\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"The DataLoader is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87dc8a6",
   "metadata": {},
   "source": [
    "## Let's make a function to add embeddings to a dataloader object ||||run on kernel restart||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da9d11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: glm_embeddings_1.pkl\n",
      "Embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Let's load the embeddings again here, so I don't have to jump up and down whenever I reload the kernel)\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            glm_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72ad776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add GLM embeddings to a dictionary of dataloaders before loading them into the model\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def add_glm_embeddings_to_dataloaders(dataloaders, glm_embeddings):\n",
    "    embedded_loaders = {}\n",
    "    embedding_dim = 320\n",
    "    for split, loader in dataloaders.items():\n",
    "        embedded_data_list = []\n",
    "        for batch in loader:\n",
    "            for graph in batch.to_data_list():\n",
    "                node_embeddings = []\n",
    "                if hasattr(graph, 'string_labels'):\n",
    "                    for label in graph.string_labels:\n",
    "                        if label in glm_embeddings:\n",
    "                            embedding = glm_embeddings[label]\n",
    "                            node_embeddings.append(torch.tensor(embedding))\n",
    "                        else:\n",
    "                            node_embeddings.append(torch.zeros(embedding_dim))\n",
    "                    graph.x = torch.stack(node_embeddings).float()\n",
    "                    #print(f\"Shape of graph.x after adding embeddings: {graph.x.shape}\")\n",
    "                embedded_data_list.append(graph)\n",
    "\n",
    "        embedded_loaders[split] = DataLoader(embedded_data_list, batch_size=32)\n",
    "        print(f\"GLM embeddings added to {split} DataLoader.\")\n",
    "    return embedded_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b83ae5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multilabel_kingdom_loaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# testing the function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m multilabel_kingdom_loaders_emb = add_glm_embeddings_to_dataloaders(\u001b[43mmultilabel_kingdom_loaders\u001b[49m, glm_embeddings)\n",
      "\u001b[31mNameError\u001b[39m: name 'multilabel_kingdom_loaders' is not defined"
     ]
    }
   ],
   "source": [
    "# testing the function\n",
    "\n",
    "multilabel_kingdom_loaders_emb = add_glm_embeddings_to_dataloaders(multilabel_kingdom_loaders, glm_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef713ec2",
   "metadata": {},
   "source": [
    "## Lets look at the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "83e13ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample string_labels from the first graph:\n",
      "['6dTal', 'a1-2', 'Rhaf', 'b1-5', 'Sug']\n"
     ]
    }
   ],
   "source": [
    "# Looking at string labels\n",
    "\n",
    "train_loader = multilabel_kingdom_loaders_emb['train'] \n",
    "batch = next(iter(train_loader))\n",
    "first_graph_labels = batch[2].string_labels\n",
    "print(\"Sample string_labels from the first graph:\")\n",
    "print(first_graph_labels[:20])  # Print the first 20 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e6368a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embeddings from the first graph:\n",
      "tensor([[ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676]])\n"
     ]
    }
   ],
   "source": [
    "# looking at the embeddings themselves\n",
    "\n",
    "train_loader = multilabel_kingdom_loaders_emb['train'] \n",
    "batch = next(iter(train_loader))\n",
    "first_graph_embeddings = batch[2].x\n",
    "print(\"Sample embeddings from the first graph:\")\n",
    "print(first_graph_embeddings[:20])  # Print the first 20 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef4130ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample keys from glm_embeddings:\n",
      "['!GlcNAc', '-10', '-12', '-2', '-4', '-6', '-8', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic', '1,5-Anhydro-Man-ol', '1,5-Anhydro-ManNAc-ol', '1,5-Anhydro-Xyl-ol', '1,5-Anhydro-Xyl2F-ol', '1-1', '1-2', '1-3', '1-4', '1-5', '1-6', '1-?', '1b-4', '1dAlt-ol', '1dEry-ol', '2,3-Anhydro-All', '2,3-Anhydro-Man', '2,3-Anhydro-Rib', '2,5-Anhydro-D-Alt-ol', '2,5-Anhydro-D-Alt3S-ol', '2,5-Anhydro-D-Tal', '2,5-Anhydro-Glc', '2,5-Anhydro-L-Man-ol', '2,5-Anhydro-Man', '2,5-Anhydro-Man-ol', '2,5-Anhydro-Man1S-ol', '2,5-Anhydro-Man3S-ol', '2,5-Anhydro-Man6S', '2,5-Anhydro-Tal-ol', '2,5-Anhydro-Tal6P', '2,6-Anhydro-Glc5NAc-ol', '2,6-Anhydro-L-Gul-ol', '2,6-Anhydro-L-Gul-onic', '2,6-Anhydro-Man-ol', '2,6-Anhydro-Tal5NAc-ol', '2,7-Anhydro-Kdo', '2,7-Anhydro-Kdof', '2-3', '2-4', '2-5', '2-6', '2dAraHexA', '3,6-Anhydro-Fruf', '3,6-Anhydro-Gal', '3,6-Anhydro-Gal2S', '3,6-Anhydro-Glc', '3,6-Anhydro-L-Gal', '3,6-Anhydro-L-Gal2Me', '3-1', '3-5', '3-Anhydro-Gal', '3-Anhydro-Gal2S', '3dFuc', '3dGal', '3dLyxHep-ulosaric', '4,7-Anhydro-Kdo', '4,7-Anhydro-KdoOPEtN', '4,8-Anhydro-Kdo', '4-1', '4-5', '4d8dNeu5Ac', '4dAraHex', '4dEry-ol', '4dFuc', '4dGal', '4dNeu5Ac', '4dThrHexNAcA4en', '4eLeg5Ac7Ac', '5-1', '5-2', '5-3', '5-4', '5-5', '5-6', '5dAraf', '5dAraf3Me', '5dLyxf3CFo', '5dLyxf3CMe', '5dPenf3CFo', '6-1', '6-3', '6-4', '6dAll', '6dAll3Me', '6dAlt', '6dAltNAc', '6dAltNAc1PP4N', '6dAltNAc1PP4NAc', '6dAltNAc3PCho', '6dAltOAc', '6dAltf', '6dAltfOAc', '6dFruf', '6dGal', '6dGalNAc', '6dGul', '6dHex', '6dHexN', '6dHexNAc4NAc', '6dManHep', '6dTal', '6dTal1PP', '6dTal2Ac', '6dTal2Ac3Ac', '6dTal2Ac3Ac4Ac', '6dTal2Ac3Me', '6dTal2Ac3Me4Ac', '6dTal2Ac4Ac', '6dTal2Me', '6dTal2Me4Ac', '6dTal3Me', '6dTal4Ac', '6dTalNAc', '6dTalNAc1PP', '6dTalNAc4Ac', '6dTalNAcOAc', '6dTalOAc', '6dTalOAcOAc', '6dTalOAcOMe', '6dTalOMe', '6dTalOMe-ol', '6dTalf', '7dNeu5Ac', '8dNeu5Ac', '8eAci5Ac7Ac', '8eLeg', '8eLeg5Ac7Ac', '8eLeg5Ac7Ac8Ac', '8eLeg5Ac7AcGro', '8eLeg5But7Ac', '8eLegNAcNBut', '8ePse5Ac7Ac', '9dNeu5Ac', '?1-2', '?1-3', '?1-4', '?1-6', '?1-?', '?2-?', 'Abe', 'Abe1PP', 'Abe2Ac', 'AbeOAc', 'Acarbose', 'AcefA', 'Aci5Ac7Ac', 'AcoNAc', 'All', 'All-ol', 'All1S2S3S4S', 'All2Ac3Ac', 'All2S3S4S', 'All3Ac', 'All6Ac', 'AllN', 'AllN1P', 'AllNAc', 'AllNAc6Me', 'AllOMe', 'Alt', 'AltA', 'AltA2N', 'AltA2S', 'AltAN', 'AltNAc', 'AltNAcA', 'AltNAcA1Prop', 'Altf', 'AltfOAc', 'Amikacin', 'Api', 'ApiOAc', 'ApiOMe-ol', 'Apif', 'Ara', 'Ara-ol', 'Ara1Cer2Ac', 'Ara1Me', 'Ara1N4P', 'Ara1P4N', 'Ara1PP']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample keys from glm_embeddings:\")\n",
    "print(list(glm_embeddings.keys())[:200])  # Print the first 20 keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fafdb1",
   "metadata": {},
   "source": [
    "## Copies of glycowork functions edited to use embeddings directly ||||run on kernel restart||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SweetNet class\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Union, Literal\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.nn import GraphConv\n",
    "    from torch_geometric.nn import global_mean_pool as gap\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "  raise ImportError(\"<torch or torch_geometric missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from glycowork.glycan_data.loader import lib, download_model \n",
    "\n",
    "class SweetNet(torch.nn.Module):\n",
    "    def __init__(self, lib_size: int, # number of unique tokens for graph nodes\n",
    "                 num_classes: int = 1, # number of output classes (>1 for multilabel)\n",
    "                 hidden_dim: int = 320, # dimension of hidden layers (changed from 128 to 320)\n",
    "                 use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "                ) -> None:\n",
    "        \"given glycan graphs as input, predicts properties via a graph neural network\"\n",
    "        #print(\"Using SweetNet from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "        super(SweetNet, self).__init__()\n",
    "        self.use_external_embeddings = use_external_embeddings\n",
    "        # Convolution operations on the graph\n",
    "        self.conv1 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GraphConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Node embedding\n",
    "        if use_external_embeddings:\n",
    "            self.embedding_dim = hidden_dim\n",
    "        else:\n",
    "            self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    " \n",
    "\n",
    "        #self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    "        # Fully connected part\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, 1024)\n",
    "        self.lin2 = torch.nn.Linear(1024, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, num_classes)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(1024)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.act1 = torch.nn.LeakyReLU()\n",
    "        self.act2 = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor,\n",
    "                inference: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \n",
    "        # Getting node features\n",
    "        if self.use_external_embeddings:\n",
    "           # Use external embeddings (already in x)\n",
    "            pass # x is already the embeddings\n",
    "        else:\n",
    "             # Use internal embedding\n",
    "            x = self.item_embedding(x).squeeze(1)\n",
    "            \n",
    "        \n",
    "        # Graph convolution operations (now using GLM embeddings as x)\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = gap(x, batch)\n",
    "\n",
    "        # Fully connected part\n",
    "        x = self.act1(self.bn1(self.lin1(x)))\n",
    "        x_out = self.bn2(self.lin2(x))\n",
    "        x = F.dropout(self.act2(x_out), p = 0.5, training = self.training)\n",
    "\n",
    "        x = self.lin3(x).squeeze(1)\n",
    "\n",
    "        if inference:\n",
    "          return x, x_out\n",
    "        else:\n",
    "          return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333ffcc",
   "metadata": {},
   "source": [
    "### Other functions I might modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model function\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import torch\n",
    "    # Choose the correct computing architecture\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "    raise ImportError(\"<torch missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, mean_squared_error, \\\n",
    "    label_ranking_average_precision_score, ndcg_score, roc_auc_score, mean_absolute_error, r2_score\n",
    "from glycowork.motif.annotate import annotate_dataset\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 7, # epochs to wait after last improvement\n",
    "                 verbose: bool = False # whether to print messages\n",
    "                ) -> None:\n",
    "        \"Early stops the training if validation loss doesn't improve after a given patience\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = 0\n",
    "\n",
    "    def __call__(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), 'drive/My Drive/checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def sigmoid(x: float # input value\n",
    "          ) -> float: # sigmoid transformed value\n",
    "    \"Apply sigmoid transformation to input\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def disable_running_stats(model: torch.nn.Module # model to disable batch norm\n",
    "                       ) -> None:\n",
    "    \"Disable batch normalization running statistics\"\n",
    "\n",
    "    def _disable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "\n",
    "def enable_running_stats(model: torch.nn.Module # model to enable batch norm\n",
    "                      ) -> None:\n",
    "    \"Enable batch normalization running statistics\"\n",
    "\n",
    "    def _enable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def train_model(model: torch.nn.Module, # graph neural network for analyzing glycans\n",
    "               dataloaders: Dict[str, torch.utils.data.DataLoader], # dict with 'train' and 'val' loaders\n",
    "               criterion: torch.nn.Module, # PyTorch loss function\n",
    "               optimizer: torch.optim.Optimizer, # PyTorch optimizer, has to be SAM if mode != \"regression\"\n",
    "               scheduler: torch.optim.lr_scheduler._LRScheduler, # PyTorch learning rate decay\n",
    "               num_epochs: int = 25, # number of epochs for training\n",
    "               patience: int = 50, # epochs without improvement until early stop\n",
    "               mode: str = 'classification', # 'classification', 'multilabel', or 'regression'\n",
    "               mode2: str = 'multi', # 'multi' or 'binary' classification\n",
    "               return_metrics: bool = False, # whether to return metrics\n",
    "               use_external_embeddings: bool = False # whether to use external embeddings\n",
    "              ) -> Union[torch.nn.Module, tuple[torch.nn.Module, dict[str, dict[str, list[float]]]]]: # best model from training and the training and validation metrics\n",
    "    \"trains a deep learning model on predicting glycan properties\"\n",
    "\n",
    "    since = time.time()\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "    best_lead_metric = float(\"inf\")\n",
    "\n",
    "    if mode == 'classification':\n",
    "         # Removed auroc from metrics to avoid calculation issues for now\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": []}\n",
    "    elif mode == 'multilabel':\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": [], \"lrap\": [], \"ndcg\": []}\n",
    "    else:\n",
    "        blank_metrics = {\"loss\": [], \"mse\": [], \"mae\": [], \"r2\": []}\n",
    "\n",
    "    metrics = {\"train\": copy.deepcopy(blank_metrics), \"val\": copy.deepcopy(blank_metrics)}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_metrics = copy.deepcopy(blank_metrics)\n",
    "            running_metrics[\"weights\"] = []\n",
    "\n",
    "            for data in dataloaders[phase]:\n",
    "                # Get all relevant node attributes\n",
    "                print(f\"Phase: {phase}, Data: {data}\")\n",
    "                print(f\"Phase: {phase}, Data.x: {getattr(data, 'x', None)}\") # Check if x exists and its value\n",
    "                if use_external_embeddings:\n",
    "                    x, y, edge_index, batch = data.x, data.y, data.edge_index, data.batch\n",
    "                else:\n",
    "                    x, y, edge_index, batch = data.labels, data.y, data.edge_index, data.batch\n",
    "                prot = getattr(data, 'train_idx', None)\n",
    "                if prot is not None:\n",
    "                    prot = prot.view(max(batch) + 1, -1).to(device)\n",
    "                x = x.to(device)\n",
    "                if mode == 'multilabel':\n",
    "                    y = y.view(max(batch) + 1, -1).to(device)\n",
    "                else:\n",
    "                    y = y.to(device)\n",
    "                y = y.view(-1, 1) if mode == 'regression' else y\n",
    "                edge_index = edge_index.to(device)\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # First forward pass\n",
    "                    if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                        enable_running_stats(model)\n",
    "                    pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                    loss = criterion(pred, y)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                            optimizer.first_step(zero_grad = True)\n",
    "                            # Second forward pass\n",
    "                            disable_running_stats(model)\n",
    "                            second_pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                            criterion(second_pred, y).backward()\n",
    "                            optimizer.second_step(zero_grad = True)\n",
    "                        else:\n",
    "                            optimizer.step()\n",
    "                \n",
    "                # Check for single-class batches\n",
    "                unique_classes = torch.unique(y).cpu().numpy()\n",
    "                if len(unique_classes) == 1:\n",
    "                    print(f\"WARNING: Single-class batch detected in {phase} phase at epoch {epoch}!\")\n",
    "                    print(f\"Batch labels: {unique_classes}\")\n",
    "\n",
    "                # Collecting relevant metrics\n",
    "                running_metrics[\"loss\"].append(loss.item())\n",
    "                running_metrics[\"weights\"].append(batch.max().cpu() + 1)\n",
    "\n",
    "                y_det = y.detach().cpu().numpy()\n",
    "                pred_det = pred.cpu().detach().numpy()\n",
    "                if mode == 'classification':\n",
    "                    if mode2 == 'multi':\n",
    "                        pred_proba = np.exp(pred_det) / np.sum(np.exp(pred_det), axis = 1, keepdims = True)  # numpy softmax\n",
    "                        pred2 = np.argmax(pred_det, axis = 1)\n",
    "                    else:\n",
    "                        pred_proba = sigmoid(pred_det)\n",
    "                        pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det, pred2))\n",
    "                    # commented out auroc because it was throwing errors\n",
    "                    #running_metrics[\"auroc\"].append(roc_auc_score(y_det.astype(int), pred_proba if mode2 == 'binary' else pred_proba[:, 1]))\n",
    "                elif mode == 'multilabel':\n",
    "                    pred_proba = sigmoid(pred_det)\n",
    "                    pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det.flatten(), pred2.flatten()))\n",
    "                    running_metrics[\"lrap\"].append(label_ranking_average_precision_score(y_det.astype(int), pred_proba))\n",
    "                    running_metrics[\"ndcg\"].append(ndcg_score(y_det.astype(int), pred_proba))\n",
    "                else:\n",
    "                    running_metrics[\"mse\"].append(mean_squared_error(y_det, pred_det))\n",
    "                    running_metrics[\"mae\"].append(mean_absolute_error(y_det, pred_det))\n",
    "                    running_metrics[\"r2\"].append(r2_score(y_det, pred_det))\n",
    "\n",
    "            # Averaging metrics at end of epoch\n",
    "            for key in running_metrics:\n",
    "                if key == \"weights\":\n",
    "                    continue\n",
    "                metrics[phase][key].append(np.average(running_metrics[key], weights = running_metrics[\"weights\"]))\n",
    "\n",
    "            if mode == 'classification':\n",
    "                print('{} Loss: {:.4f} Accuracy: {:.4f} MCC: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            elif mode == 'multilabel':\n",
    "                print('{} Loss: {:.4f} LRAP: {:.4f} NDCG: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            else:\n",
    "                print('{} Loss: {:.4f} MSE: {:.4f} MAE: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"mse\"][-1], metrics[phase][\"mae\"][-1]))\n",
    "\n",
    "            # Keep best model state_dict\n",
    "            if phase == \"val\":\n",
    "                if metrics[phase][\"loss\"][-1] <= best_loss:\n",
    "                    best_loss = metrics[phase][\"loss\"][-1]\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                    # Extract the lead metric (ACC, LRAP, or MSE) of the new best model\n",
    "                    if mode == 'classification':\n",
    "                        best_lead_metric = metrics[phase][\"acc\"][-1]\n",
    "                    elif mode == 'multilabel':\n",
    "                        best_lead_metric = metrics[phase][\"lrap\"][-1]\n",
    "                    else:\n",
    "                        best_lead_metric = metrics[phase][\"mse\"][-1]\n",
    "\n",
    "                # Check Early Stopping & adjust learning rate if needed\n",
    "                early_stopping(metrics[phase][\"loss\"][-1], model)\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(metrics[phase][\"loss\"][-1])\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    if mode == 'classification':\n",
    "        print('Best val loss: {:4f}, best Accuracy score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    elif mode == 'multilabel':\n",
    "        print('Best val loss: {:4f}, best LRAP score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    else:\n",
    "        print('Best val loss: {:4f}, best MSE score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    if return_metrics:\n",
    "        return model, metrics\n",
    "\n",
    "    # Plot loss & score over the course of training\n",
    "    _, _ = plt.subplots(nrows=2, ncols=1)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(epoch + 1), metrics[\"val\"][\"loss\"])\n",
    "    plt.title('Model Training')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.legend(['Validation Loss'], loc='best')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    if mode == 'classification':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"acc\"])\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "        plt.legend(['Validation Accuracy'], loc='best')\n",
    "    elif mode == 'multilabel':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"lrap\"])\n",
    "        plt.ylabel('Validation LRAP')\n",
    "        plt.legend(['Validation LRAP'], loc='best')\n",
    "    else:\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"mse\"])\n",
    "        plt.ylabel('Validation MSE')\n",
    "        plt.legend(['Validation MSE'], loc='best')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init_weights function\n",
    "\n",
    "def init_weights(model: torch.nn.Module, # neural network for analyzing glycans\n",
    "                mode: str = 'sparse', # initialization algorithm: 'sparse', 'kaiming', 'xavier'\n",
    "                sparsity: float = 0.1 # proportion of sparsity after initialization\n",
    "               ) -> None:\n",
    "    \"initializes linear layers of PyTorch model with a weight initialization\"\n",
    "    #print(\"Using init_weights from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if isinstance(model, torch.nn.Linear):\n",
    "        if mode == 'sparse':\n",
    "            torch.nn.init.sparse_(model.weight, sparsity = sparsity)\n",
    "        elif mode == 'kaiming':\n",
    "            torch.nn.init.kaiming_uniform_(model.weight)\n",
    "        elif mode == 'xavier':\n",
    "            torch.nn.init.xavier_uniform_(model.weight)\n",
    "        else:\n",
    "            print(\"This initialization option is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e02dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_model function\n",
    "\n",
    "def prep_model(model_type: Literal[\"SweetNet\", \"LectinOracle\", \"LectinOracle_flex\", \"NSequonPred\"], # type of model to create\n",
    "              num_classes: int, # number of unique classes for classification\n",
    "              libr: Optional[Dict[str, int]] = None, # dictionary of form glycoletter:index\n",
    "              trained: bool = False, # whether to use pretrained model\n",
    "              # set hidden_dim to 320 rather than 128 for the pretrained model\n",
    "              # but 128 is the default for the model in the paper\n",
    "              hidden_dim: int = 320, # hidden dimension for the model (SweetNet/LectinOracle only)\n",
    "              use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "             ) -> torch.nn.Module: # initialized PyTorch model\n",
    "    \"wrapper to instantiate model, initialize it, and put it on the GPU\"\n",
    "    #print(\"Using prep_model from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if libr is None:\n",
    "      libr = lib\n",
    "    if model_type == 'SweetNet':\n",
    "      model = SweetNet(len(libr), num_classes = num_classes, hidden_dim = hidden_dim, use_external_embeddings = use_external_embeddings)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'sparse'))\n",
    "      if trained:\n",
    "        if hidden_dim != 128:\n",
    "          raise ValueError(\"Hidden dimension must be 128 for pretrained model\")\n",
    "        model_path = download_model(\"glycowork_sweetnet_species.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle':\n",
    "      model = LectinOracle(len(libr), num_classes = num_classes, input_size_prot = int(10*hidden_dim))\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle_flex':\n",
    "      model = LectinOracle_flex(len(libr), num_classes = num_classes)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle_flex.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'NSequonPred':\n",
    "      model = NSequonPred()\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"NSequonPred_batch32.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    else:\n",
    "      print(\"Invalid Model Type\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2749ab38",
   "metadata": {},
   "source": [
    "## Copies of glycowork functions edited for glm embedded dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5658e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SweetNet class\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Union, Literal\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.nn import GraphConv\n",
    "    from torch_geometric.nn import global_mean_pool as gap\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "  raise ImportError(\"<torch or torch_geometric missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from glycowork.glycan_data.loader import lib, download_model \n",
    "\n",
    "class SweetNet(torch.nn.Module):\n",
    "    def __init__(self, lib_size: int, # number of unique tokens for graph nodes\n",
    "                 num_classes: int = 1, # number of output classes (>1 for multilabel)\n",
    "                 hidden_dim: int = 320, # dimension of hidden layers (changed from 128 to 320)\n",
    "                 use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "                ) -> None:\n",
    "        \"given glycan graphs as input, predicts properties via a graph neural network\"\n",
    "        #print(\"Using SweetNet from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "        super(SweetNet, self).__init__()\n",
    "        self.use_external_embeddings = use_external_embeddings\n",
    "        # Convolution operations on the graph\n",
    "        self.conv1 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GraphConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Node embedding\n",
    "        if use_external_embeddings:\n",
    "            self.embedding_dim = hidden_dim\n",
    "        else:\n",
    "            self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    " \n",
    "\n",
    "        #self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    "        # Fully connected part\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, 1024)\n",
    "        self.lin2 = torch.nn.Linear(1024, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, num_classes)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(1024)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.act1 = torch.nn.LeakyReLU()\n",
    "        self.act2 = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor,\n",
    "                inference: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \n",
    "        # Getting node features\n",
    "        if self.use_external_embeddings:\n",
    "           # Use external embeddings (already in x)\n",
    "            pass # x is already the embeddings\n",
    "        else:\n",
    "             # Use internal embedding\n",
    "            x = self.item_embedding(x).squeeze(1)\n",
    "            \n",
    "        \n",
    "        # Graph convolution operations (now using GLM embeddings as x)\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = gap(x, batch)\n",
    "\n",
    "        # Fully connected part\n",
    "        x = self.act1(self.bn1(self.lin1(x)))\n",
    "        x_out = self.bn2(self.lin2(x))\n",
    "        x = F.dropout(self.act2(x_out), p = 0.5, training = self.training)\n",
    "\n",
    "        x = self.lin3(x).squeeze(1)\n",
    "\n",
    "        if inference:\n",
    "          return x, x_out\n",
    "        else:\n",
    "          return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03900750",
   "metadata": {},
   "source": [
    "### Other functions I might modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eccb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model function\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import torch\n",
    "    # Choose the correct computing architecture\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "    raise ImportError(\"<torch missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, mean_squared_error, \\\n",
    "    label_ranking_average_precision_score, ndcg_score, roc_auc_score, mean_absolute_error, r2_score\n",
    "from glycowork.motif.annotate import annotate_dataset\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 7, # epochs to wait after last improvement\n",
    "                 verbose: bool = False # whether to print messages\n",
    "                ) -> None:\n",
    "        \"Early stops the training if validation loss doesn't improve after a given patience\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = 0\n",
    "\n",
    "    def __call__(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), 'drive/My Drive/checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def sigmoid(x: float # input value\n",
    "          ) -> float: # sigmoid transformed value\n",
    "    \"Apply sigmoid transformation to input\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def disable_running_stats(model: torch.nn.Module # model to disable batch norm\n",
    "                       ) -> None:\n",
    "    \"Disable batch normalization running statistics\"\n",
    "\n",
    "    def _disable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "\n",
    "def enable_running_stats(model: torch.nn.Module # model to enable batch norm\n",
    "                      ) -> None:\n",
    "    \"Enable batch normalization running statistics\"\n",
    "\n",
    "    def _enable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def train_model(model: torch.nn.Module, # graph neural network for analyzing glycans\n",
    "               dataloaders: Dict[str, torch.utils.data.DataLoader], # dict with 'train' and 'val' loaders\n",
    "               criterion: torch.nn.Module, # PyTorch loss function\n",
    "               optimizer: torch.optim.Optimizer, # PyTorch optimizer, has to be SAM if mode != \"regression\"\n",
    "               scheduler: torch.optim.lr_scheduler._LRScheduler, # PyTorch learning rate decay\n",
    "               num_epochs: int = 25, # number of epochs for training\n",
    "               patience: int = 50, # epochs without improvement until early stop\n",
    "               mode: str = 'classification', # 'classification', 'multilabel', or 'regression'\n",
    "               mode2: str = 'multi', # 'multi' or 'binary' classification\n",
    "               return_metrics: bool = False, # whether to return metrics\n",
    "               use_external_embeddings: bool = False # whether to use external embeddings\n",
    "              ) -> Union[torch.nn.Module, tuple[torch.nn.Module, dict[str, dict[str, list[float]]]]]: # best model from training and the training and validation metrics\n",
    "    \"trains a deep learning model on predicting glycan properties\"\n",
    "\n",
    "    since = time.time()\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "    best_lead_metric = float(\"inf\")\n",
    "\n",
    "    if mode == 'classification':\n",
    "         # Removed auroc from metrics to avoid calculation issues for now\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": []}\n",
    "    elif mode == 'multilabel':\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": [], \"lrap\": [], \"ndcg\": []}\n",
    "    else:\n",
    "        blank_metrics = {\"loss\": [], \"mse\": [], \"mae\": [], \"r2\": []}\n",
    "\n",
    "    metrics = {\"train\": copy.deepcopy(blank_metrics), \"val\": copy.deepcopy(blank_metrics)}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_metrics = copy.deepcopy(blank_metrics)\n",
    "            running_metrics[\"weights\"] = []\n",
    "\n",
    "            for data in dataloaders[phase]:\n",
    "                # Get all relevant node attributes\n",
    "                print(f\"Phase: {phase}, Data: {data}\")\n",
    "                print(f\"Phase: {phase}, Data.x: {getattr(data, 'x', None)}\") # Check if x exists and its value\n",
    "                if use_external_embeddings:\n",
    "                    x, y, edge_index, batch = data.x, data.y, data.edge_index, data.batch\n",
    "                else:\n",
    "                    x, y, edge_index, batch = data.labels, data.y, data.edge_index, data.batch\n",
    "                prot = getattr(data, 'train_idx', None)\n",
    "                if prot is not None:\n",
    "                    prot = prot.view(max(batch) + 1, -1).to(device)\n",
    "                x = x.to(device)\n",
    "                if mode == 'multilabel':\n",
    "                    y = y.view(max(batch) + 1, -1).to(device)\n",
    "                else:\n",
    "                    y = y.to(device)\n",
    "                y = y.view(-1, 1) if mode == 'regression' else y\n",
    "                edge_index = edge_index.to(device)\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # First forward pass\n",
    "                    if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                        enable_running_stats(model)\n",
    "                    pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                    loss = criterion(pred, y)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                            optimizer.first_step(zero_grad = True)\n",
    "                            # Second forward pass\n",
    "                            disable_running_stats(model)\n",
    "                            second_pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                            criterion(second_pred, y).backward()\n",
    "                            optimizer.second_step(zero_grad = True)\n",
    "                        else:\n",
    "                            optimizer.step()\n",
    "                \n",
    "                # Check for single-class batches\n",
    "                unique_classes = torch.unique(y).cpu().numpy()\n",
    "                if len(unique_classes) == 1:\n",
    "                    print(f\"WARNING: Single-class batch detected in {phase} phase at epoch {epoch}!\")\n",
    "                    print(f\"Batch labels: {unique_classes}\")\n",
    "\n",
    "                # Collecting relevant metrics\n",
    "                running_metrics[\"loss\"].append(loss.item())\n",
    "                running_metrics[\"weights\"].append(batch.max().cpu() + 1)\n",
    "\n",
    "                y_det = y.detach().cpu().numpy()\n",
    "                pred_det = pred.cpu().detach().numpy()\n",
    "                if mode == 'classification':\n",
    "                    if mode2 == 'multi':\n",
    "                        pred_proba = np.exp(pred_det) / np.sum(np.exp(pred_det), axis = 1, keepdims = True)  # numpy softmax\n",
    "                        pred2 = np.argmax(pred_det, axis = 1)\n",
    "                    else:\n",
    "                        pred_proba = sigmoid(pred_det)\n",
    "                        pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det, pred2))\n",
    "                    # commented out auroc because it was throwing errors\n",
    "                    #running_metrics[\"auroc\"].append(roc_auc_score(y_det.astype(int), pred_proba if mode2 == 'binary' else pred_proba[:, 1]))\n",
    "                elif mode == 'multilabel':\n",
    "                    pred_proba = sigmoid(pred_det)\n",
    "                    pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det.flatten(), pred2.flatten()))\n",
    "                    running_metrics[\"lrap\"].append(label_ranking_average_precision_score(y_det.astype(int), pred_proba))\n",
    "                    running_metrics[\"ndcg\"].append(ndcg_score(y_det.astype(int), pred_proba))\n",
    "                else:\n",
    "                    running_metrics[\"mse\"].append(mean_squared_error(y_det, pred_det))\n",
    "                    running_metrics[\"mae\"].append(mean_absolute_error(y_det, pred_det))\n",
    "                    running_metrics[\"r2\"].append(r2_score(y_det, pred_det))\n",
    "\n",
    "            # Averaging metrics at end of epoch\n",
    "            for key in running_metrics:\n",
    "                if key == \"weights\":\n",
    "                    continue\n",
    "                metrics[phase][key].append(np.average(running_metrics[key], weights = running_metrics[\"weights\"]))\n",
    "\n",
    "            if mode == 'classification':\n",
    "                print('{} Loss: {:.4f} Accuracy: {:.4f} MCC: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            elif mode == 'multilabel':\n",
    "                print('{} Loss: {:.4f} LRAP: {:.4f} NDCG: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            else:\n",
    "                print('{} Loss: {:.4f} MSE: {:.4f} MAE: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"mse\"][-1], metrics[phase][\"mae\"][-1]))\n",
    "\n",
    "            # Keep best model state_dict\n",
    "            if phase == \"val\":\n",
    "                if metrics[phase][\"loss\"][-1] <= best_loss:\n",
    "                    best_loss = metrics[phase][\"loss\"][-1]\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                    # Extract the lead metric (ACC, LRAP, or MSE) of the new best model\n",
    "                    if mode == 'classification':\n",
    "                        best_lead_metric = metrics[phase][\"acc\"][-1]\n",
    "                    elif mode == 'multilabel':\n",
    "                        best_lead_metric = metrics[phase][\"lrap\"][-1]\n",
    "                    else:\n",
    "                        best_lead_metric = metrics[phase][\"mse\"][-1]\n",
    "\n",
    "                # Check Early Stopping & adjust learning rate if needed\n",
    "                early_stopping(metrics[phase][\"loss\"][-1], model)\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(metrics[phase][\"loss\"][-1])\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    if mode == 'classification':\n",
    "        print('Best val loss: {:4f}, best Accuracy score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    elif mode == 'multilabel':\n",
    "        print('Best val loss: {:4f}, best LRAP score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    else:\n",
    "        print('Best val loss: {:4f}, best MSE score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    if return_metrics:\n",
    "        return model, metrics\n",
    "\n",
    "    # Plot loss & score over the course of training\n",
    "    _, _ = plt.subplots(nrows=2, ncols=1)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(epoch + 1), metrics[\"val\"][\"loss\"])\n",
    "    plt.title('Model Training')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.legend(['Validation Loss'], loc='best')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    if mode == 'classification':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"acc\"])\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "        plt.legend(['Validation Accuracy'], loc='best')\n",
    "    elif mode == 'multilabel':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"lrap\"])\n",
    "        plt.ylabel('Validation LRAP')\n",
    "        plt.legend(['Validation LRAP'], loc='best')\n",
    "    else:\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"mse\"])\n",
    "        plt.ylabel('Validation MSE')\n",
    "        plt.legend(['Validation MSE'], loc='best')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57a375b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init_weights function\n",
    "\n",
    "def init_weights(model: torch.nn.Module, # neural network for analyzing glycans\n",
    "                mode: str = 'sparse', # initialization algorithm: 'sparse', 'kaiming', 'xavier'\n",
    "                sparsity: float = 0.1 # proportion of sparsity after initialization\n",
    "               ) -> None:\n",
    "    \"initializes linear layers of PyTorch model with a weight initialization\"\n",
    "    #print(\"Using init_weights from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if isinstance(model, torch.nn.Linear):\n",
    "        if mode == 'sparse':\n",
    "            torch.nn.init.sparse_(model.weight, sparsity = sparsity)\n",
    "        elif mode == 'kaiming':\n",
    "            torch.nn.init.kaiming_uniform_(model.weight)\n",
    "        elif mode == 'xavier':\n",
    "            torch.nn.init.xavier_uniform_(model.weight)\n",
    "        else:\n",
    "            print(\"This initialization option is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a059da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_model function\n",
    "\n",
    "def prep_model(model_type: Literal[\"SweetNet\", \"LectinOracle\", \"LectinOracle_flex\", \"NSequonPred\"], # type of model to create\n",
    "              num_classes: int, # number of unique classes for classification\n",
    "              libr: Optional[Dict[str, int]] = None, # dictionary of form glycoletter:index\n",
    "              trained: bool = False, # whether to use pretrained model\n",
    "              # set hidden_dim to 320 rather than 128 for the pretrained model\n",
    "              # but 128 is the default for the model in the paper\n",
    "              hidden_dim: int = 320, # hidden dimension for the model (SweetNet/LectinOracle only)\n",
    "              use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "             ) -> torch.nn.Module: # initialized PyTorch model\n",
    "    \"wrapper to instantiate model, initialize it, and put it on the GPU\"\n",
    "    #print(\"Using prep_model from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if libr is None:\n",
    "      libr = lib\n",
    "    if model_type == 'SweetNet':\n",
    "      model = SweetNet(len(libr), num_classes = num_classes, hidden_dim = hidden_dim, use_external_embeddings = use_external_embeddings)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'sparse'))\n",
    "      if trained:\n",
    "        if hidden_dim != 128:\n",
    "          raise ValueError(\"Hidden dimension must be 128 for pretrained model\")\n",
    "        model_path = download_model(\"glycowork_sweetnet_species.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle':\n",
    "      model = LectinOracle(len(libr), num_classes = num_classes, input_size_prot = int(10*hidden_dim))\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle_flex':\n",
    "      model = LectinOracle_flex(len(libr), num_classes = num_classes)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle_flex.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'NSequonPred':\n",
    "      model = NSequonPred()\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"NSequonPred_batch32.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    else:\n",
    "      print(\"Invalid Model Type\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbbca33",
   "metadata": {},
   "source": [
    "## Testing using same framework as iteration 0 (basic kingdom sweetnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# testing the modified SweetNet model on the GlycoWork dataset \n",
    "from glycowork.glycan_data.loader import df_species\n",
    "from glycowork.ml.train_test_split import hierarchy_filter\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "from glycowork.ml import model_training\n",
    "\n",
    "# silence the avalanche of \"undefined\" warnings\n",
    "#import warnings\n",
    "#from sklearn.exceptions import UndefinedMetricWarning\n",
    "#warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "train_x, val_x, train_y, val_y, id_val, class_list, class_converter = hierarchy_filter(df_species,\n",
    "                                                                                       rank = 'Kingdom')\n",
    "\n",
    "dataloaders = split_data_to_train(train_x, val_x, train_y, val_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets split out the training code so I don't have to load the data each time\n",
    "\n",
    "model = prep_model('SweetNet', len(class_list))\n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = len(class_list))\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'classification',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4382d5b4",
   "metadata": {},
   "source": [
    "### Extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdade799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the Kingdoms in the original data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "kingdom_counts = df_species['Kingdom'].value_counts()\n",
    "print(\"Kingdom distribution in the original data:\")\n",
    "print(kingdom_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f112562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Print the first few rows to get an overview\n",
    "print(df_species.head())\n",
    "\n",
    "# Print the data types of the columns\n",
    "print(df_species.info())\n",
    "\n",
    "# Check for duplicated glycans\n",
    "print(f\"Number of unique glycans: {df_species['glycan'].nunique()}\")\n",
    "print(f\"Total number of rows: {len(df_species)}\")\n",
    "\n",
    "# If you suspect a column contains multiple kingdoms:\n",
    "if 'Kingdom' in df_species.columns:  # Replace 'Kingdom' with the actual column name\n",
    "    # Check the first few values of that column\n",
    "    print(df_species['Kingdom'].head())\n",
    "    # If it's a string with delimiters, count the delimiters\n",
    "    if isinstance(df_species['Kingdom'][0], str) and ',' in df_species['Kingdom'][0]: #assuming ',' is the delimiter\n",
    "        print(f\"Example value: {df_species['Kingdom'][0]}\")\n",
    "        print(f\"Number of commas in the first value: {df_species['Kingdom'][0].count(',')}\")\n",
    "\n",
    "    # Check for multiple columns representing kingdoms\n",
    "    for col in df_species.columns:\n",
    "        if col in ['Animalia', 'Bacteria', 'Plantae']:  # Add all possible kingdom column names\n",
    "            print(f\"Column '{col}': Data type = {df_species[col].dtype}, Unique values = {df_species[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set()\n",
    "duplicates_count = 0\n",
    "\n",
    "for item in train_x:\n",
    "    if item in seen:\n",
    "        duplicates_count += 1\n",
    "    else:\n",
    "        seen.add(item)\n",
    "\n",
    "print(f\"Number of duplicates: {duplicates_count}\")\n",
    "print(f\"Number of unique items: {len(seen)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e20752",
   "metadata": {},
   "source": [
    "## Time to train a multi-class multi-label Sweetnet on our properly loaded data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b59cf29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 3.7808 LRAP: 0.0000 NDCG: 0.0549\n",
      "val Loss: 3.8497 LRAP: 0.0000 NDCG: -0.0865\n",
      "Validation loss decreased (0.000000 --> 3.849696).  Saving model ...\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 3.6579 LRAP: 0.0039 NDCG: 0.1479\n",
      "val Loss: 3.8134 LRAP: 0.0000 NDCG: -0.0919\n",
      "Validation loss decreased (3.849696 --> 3.813416).  Saving model ...\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 3.5844 LRAP: 0.0049 NDCG: 0.2212\n",
      "val Loss: 3.7751 LRAP: 0.0000 NDCG: 0.0114\n",
      "Validation loss decreased (3.813416 --> 3.775098).  Saving model ...\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 3.4893 LRAP: 0.0029 NDCG: 0.3154\n",
      "val Loss: 3.7213 LRAP: 0.0000 NDCG: 0.3322\n",
      "Validation loss decreased (3.775098 --> 3.721289).  Saving model ...\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 3.4120 LRAP: 0.0029 NDCG: 0.3777\n",
      "val Loss: 3.6338 LRAP: 0.0000 NDCG: 0.3627\n",
      "Validation loss decreased (3.721289 --> 3.633847).  Saving model ...\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 3.3059 LRAP: 0.0078 NDCG: 0.4122\n",
      "val Loss: 3.3889 LRAP: 0.0137 NDCG: 0.4457\n",
      "Validation loss decreased (3.633847 --> 3.388896).  Saving model ...\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 3.1636 LRAP: 0.0127 NDCG: 0.4394\n",
      "val Loss: 3.1637 LRAP: 0.0091 NDCG: 0.4482\n",
      "Validation loss decreased (3.388896 --> 3.163668).  Saving model ...\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 3.0245 LRAP: 0.0137 NDCG: 0.4561\n",
      "val Loss: 2.7286 LRAP: 0.0091 NDCG: 0.4934\n",
      "Validation loss decreased (3.163668 --> 2.728603).  Saving model ...\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 2.8416 LRAP: 0.0324 NDCG: 0.5081\n",
      "val Loss: 2.4915 LRAP: 0.0183 NDCG: 0.5159\n",
      "Validation loss decreased (2.728603 --> 2.491494).  Saving model ...\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 2.6125 LRAP: 0.0314 NDCG: 0.5302\n",
      "val Loss: 2.4159 LRAP: 0.0457 NDCG: 0.5363\n",
      "Validation loss decreased (2.491494 --> 2.415921).  Saving model ...\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 2.3722 LRAP: 0.0706 NDCG: 0.5536\n",
      "val Loss: 2.1743 LRAP: 0.0913 NDCG: 0.5687\n",
      "Validation loss decreased (2.415921 --> 2.174310).  Saving model ...\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 2.0799 LRAP: 0.1314 NDCG: 0.5834\n",
      "val Loss: 1.8352 LRAP: 0.0502 NDCG: 0.5534\n",
      "Validation loss decreased (2.174310 --> 1.835168).  Saving model ...\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 1.8543 LRAP: 0.2549 NDCG: 0.6134\n",
      "val Loss: 1.7666 LRAP: 0.3014 NDCG: 0.6251\n",
      "Validation loss decreased (1.835168 --> 1.766580).  Saving model ...\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 1.7424 LRAP: 0.3294 NDCG: 0.6362\n",
      "val Loss: 1.7057 LRAP: 0.3288 NDCG: 0.6312\n",
      "Validation loss decreased (1.766580 --> 1.705727).  Saving model ...\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 1.6871 LRAP: 0.3500 NDCG: 0.6439\n",
      "val Loss: 1.6507 LRAP: 0.3470 NDCG: 0.6407\n",
      "Validation loss decreased (1.705727 --> 1.650738).  Saving model ...\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 1.6640 LRAP: 0.4010 NDCG: 0.6506\n",
      "val Loss: 1.6391 LRAP: 0.3653 NDCG: 0.6373\n",
      "Validation loss decreased (1.650738 --> 1.639074).  Saving model ...\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 1.6285 LRAP: 0.4127 NDCG: 0.6537\n",
      "val Loss: 1.6493 LRAP: 0.3470 NDCG: 0.6352\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 1.6083 LRAP: 0.4039 NDCG: 0.6483\n",
      "val Loss: 1.6175 LRAP: 0.3562 NDCG: 0.6362\n",
      "Validation loss decreased (1.639074 --> 1.617467).  Saving model ...\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 1.5976 LRAP: 0.4020 NDCG: 0.6500\n",
      "val Loss: 1.6198 LRAP: 0.3379 NDCG: 0.6273\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 1.5799 LRAP: 0.3912 NDCG: 0.6486\n",
      "val Loss: 1.5935 LRAP: 0.3516 NDCG: 0.6249\n",
      "Validation loss decreased (1.617467 --> 1.593530).  Saving model ...\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 1.5687 LRAP: 0.4020 NDCG: 0.6499\n",
      "val Loss: 1.5742 LRAP: 0.4110 NDCG: 0.6308\n",
      "Validation loss decreased (1.593530 --> 1.574185).  Saving model ...\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 1.5435 LRAP: 0.4049 NDCG: 0.6522\n",
      "val Loss: 1.5734 LRAP: 0.3973 NDCG: 0.6332\n",
      "Validation loss decreased (1.574185 --> 1.573384).  Saving model ...\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 1.5454 LRAP: 0.4039 NDCG: 0.6538\n",
      "val Loss: 1.5364 LRAP: 0.4201 NDCG: 0.6545\n",
      "Validation loss decreased (1.573384 --> 1.536368).  Saving model ...\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 1.5309 LRAP: 0.4010 NDCG: 0.6530\n",
      "val Loss: 1.5264 LRAP: 0.3333 NDCG: 0.6214\n",
      "Validation loss decreased (1.536368 --> 1.526361).  Saving model ...\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 1.5303 LRAP: 0.3716 NDCG: 0.6446\n",
      "val Loss: 1.5251 LRAP: 0.3470 NDCG: 0.6309\n",
      "Validation loss decreased (1.526361 --> 1.525083).  Saving model ...\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 1.5185 LRAP: 0.3853 NDCG: 0.6461\n",
      "val Loss: 1.5195 LRAP: 0.3653 NDCG: 0.6300\n",
      "Validation loss decreased (1.525083 --> 1.519474).  Saving model ...\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 1.5089 LRAP: 0.3725 NDCG: 0.6394\n",
      "val Loss: 1.5363 LRAP: 0.3516 NDCG: 0.6277\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 1.4982 LRAP: 0.3794 NDCG: 0.6414\n",
      "val Loss: 1.4896 LRAP: 0.3607 NDCG: 0.6347\n",
      "Validation loss decreased (1.519474 --> 1.489632).  Saving model ...\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 1.5054 LRAP: 0.4078 NDCG: 0.6514\n",
      "val Loss: 1.5018 LRAP: 0.5068 NDCG: 0.6704\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 1.4933 LRAP: 0.4392 NDCG: 0.6536\n",
      "val Loss: 1.5135 LRAP: 0.5114 NDCG: 0.6729\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 1.4876 LRAP: 0.4412 NDCG: 0.6562\n",
      "val Loss: 1.4855 LRAP: 0.4886 NDCG: 0.6609\n",
      "Validation loss decreased (1.489632 --> 1.485502).  Saving model ...\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 1.4639 LRAP: 0.4500 NDCG: 0.6575\n",
      "val Loss: 1.4711 LRAP: 0.4292 NDCG: 0.6523\n",
      "Validation loss decreased (1.485502 --> 1.471125).  Saving model ...\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 1.4808 LRAP: 0.4490 NDCG: 0.6588\n",
      "val Loss: 1.4752 LRAP: 0.5160 NDCG: 0.6711\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 1.4699 LRAP: 0.4667 NDCG: 0.6646\n",
      "val Loss: 1.4616 LRAP: 0.5023 NDCG: 0.6676\n",
      "Validation loss decreased (1.471125 --> 1.461560).  Saving model ...\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 1.4718 LRAP: 0.4676 NDCG: 0.6600\n",
      "val Loss: 1.4735 LRAP: 0.4612 NDCG: 0.6602\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 1.4612 LRAP: 0.4755 NDCG: 0.6638\n",
      "val Loss: 1.4679 LRAP: 0.5525 NDCG: 0.6761\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 1.4480 LRAP: 0.4951 NDCG: 0.6646\n",
      "val Loss: 1.4510 LRAP: 0.5753 NDCG: 0.6782\n",
      "Validation loss decreased (1.461560 --> 1.451046).  Saving model ...\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 1.4346 LRAP: 0.5137 NDCG: 0.6695\n",
      "val Loss: 1.4427 LRAP: 0.5708 NDCG: 0.6761\n",
      "Validation loss decreased (1.451046 --> 1.442735).  Saving model ...\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 1.4224 LRAP: 0.5078 NDCG: 0.6680\n",
      "val Loss: 1.4367 LRAP: 0.5205 NDCG: 0.6629\n",
      "Validation loss decreased (1.442735 --> 1.436726).  Saving model ...\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 1.4137 LRAP: 0.5304 NDCG: 0.6747\n",
      "val Loss: 1.4139 LRAP: 0.5753 NDCG: 0.6756\n",
      "Validation loss decreased (1.436726 --> 1.413933).  Saving model ...\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 1.4103 LRAP: 0.5304 NDCG: 0.6752\n",
      "val Loss: 1.4189 LRAP: 0.5845 NDCG: 0.6825\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 1.4041 LRAP: 0.5608 NDCG: 0.6827\n",
      "val Loss: 1.4130 LRAP: 0.6164 NDCG: 0.6872\n",
      "Validation loss decreased (1.413933 --> 1.412990).  Saving model ...\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 1.3945 LRAP: 0.5598 NDCG: 0.6761\n",
      "val Loss: 1.4057 LRAP: 0.6073 NDCG: 0.6837\n",
      "Validation loss decreased (1.412990 --> 1.405732).  Saving model ...\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 1.3833 LRAP: 0.5696 NDCG: 0.6790\n",
      "val Loss: 1.3936 LRAP: 0.6164 NDCG: 0.6860\n",
      "Validation loss decreased (1.405732 --> 1.393634).  Saving model ...\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 1.3739 LRAP: 0.5706 NDCG: 0.6847\n",
      "val Loss: 1.3962 LRAP: 0.6119 NDCG: 0.6954\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 1.3581 LRAP: 0.5627 NDCG: 0.6819\n",
      "val Loss: 1.3781 LRAP: 0.6119 NDCG: 0.6965\n",
      "Validation loss decreased (1.393634 --> 1.378052).  Saving model ...\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 1.3673 LRAP: 0.5775 NDCG: 0.6785\n",
      "val Loss: 1.3736 LRAP: 0.6347 NDCG: 0.6996\n",
      "Validation loss decreased (1.378052 --> 1.373642).  Saving model ...\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 1.3474 LRAP: 0.5784 NDCG: 0.6846\n",
      "val Loss: 1.3710 LRAP: 0.6210 NDCG: 0.7052\n",
      "Validation loss decreased (1.373642 --> 1.370959).  Saving model ...\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 1.3395 LRAP: 0.6069 NDCG: 0.6941\n",
      "val Loss: 1.3612 LRAP: 0.6073 NDCG: 0.6927\n",
      "Validation loss decreased (1.370959 --> 1.361157).  Saving model ...\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 1.3229 LRAP: 0.5902 NDCG: 0.6852\n",
      "val Loss: 1.3606 LRAP: 0.5845 NDCG: 0.6825\n",
      "Validation loss decreased (1.361157 --> 1.360624).  Saving model ...\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 1.3405 LRAP: 0.5941 NDCG: 0.6879\n",
      "val Loss: 1.3521 LRAP: 0.6301 NDCG: 0.6980\n",
      "Validation loss decreased (1.360624 --> 1.352059).  Saving model ...\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 1.3268 LRAP: 0.6059 NDCG: 0.6962\n",
      "val Loss: 1.3448 LRAP: 0.6393 NDCG: 0.7069\n",
      "Validation loss decreased (1.352059 --> 1.344760).  Saving model ...\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 1.3190 LRAP: 0.5961 NDCG: 0.7045\n",
      "val Loss: 1.3486 LRAP: 0.6119 NDCG: 0.7118\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 1.3085 LRAP: 0.6167 NDCG: 0.7109\n",
      "val Loss: 1.3362 LRAP: 0.6073 NDCG: 0.7044\n",
      "Validation loss decreased (1.344760 --> 1.336195).  Saving model ...\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 1.2951 LRAP: 0.6147 NDCG: 0.7042\n",
      "val Loss: 1.3383 LRAP: 0.6347 NDCG: 0.7138\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 1.2932 LRAP: 0.6029 NDCG: 0.7068\n",
      "val Loss: 1.3253 LRAP: 0.6256 NDCG: 0.7139\n",
      "Validation loss decreased (1.336195 --> 1.325282).  Saving model ...\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 1.2857 LRAP: 0.6176 NDCG: 0.7181\n",
      "val Loss: 1.3188 LRAP: 0.6256 NDCG: 0.7180\n",
      "Validation loss decreased (1.325282 --> 1.318825).  Saving model ...\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 1.2835 LRAP: 0.6216 NDCG: 0.7153\n",
      "val Loss: 1.3156 LRAP: 0.5936 NDCG: 0.7062\n",
      "Validation loss decreased (1.318825 --> 1.315645).  Saving model ...\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 1.2819 LRAP: 0.6373 NDCG: 0.7176\n",
      "val Loss: 1.3293 LRAP: 0.6301 NDCG: 0.7145\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 1.2747 LRAP: 0.6431 NDCG: 0.7214\n",
      "val Loss: 1.3075 LRAP: 0.6210 NDCG: 0.7109\n",
      "Validation loss decreased (1.315645 --> 1.307491).  Saving model ...\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 1.2776 LRAP: 0.6422 NDCG: 0.7178\n",
      "val Loss: 1.3005 LRAP: 0.6393 NDCG: 0.7151\n",
      "Validation loss decreased (1.307491 --> 1.300547).  Saving model ...\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 1.2840 LRAP: 0.6059 NDCG: 0.7122\n",
      "val Loss: 1.3311 LRAP: 0.6484 NDCG: 0.7249\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 1.2891 LRAP: 0.6294 NDCG: 0.7107\n",
      "val Loss: 1.3239 LRAP: 0.5982 NDCG: 0.6918\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 1.2635 LRAP: 0.6333 NDCG: 0.7203\n",
      "val Loss: 1.3124 LRAP: 0.6347 NDCG: 0.7139\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 1.2532 LRAP: 0.6314 NDCG: 0.7183\n",
      "val Loss: 1.3055 LRAP: 0.6256 NDCG: 0.7109\n",
      "EarlyStopping counter: 4 out of 50\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 1.2433 LRAP: 0.6304 NDCG: 0.7232\n",
      "val Loss: 1.2922 LRAP: 0.6027 NDCG: 0.7051\n",
      "Validation loss decreased (1.300547 --> 1.292248).  Saving model ...\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 1.2391 LRAP: 0.6343 NDCG: 0.7189\n",
      "val Loss: 1.3007 LRAP: 0.6164 NDCG: 0.7096\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 1.2397 LRAP: 0.6265 NDCG: 0.7184\n",
      "val Loss: 1.2921 LRAP: 0.6484 NDCG: 0.7176\n",
      "Validation loss decreased (1.292248 --> 1.292147).  Saving model ...\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 1.2351 LRAP: 0.6275 NDCG: 0.7183\n",
      "val Loss: 1.3115 LRAP: 0.6164 NDCG: 0.7060\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 1.2363 LRAP: 0.6255 NDCG: 0.7202\n",
      "val Loss: 1.2947 LRAP: 0.6027 NDCG: 0.7061\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 1.2231 LRAP: 0.6275 NDCG: 0.7241\n",
      "val Loss: 1.2907 LRAP: 0.6119 NDCG: 0.7096\n",
      "Validation loss decreased (1.292147 --> 1.290748).  Saving model ...\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 1.2245 LRAP: 0.6471 NDCG: 0.7245\n",
      "val Loss: 1.3043 LRAP: 0.6347 NDCG: 0.7105\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 1.2401 LRAP: 0.6373 NDCG: 0.7255\n",
      "val Loss: 1.2942 LRAP: 0.6347 NDCG: 0.7141\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 1.2162 LRAP: 0.6461 NDCG: 0.7226\n",
      "val Loss: 1.2841 LRAP: 0.6256 NDCG: 0.7180\n",
      "Validation loss decreased (1.290748 --> 1.284125).  Saving model ...\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 1.2374 LRAP: 0.6363 NDCG: 0.7223\n",
      "val Loss: 1.2858 LRAP: 0.6119 NDCG: 0.7166\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 1.2238 LRAP: 0.6382 NDCG: 0.7198\n",
      "val Loss: 1.2855 LRAP: 0.6484 NDCG: 0.7101\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 1.2080 LRAP: 0.6461 NDCG: 0.7177\n",
      "val Loss: 1.2817 LRAP: 0.6027 NDCG: 0.7004\n",
      "Validation loss decreased (1.284125 --> 1.281664).  Saving model ...\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 1.2119 LRAP: 0.6500 NDCG: 0.7246\n",
      "val Loss: 1.2862 LRAP: 0.6438 NDCG: 0.7155\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 1.2157 LRAP: 0.6647 NDCG: 0.7317\n",
      "val Loss: 1.2882 LRAP: 0.6484 NDCG: 0.7141\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 1.2199 LRAP: 0.6382 NDCG: 0.7175\n",
      "val Loss: 1.3034 LRAP: 0.5982 NDCG: 0.6904\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 1.2036 LRAP: 0.6539 NDCG: 0.7187\n",
      "val Loss: 1.2760 LRAP: 0.6119 NDCG: 0.6956\n",
      "Validation loss decreased (1.281664 --> 1.275996).  Saving model ...\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 1.2067 LRAP: 0.6569 NDCG: 0.7302\n",
      "val Loss: 1.2823 LRAP: 0.6575 NDCG: 0.7168\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 1.1921 LRAP: 0.6549 NDCG: 0.7278\n",
      "val Loss: 1.2849 LRAP: 0.6347 NDCG: 0.7048\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 1.1844 LRAP: 0.6578 NDCG: 0.7251\n",
      "val Loss: 1.2733 LRAP: 0.6256 NDCG: 0.7050\n",
      "Validation loss decreased (1.275996 --> 1.273283).  Saving model ...\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 1.1744 LRAP: 0.6461 NDCG: 0.7284\n",
      "val Loss: 1.2767 LRAP: 0.6438 NDCG: 0.7201\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 1.1786 LRAP: 0.6422 NDCG: 0.7276\n",
      "val Loss: 1.2696 LRAP: 0.6164 NDCG: 0.7123\n",
      "Validation loss decreased (1.273283 --> 1.269646).  Saving model ...\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 1.1743 LRAP: 0.6510 NDCG: 0.7252\n",
      "val Loss: 1.2617 LRAP: 0.6301 NDCG: 0.7204\n",
      "Validation loss decreased (1.269646 --> 1.261686).  Saving model ...\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 1.1797 LRAP: 0.6471 NDCG: 0.7310\n",
      "val Loss: 1.2697 LRAP: 0.6164 NDCG: 0.7156\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 1.1685 LRAP: 0.6422 NDCG: 0.7313\n",
      "val Loss: 1.2670 LRAP: 0.6164 NDCG: 0.7156\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 1.1730 LRAP: 0.6480 NDCG: 0.7310\n",
      "val Loss: 1.2659 LRAP: 0.6256 NDCG: 0.7143\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 1.1738 LRAP: 0.6471 NDCG: 0.7232\n",
      "val Loss: 1.2971 LRAP: 0.6301 NDCG: 0.7094\n",
      "EarlyStopping counter: 4 out of 50\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 1.1678 LRAP: 0.6500 NDCG: 0.7309\n",
      "val Loss: 1.2636 LRAP: 0.6393 NDCG: 0.7129\n",
      "EarlyStopping counter: 5 out of 50\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 1.1585 LRAP: 0.6471 NDCG: 0.7292\n",
      "val Loss: 1.2609 LRAP: 0.6301 NDCG: 0.7119\n",
      "Validation loss decreased (1.261686 --> 1.260854).  Saving model ...\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 1.1584 LRAP: 0.6471 NDCG: 0.7295\n",
      "val Loss: 1.2614 LRAP: 0.6164 NDCG: 0.7096\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 1.1479 LRAP: 0.6549 NDCG: 0.7302\n",
      "val Loss: 1.2639 LRAP: 0.6210 NDCG: 0.7107\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 1.1568 LRAP: 0.6500 NDCG: 0.7321\n",
      "val Loss: 1.2656 LRAP: 0.6119 NDCG: 0.7133\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 1.1633 LRAP: 0.6461 NDCG: 0.7323\n",
      "val Loss: 1.2658 LRAP: 0.6119 NDCG: 0.7133\n",
      "EarlyStopping counter: 4 out of 50\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 1.1463 LRAP: 0.6461 NDCG: 0.7336\n",
      "val Loss: 1.2633 LRAP: 0.6119 NDCG: 0.7133\n",
      "EarlyStopping counter: 5 out of 50\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 1.1413 LRAP: 0.6598 NDCG: 0.7333\n",
      "val Loss: 1.2629 LRAP: 0.6119 NDCG: 0.7156\n",
      "EarlyStopping counter: 6 out of 50\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 1.1541 LRAP: 0.6471 NDCG: 0.7274\n",
      "val Loss: 1.2629 LRAP: 0.6119 NDCG: 0.7145\n",
      "EarlyStopping counter: 7 out of 50\n",
      "\n",
      "Training complete in 0m 48s\n",
      "Best val loss: 1.260854, best LRAP score: 0.8946\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders\n",
    "model = prep_model('SweetNet', classes) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes)\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'multilabel',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Try with my new glm-infused data\n",
    "\n",
    "classes = 15 # 15 kingdoms in the dataset, should read that from the data instead\n",
    "dataloaders = multilabel_kingdom_loaders_emb\n",
    "model = prep_model('SweetNet', classes, use_external_embeddings = True) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes) #changed to 0.01 from 0.005 from 0.0005\n",
    "model_ft = train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 1, mode = 'multilabel', use_external_embeddings = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5025834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweetnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
