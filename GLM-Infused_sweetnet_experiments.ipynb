{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2416a1b",
   "metadata": {},
   "source": [
    "# GLM-Infused SweetNet Experiments\n",
    "\n",
    "Experimenting with a modified version of SweetNet that allows it to take pre-trained embeddings as input. To get there I need a way to take the embeddings Iâ€™ve gotten from roman and transform them into nice inputs for the model, and a way to set the initial features using these inputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652c8f8",
   "metadata": {},
   "source": [
    "## Importing and exploring the GLM Embedding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0485fe3",
   "metadata": {},
   "source": [
    "### Exploring embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: glm_embeddings_1.pkl\n",
      "Embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# quick thing to load a pickle file\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            loaded_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of loaded object: <class 'dict'>\n",
      "Number of items (if dictionary): 2565\n",
      "Example keys (first 5): ['!GlcNAc', '-10', '-12', '-2', '-4']\n"
     ]
    }
   ],
   "source": [
    "# lets do some quick exploration\n",
    "\n",
    "# --- Explore the loaded data ---\n",
    "print(f\"Type of loaded object: {type(loaded_embeddings)}\")\n",
    "\n",
    "# Common formats for embeddings: dictionary or numpy array\n",
    "if isinstance(loaded_embeddings, dict):\n",
    "    print(f\"Number of items (if dictionary): {len(loaded_embeddings)}\")\n",
    "    # print some keys to see what they look like\n",
    "    print(f\"Example keys (first 5): {list(loaded_embeddings.keys())[:5]}\")\n",
    "elif hasattr(loaded_embeddings, 'shape'):\n",
    "    print(f\"Shape (if array/tensor): {loaded_embeddings.shape}\")\n",
    "    if hasattr(loaded_embeddings, 'dtype'):\n",
    "         print(f\"Data type (if array/tensor): {loaded_embeddings.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e6aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-6', '-8', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic', '1,5-Anhydro-Man-ol', '1,5-Anhydro-ManNAc-ol', '1,5-Anhydro-Xyl-ol', '1,5-Anhydro-Xyl2F-ol', '1-1', '1-2', '1-3', '1-4', '1-5', '1-6']\n"
     ]
    }
   ],
   "source": [
    "print(list(loaded_embeddings.keys())[5:30]) # Print more keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of value for '-8': <class 'numpy.ndarray'>\n",
      "Shape of value: (320,)\n",
      "Dtype of value: float32\n",
      "[ 9.33886290e-01 -7.57189512e-01 -5.22765040e-01  4.93726492e-01\n",
      "  3.03156078e-01 -1.72754931e+00  2.03015614e+00 -1.13539708e+00\n",
      " -8.32044244e-01 -6.09763384e-01 -5.63947335e-02 -2.68140852e-01\n",
      " -6.37493312e-01  1.45667583e-01 -7.75620103e-01 -1.39048725e-01\n",
      "  1.06042847e-01 -3.74972522e-01  7.91566074e-01 -1.03034627e+00\n",
      " -1.12639211e-01 -3.78986076e-03  5.92547238e-01  2.81559825e-01\n",
      " -5.21002829e-01  9.35327411e-01  2.56601274e-01 -3.91364455e-01\n",
      "  2.72188634e-02  5.00928342e-01 -5.55309415e-01  1.28289807e+00\n",
      " -6.45282388e-01  5.19899249e-01  6.10100806e-01  1.84122849e+00\n",
      "  3.11432898e-01 -7.64928609e-02 -1.05589128e+00  6.50653005e-01\n",
      "  9.70111132e-01  7.40227938e-01  8.39829683e-01 -3.04328918e-01\n",
      " -1.06630003e+00  4.53770608e-01  4.27673876e-01 -6.02427721e-01\n",
      "  4.39536482e-01 -1.16493046e+00 -2.04154789e-01  1.13036299e+00\n",
      "  2.51586974e-01  1.04393315e+00  2.60879964e-01  4.63881493e-02\n",
      "  8.49927664e-01 -1.21275747e+00 -5.25301337e-01 -7.54553556e-01\n",
      " -5.36846638e-01  1.71898973e+00  1.07118464e+00  1.25938666e+00\n",
      "  7.28268623e-01  2.50012755e-01 -8.84264708e-01  3.54878515e-01\n",
      " -9.51814711e-01  1.92197442e-01  6.22674108e-01 -7.19715357e-02\n",
      " -2.53418744e-01  6.10054433e-01 -1.37844992e+00  1.10613918e+00\n",
      " -7.89550483e-01  4.11728621e-01 -1.39660871e+00 -2.74130464e-01\n",
      " -4.85218346e-01 -1.64008796e+00 -2.54515797e-01 -4.76354361e-02\n",
      "  1.70321250e+00  1.37953115e+00  6.62403643e-01  1.23904690e-01\n",
      " -2.03382596e-02  2.49572158e-01 -1.19476050e-01  1.01610112e+00\n",
      "  1.54832602e-01 -3.18885893e-01  1.02479362e+00  2.19304472e-01\n",
      " -1.77515924e-01 -2.96848416e-01 -1.51161349e+00 -1.55658543e+00\n",
      "  6.01615787e-01 -1.18876457e+00  6.75462842e-01 -1.21065450e+00\n",
      "  1.00956786e+00  5.41580915e-01  4.89682317e-01 -4.31063682e-01\n",
      " -6.99561596e-01 -9.50598717e-01 -4.71236914e-01  8.96337509e-01\n",
      "  1.97975963e-01  6.51351273e-01 -1.65811467e+00 -2.37476051e-01\n",
      "  1.22424424e+00  3.85935336e-01  1.74970782e+00  1.08295810e+00\n",
      " -2.08416104e-01 -1.44780791e+00 -3.18115175e-01 -2.69204080e-02\n",
      " -7.30906725e-01  3.65380794e-01 -5.23220778e-01 -1.59638667e+00\n",
      "  9.76120412e-01  4.75375116e-01  1.10794783e+00 -9.16275680e-01\n",
      "  8.67535770e-01 -2.21260801e-01  3.58714461e-02 -1.62487292e+00\n",
      "  9.47338939e-01  2.52621353e-01 -2.44861484e-01  4.85217899e-01\n",
      " -1.72671735e-01  1.49431840e-01 -9.26872373e-01 -6.38668120e-01\n",
      " -1.37115136e-01  1.30791855e+00  1.25448748e-01  3.05962026e-01\n",
      " -2.51638025e-01  6.88706279e-01 -6.43941760e-01  6.10008895e-01\n",
      "  2.45932966e-01  1.53176570e+00 -2.05617994e-01  5.01646757e-01\n",
      " -4.11370814e-01 -5.36742508e-01 -1.23477876e-02  6.50121808e-01\n",
      " -3.78578186e-01  6.62264466e-01  1.53327346e-01 -9.97333288e-01\n",
      "  2.86916673e-01 -3.98133188e-01  1.19174033e-01 -1.07086766e+00\n",
      "  5.68605885e-02  8.55352730e-02 -2.43456244e-01 -5.13940752e-01\n",
      "  9.52608764e-01 -3.56329709e-01 -9.76832956e-02  1.55454218e-01\n",
      "  1.07665420e-01  7.78901517e-01  1.94103813e+00  5.98729789e-01\n",
      "  1.49250478e-01  6.60319090e-01 -9.16693985e-01 -1.80390513e+00\n",
      " -1.08837974e+00 -5.85823774e-01 -5.17625034e-01  1.13187218e+00\n",
      " -3.11186165e-01 -1.56313211e-01  4.89638031e-01  6.32191420e-01\n",
      " -9.01452422e-01  3.40963513e-01  3.77618819e-01  4.78747129e-01\n",
      " -1.26142776e+00  1.63014054e+00 -7.38181099e-02 -8.88819635e-01\n",
      " -9.81908560e-01 -3.11309278e-01 -2.87041283e+00 -6.68797910e-01\n",
      "  1.15292573e+00  1.82262063e+00  6.86679184e-01  3.54639411e-01\n",
      "  1.14279723e+00  1.23592412e+00 -4.26488072e-01  5.78116417e-01\n",
      "  2.67315298e-01  1.73516899e-01 -6.95198655e-01 -7.84443021e-01\n",
      "  1.87699527e-01  7.76465774e-01  1.17747712e+00  2.98208922e-01\n",
      "  1.80739570e+00 -6.55146241e-02  2.10267353e+00 -1.49224257e+00\n",
      "  1.67633876e-01 -5.96812427e-01  4.02143002e-01 -5.80711842e-01\n",
      " -6.86030865e-01  2.82077312e-01  4.62324202e-01 -8.51680398e-01\n",
      " -6.37305975e-01 -1.97909772e-01  8.27008903e-01 -2.47440666e-01\n",
      "  5.40550411e-01  2.20697820e-02 -3.67172241e-01  1.37753654e+00\n",
      "  2.57560164e-01  1.12044883e+00  1.47008979e+00 -3.09366286e-01\n",
      "  1.41206241e+00 -1.07911384e+00 -3.82883579e-01  1.15288660e-01\n",
      "  6.46931171e-01 -1.63524508e+00 -4.82143342e-01 -2.22676694e-02\n",
      " -2.94011176e-01  1.76649165e+00 -1.42879653e+00 -1.01673603e+00\n",
      "  6.92535341e-01  1.08943865e-01 -1.51619220e+00 -1.31418991e+00\n",
      " -5.36556542e-01 -9.08092409e-02 -3.43192220e-02 -5.01663029e-01\n",
      " -4.27816272e-01  5.04320741e-01 -8.19638968e-01  1.27975166e-01\n",
      "  6.98855758e-01  4.11748588e-01 -2.63869703e-01 -1.72789741e+00\n",
      "  2.40177006e-01 -3.30802739e-01  1.47785515e-01  4.70187128e-01\n",
      "  3.38367313e-01 -1.54152012e+00  3.17173868e-01 -1.70832485e-01\n",
      "  9.85031009e-01 -1.51257575e+00  7.86181986e-01  2.95546353e-01\n",
      "  4.57608998e-02 -6.43859148e-01  4.83155847e-01 -1.51108074e+00\n",
      " -1.82736918e-01 -3.47120881e-01 -5.70403397e-01 -1.21720120e-01\n",
      " -1.61197579e+00  1.02913380e-03 -4.93016541e-02 -1.70051694e+00\n",
      " -4.81017500e-01 -9.90746021e-01  3.51191968e-01 -6.38143182e-01\n",
      "  8.80924284e-01  1.06428635e+00 -1.31740403e+00 -1.46576715e+00\n",
      " -8.72395873e-01  1.48068953e+00 -2.76599586e-01 -1.15330029e+00\n",
      "  1.45732999e-01 -1.63671541e+00  2.22910285e-01 -3.31862628e-01\n",
      "  5.65533102e-01 -4.64938819e-01  1.83547580e+00 -7.03186333e-01\n",
      "  2.57217407e-01  1.83000445e+00  1.64521456e-01  1.26764941e+00]\n"
     ]
    }
   ],
   "source": [
    "example_key = '!GlcNAc' \n",
    "if example_key in loaded_embeddings:\n",
    "    embedding_vector = loaded_embeddings[example_key]\n",
    "    print(f\"Type of value for '{example_key}': {type(embedding_vector)}\")\n",
    "    if hasattr(embedding_vector, 'shape'):\n",
    "        print(f\"Shape of value: {embedding_vector.shape}\") # This gives dimensionality!\n",
    "        print(f\"Dtype of value: {embedding_vector.dtype}\")\n",
    "    print(embedding_vector) # Print the vector itself if it's not too long\n",
    "else:\n",
    "    print(f\"Key '{example_key}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7f66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'other': 122, 'linkage_or_modification': 36, 'monosaccharide': 2407})\n"
     ]
    }
   ],
   "source": [
    "# let's look at the keys a bit more closely\n",
    "\n",
    "import collections\n",
    "\n",
    "key_types = collections.defaultdict(int)\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        key_types['linkage_or_modification'] += 1\n",
    "    elif key[0].isalpha():\n",
    "        key_types['monosaccharide'] += 1\n",
    "    else:\n",
    "        key_types['other'] += 1\n",
    "\n",
    "print(key_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd6a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'other' keys: 122\n",
      "Examples of 'other' keys: ['!GlcNAc', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic', '1,5-Anhydro-Man-ol', '1,5-Anhydro-ManNAc-ol', '1,5-Anhydro-Xyl-ol', '1,5-Anhydro-Xyl2F-ol', '1b-4', '1dAlt-ol']\n"
     ]
    }
   ],
   "source": [
    "# Let's explore those Other keys \n",
    "\n",
    "other_keys = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        other_keys.append(key)\n",
    "\n",
    "print(f\"Number of 'other' keys: {len(other_keys)}\")\n",
    "print(f\"Examples of 'other' keys: {other_keys[:20]}\") # Print the first 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398bd9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More Examples of 'other' keys: ['1dEry-ol', '2,3-Anhydro-All', '2,3-Anhydro-Man', '2,3-Anhydro-Rib', '2,5-Anhydro-D-Alt-ol', '2,5-Anhydro-D-Alt3S-ol', '2,5-Anhydro-D-Tal', '2,5-Anhydro-Glc', '2,5-Anhydro-L-Man-ol', '2,5-Anhydro-Man', '2,5-Anhydro-Man-ol', '2,5-Anhydro-Man1S-ol', '2,5-Anhydro-Man3S-ol', '2,5-Anhydro-Man6S', '2,5-Anhydro-Tal-ol', '2,5-Anhydro-Tal6P', '2,6-Anhydro-Glc5NAc-ol', '2,6-Anhydro-L-Gul-ol', '2,6-Anhydro-L-Gul-onic', '2,6-Anhydro-Man-ol', '2,6-Anhydro-Tal5NAc-ol', '2,7-Anhydro-Kdo', '2,7-Anhydro-Kdof', '2dAraHexA', '3,6-Anhydro-Fruf', '3,6-Anhydro-Gal', '3,6-Anhydro-Gal2S', '3,6-Anhydro-Glc', '3,6-Anhydro-L-Gal', '3,6-Anhydro-L-Gal2Me', '3-Anhydro-Gal', '3-Anhydro-Gal2S', '3dFuc', '3dGal', '3dLyxHep-ulosaric', '4,7-Anhydro-Kdo', '4,7-Anhydro-KdoOPEtN', '4,8-Anhydro-Kdo', '4d8dNeu5Ac', '4dAraHex', '4dEry-ol', '4dFuc', '4dGal', '4dNeu5Ac', '4dThrHexNAcA4en', '4eLeg5Ac7Ac', '5dAraf', '5dAraf3Me', '5dLyxf3CFo', '5dLyxf3CMe']\n"
     ]
    }
   ],
   "source": [
    "# Let's look at 50 more keys\n",
    "\n",
    "print(f\"More Examples of 'other' keys: {other_keys[20:70]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3a766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'monosaccharide' keys: 2407\n",
      "Examples of 'monosaccharide' keys: ['Abe', 'Abe1PP', 'Abe2Ac', 'AbeOAc', 'Acarbose', 'AcefA', 'Aci5Ac7Ac', 'AcoNAc', 'All', 'All-ol', 'All1S2S3S4S', 'All2Ac3Ac', 'All2S3S4S', 'All3Ac', 'All6Ac', 'AllN', 'AllN1P', 'AllNAc', 'AllNAc6Me', 'AllOMe', 'Alt', 'AltA', 'AltA2N', 'AltA2S', 'AltAN', 'AltNAc', 'AltNAcA', 'AltNAcA1Prop', 'Altf', 'AltfOAc', 'Amikacin', 'Api', 'ApiOAc', 'ApiOMe-ol', 'Apif', 'Ara', 'Ara-ol', 'Ara1Cer2Ac', 'Ara1Me', 'Ara1N4P', 'Ara1P4N', 'Ara1PP', 'Ara1PP2NAc', 'Ara1PP4N', 'Ara1PP4NFo', 'Ara2Ac', 'Ara2Ac3Ac', 'Ara2Ac3Ac4Ac', 'Ara2Ac4Ac', 'Ara2Ac5P-ol']\n"
     ]
    }
   ],
   "source": [
    "# Let's explore those monosaccharide keys\n",
    "monosaccharide = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        monosaccharide.append(key)\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'monosaccharide' keys: {len(monosaccharide)}\")\n",
    "print(f\"Examples of 'monosaccharide' keys: {monosaccharide[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ddd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'linkage_or_modification' keys: 36\n",
      "Examples of 'linkage_or_modification' keys: ['-10', '-12', '-2', '-4', '-6', '-8', '1-1', '1-2', '1-3', '1-4', '1-5', '1-6', '1-?', '2-3', '2-4', '2-5', '2-6', '3-1', '3-5', '4-1', '4-5', '5-1', '5-2', '5-3', '5-4', '5-5', '5-6', '6-1', '6-3', '6-4', '?1-2', '?1-3', '?1-4', '?1-6', '?1-?', '?2-?']\n"
     ]
    }
   ],
   "source": [
    "# To be throughough, let's look at 50 Linkage or Modification keys as well\n",
    "linkage_or_modification = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        linkage_or_modification.append(key)\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'linkage_or_modification' keys: {len(linkage_or_modification)}\")\n",
    "print(f\"Examples of 'linkage_or_modification' keys: {linkage_or_modification[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "df823d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All embeddings in the loaded dictionary appear to be the same.\n",
      "Number of embeddings in the dictionary: 2565\n",
      "First embedding:\n",
      "[ 9.33886290e-01 -7.57189512e-01 -5.22765040e-01  4.93726492e-01\n",
      "  3.03156078e-01 -1.72754931e+00  2.03015614e+00 -1.13539708e+00\n",
      " -8.32044244e-01 -6.09763384e-01 -5.63947335e-02 -2.68140852e-01\n",
      " -6.37493312e-01  1.45667583e-01 -7.75620103e-01 -1.39048725e-01\n",
      "  1.06042847e-01 -3.74972522e-01  7.91566074e-01 -1.03034627e+00\n",
      " -1.12639211e-01 -3.78986076e-03  5.92547238e-01  2.81559825e-01\n",
      " -5.21002829e-01  9.35327411e-01  2.56601274e-01 -3.91364455e-01\n",
      "  2.72188634e-02  5.00928342e-01 -5.55309415e-01  1.28289807e+00\n",
      " -6.45282388e-01  5.19899249e-01  6.10100806e-01  1.84122849e+00\n",
      "  3.11432898e-01 -7.64928609e-02 -1.05589128e+00  6.50653005e-01\n",
      "  9.70111132e-01  7.40227938e-01  8.39829683e-01 -3.04328918e-01\n",
      " -1.06630003e+00  4.53770608e-01  4.27673876e-01 -6.02427721e-01\n",
      "  4.39536482e-01 -1.16493046e+00 -2.04154789e-01  1.13036299e+00\n",
      "  2.51586974e-01  1.04393315e+00  2.60879964e-01  4.63881493e-02\n",
      "  8.49927664e-01 -1.21275747e+00 -5.25301337e-01 -7.54553556e-01\n",
      " -5.36846638e-01  1.71898973e+00  1.07118464e+00  1.25938666e+00\n",
      "  7.28268623e-01  2.50012755e-01 -8.84264708e-01  3.54878515e-01\n",
      " -9.51814711e-01  1.92197442e-01  6.22674108e-01 -7.19715357e-02\n",
      " -2.53418744e-01  6.10054433e-01 -1.37844992e+00  1.10613918e+00\n",
      " -7.89550483e-01  4.11728621e-01 -1.39660871e+00 -2.74130464e-01\n",
      " -4.85218346e-01 -1.64008796e+00 -2.54515797e-01 -4.76354361e-02\n",
      "  1.70321250e+00  1.37953115e+00  6.62403643e-01  1.23904690e-01\n",
      " -2.03382596e-02  2.49572158e-01 -1.19476050e-01  1.01610112e+00\n",
      "  1.54832602e-01 -3.18885893e-01  1.02479362e+00  2.19304472e-01\n",
      " -1.77515924e-01 -2.96848416e-01 -1.51161349e+00 -1.55658543e+00\n",
      "  6.01615787e-01 -1.18876457e+00  6.75462842e-01 -1.21065450e+00\n",
      "  1.00956786e+00  5.41580915e-01  4.89682317e-01 -4.31063682e-01\n",
      " -6.99561596e-01 -9.50598717e-01 -4.71236914e-01  8.96337509e-01\n",
      "  1.97975963e-01  6.51351273e-01 -1.65811467e+00 -2.37476051e-01\n",
      "  1.22424424e+00  3.85935336e-01  1.74970782e+00  1.08295810e+00\n",
      " -2.08416104e-01 -1.44780791e+00 -3.18115175e-01 -2.69204080e-02\n",
      " -7.30906725e-01  3.65380794e-01 -5.23220778e-01 -1.59638667e+00\n",
      "  9.76120412e-01  4.75375116e-01  1.10794783e+00 -9.16275680e-01\n",
      "  8.67535770e-01 -2.21260801e-01  3.58714461e-02 -1.62487292e+00\n",
      "  9.47338939e-01  2.52621353e-01 -2.44861484e-01  4.85217899e-01\n",
      " -1.72671735e-01  1.49431840e-01 -9.26872373e-01 -6.38668120e-01\n",
      " -1.37115136e-01  1.30791855e+00  1.25448748e-01  3.05962026e-01\n",
      " -2.51638025e-01  6.88706279e-01 -6.43941760e-01  6.10008895e-01\n",
      "  2.45932966e-01  1.53176570e+00 -2.05617994e-01  5.01646757e-01\n",
      " -4.11370814e-01 -5.36742508e-01 -1.23477876e-02  6.50121808e-01\n",
      " -3.78578186e-01  6.62264466e-01  1.53327346e-01 -9.97333288e-01\n",
      "  2.86916673e-01 -3.98133188e-01  1.19174033e-01 -1.07086766e+00\n",
      "  5.68605885e-02  8.55352730e-02 -2.43456244e-01 -5.13940752e-01\n",
      "  9.52608764e-01 -3.56329709e-01 -9.76832956e-02  1.55454218e-01\n",
      "  1.07665420e-01  7.78901517e-01  1.94103813e+00  5.98729789e-01\n",
      "  1.49250478e-01  6.60319090e-01 -9.16693985e-01 -1.80390513e+00\n",
      " -1.08837974e+00 -5.85823774e-01 -5.17625034e-01  1.13187218e+00\n",
      " -3.11186165e-01 -1.56313211e-01  4.89638031e-01  6.32191420e-01\n",
      " -9.01452422e-01  3.40963513e-01  3.77618819e-01  4.78747129e-01\n",
      " -1.26142776e+00  1.63014054e+00 -7.38181099e-02 -8.88819635e-01\n",
      " -9.81908560e-01 -3.11309278e-01 -2.87041283e+00 -6.68797910e-01\n",
      "  1.15292573e+00  1.82262063e+00  6.86679184e-01  3.54639411e-01\n",
      "  1.14279723e+00  1.23592412e+00 -4.26488072e-01  5.78116417e-01\n",
      "  2.67315298e-01  1.73516899e-01 -6.95198655e-01 -7.84443021e-01\n",
      "  1.87699527e-01  7.76465774e-01  1.17747712e+00  2.98208922e-01\n",
      "  1.80739570e+00 -6.55146241e-02  2.10267353e+00 -1.49224257e+00\n",
      "  1.67633876e-01 -5.96812427e-01  4.02143002e-01 -5.80711842e-01\n",
      " -6.86030865e-01  2.82077312e-01  4.62324202e-01 -8.51680398e-01\n",
      " -6.37305975e-01 -1.97909772e-01  8.27008903e-01 -2.47440666e-01\n",
      "  5.40550411e-01  2.20697820e-02 -3.67172241e-01  1.37753654e+00\n",
      "  2.57560164e-01  1.12044883e+00  1.47008979e+00 -3.09366286e-01\n",
      "  1.41206241e+00 -1.07911384e+00 -3.82883579e-01  1.15288660e-01\n",
      "  6.46931171e-01 -1.63524508e+00 -4.82143342e-01 -2.22676694e-02\n",
      " -2.94011176e-01  1.76649165e+00 -1.42879653e+00 -1.01673603e+00\n",
      "  6.92535341e-01  1.08943865e-01 -1.51619220e+00 -1.31418991e+00\n",
      " -5.36556542e-01 -9.08092409e-02 -3.43192220e-02 -5.01663029e-01\n",
      " -4.27816272e-01  5.04320741e-01 -8.19638968e-01  1.27975166e-01\n",
      "  6.98855758e-01  4.11748588e-01 -2.63869703e-01 -1.72789741e+00\n",
      "  2.40177006e-01 -3.30802739e-01  1.47785515e-01  4.70187128e-01\n",
      "  3.38367313e-01 -1.54152012e+00  3.17173868e-01 -1.70832485e-01\n",
      "  9.85031009e-01 -1.51257575e+00  7.86181986e-01  2.95546353e-01\n",
      "  4.57608998e-02 -6.43859148e-01  4.83155847e-01 -1.51108074e+00\n",
      " -1.82736918e-01 -3.47120881e-01 -5.70403397e-01 -1.21720120e-01\n",
      " -1.61197579e+00  1.02913380e-03 -4.93016541e-02 -1.70051694e+00\n",
      " -4.81017500e-01 -9.90746021e-01  3.51191968e-01 -6.38143182e-01\n",
      "  8.80924284e-01  1.06428635e+00 -1.31740403e+00 -1.46576715e+00\n",
      " -8.72395873e-01  1.48068953e+00 -2.76599586e-01 -1.15330029e+00\n",
      "  1.45732999e-01 -1.63671541e+00  2.22910285e-01 -3.31862628e-01\n",
      "  5.65533102e-01 -4.64938819e-01  1.83547580e+00 -7.03186333e-01\n",
      "  2.57217407e-01  1.83000445e+00  1.64521456e-01  1.26764941e+00]\n"
     ]
    }
   ],
   "source": [
    "# wait, a couple of weeks later and I couldn't figure out why my glm-infused model wasn't converging, \n",
    "# and I just realised that the embeddings were all the same, no wonder it didn't work. \n",
    "# I should have looked at the embeddings themselves, not just the keys.\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    loaded_embeddings = pickle.load(f)\n",
    "\n",
    "first_embedding = None\n",
    "all_same = True\n",
    "if loaded_embeddings:\n",
    "    first_key = next(iter(loaded_embeddings))\n",
    "    first_embedding = loaded_embeddings[first_key]\n",
    "    for key, embedding in loaded_embeddings.items():\n",
    "        if not (embedding == first_embedding).all():\n",
    "            all_same = False\n",
    "            print(f\"Found a different embedding for key: {key}\")\n",
    "            break\n",
    "\n",
    "if all_same and first_embedding is not None:\n",
    "    print(\"All embeddings in the loaded dictionary appear to be the same.\")\n",
    "elif first_embedding is None:\n",
    "    print(\"The embedding dictionary is empty.\")\n",
    "else:\n",
    "    print(\"Embeddings in the dictionary are not all the same.\")\n",
    "\n",
    "print(f\"Number of embeddings in the dictionary: {len(loaded_embeddings)}\")\n",
    "if first_embedding is not None:\n",
    "    print(f\"First embedding:\")\n",
    "    print(first_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9ad5a",
   "metadata": {},
   "source": [
    "### Load the glycowork libr\n",
    "\n",
    "I'll load the glycowork library and compare it to the keys in the embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b14506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in glycowork vocabulary: 2565\n",
      "Example keys from glycowork vocabulary (first 20): ['!GlcNAc', '-10', '-12', '-2', '-4', '-6', '-8', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic']\n"
     ]
    }
   ],
   "source": [
    "from glycowork.glycan_data import loader\n",
    "\n",
    "glycowork_vocabulary = loader.lib\n",
    "\n",
    "print(f\"Number of items in glycowork vocabulary: {len(glycowork_vocabulary)}\")\n",
    "print(f\"Example keys from glycowork vocabulary (first 20): {list(glycowork_vocabulary.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e1f3d",
   "metadata": {},
   "source": [
    "Nice, they seem to correspond one to one!\n",
    "\n",
    "That saves me a lot of work down the line (Thanks Roman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of value for '-10': <class 'int'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# let's look at one of the keys in the glycowork vocabulary to see what they return\n",
    "example_glycowork_key = '-10'\n",
    "if example_glycowork_key in glycowork_vocabulary:\n",
    "    glycowork_value = glycowork_vocabulary[example_glycowork_key]\n",
    "    print(f\"Type of value for '{example_glycowork_key}': {type(glycowork_value)}\")\n",
    "    print(glycowork_value)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f827a0",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54413ae2",
   "metadata": {},
   "source": [
    "### Load, filter, and transform glycowork data into glycan_loaders ||||run on kernel restart||||\n",
    "This is used to load and filter glycowork data for a specific prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79b4d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the full sugarbase dataset and make a custom dataframe for the problem you want to tackle\n",
    "\n",
    "from glycowork.glycan_data.loader import build_custom_df, df_glycan\n",
    "from glycowork.ml.train_test_split import prepare_multilabel\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "def build_multilabel_dataset(glycan_dataset: str = 'df_species',\n",
    "                          glycan_class: str = 'Kingdom',\n",
    "                          min_class_size: int = 6) -> Tuple[List[str], List[List[float]], List[str]]:\n",
    "    \"\"\"\n",
    "    Loads glycan data, prepares it for multi-label classification,\n",
    "    and removes classes with fewer than min_class_size samples.\n",
    "\n",
    "    Args:\n",
    "        glycan_dataset: The glycowork dataset to use. Options include: \n",
    "            'df_species', 'df_tissue', and 'df_disease'.            \n",
    "        glycan_class: The class to predict. Options include:\n",
    "            df_species: 'Species', 'Genus', 'Family', 'Order', 'Class', 'Phylum', 'Kingdom', 'Domain', 'ref'\n",
    "            df_tissue:  'tissue_sample', 'tissue_species', 'tissue_id', 'tissue_ref'\n",
    "            df_disease: 'disease_association', 'disease_sample', 'disease_direction', 'disease_species', 'disease_id', 'disease_ref'\n",
    "        min_class_size: Minimum number of samples required for a class to be included. Set to 1 to include all classes.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - glycan_sequences: List of glycan strings after filtering rare classes.\n",
    "        - binary_labels: List of corresponding multi-label binary vectors.\n",
    "        - label_names: The ordered list of names for each position in the binary vectors.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    all_glycan_data = df_glycan\n",
    "\n",
    "    # Build custom dataframe\n",
    "    custom_glycan_df = build_custom_df(all_glycan_data, glycan_dataset)\n",
    "\n",
    "    # Extract the list of unique individual labels from the chosen class from the custom_glycan_df\n",
    "    # These are used to dechipher the labels when the model is used for prediction\n",
    "    all_possible_label_names = sorted(list(custom_glycan_df[glycan_class].unique()))\n",
    "    print(f\"Found {len(all_possible_label_names)} unique individual classes/labels.\")\n",
    "\n",
    "\n",
    "    # Prepare for multi-label prediction\n",
    "    glycans, labels = prepare_multilabel(custom_glycan_df, glycan_class)\n",
    "\n",
    "    # if needed, removes classes with fewer than min_class_size samples.\n",
    "    if(min_class_size > 1):\n",
    "\n",
    "        # Convert labels to string representation for counting\n",
    "        label_strings = [''.join(map(str, label)) for label in labels]\n",
    "\n",
    "        # Count occurrences of each label combination\n",
    "        label_counts = Counter(label_strings)\n",
    "\n",
    "        # Filter glycans and labels based on class size\n",
    "        glycan_sequences = [glycans[i] for i, label_str in enumerate(label_strings) if label_counts[label_str] >= min_class_size]\n",
    "        binary_labels_unfiltered = [labels[i] for i, label_str in enumerate(label_strings) if label_counts[label_str] >= min_class_size]\n",
    "        print(f\"Number of unique glycans left after filtering rare classes (size >= {min_class_size}): {len(glycan_sequences)}/{len(glycans)}\")\n",
    "        # Filter out individual labels with no positive examples after glycan filtering\n",
    "\n",
    "        # Convert binary_labels to numpy array for easier column manipulation\n",
    "        binary_labels_np = np.array(binary_labels_unfiltered)\n",
    "\n",
    "        # Find indices of labels with at least one positive example\n",
    "        # Sum across rows (axis=0) to get count for each label\n",
    "        label_sums = binary_labels_np.sum(axis=0)\n",
    "        active_label_indices = np.where(label_sums > 0)[0]\n",
    "\n",
    "        # Create the final list of label names using the active indices\n",
    "        # Use the initially generated sorted list (all_possible_label_names)\n",
    "        # because its order matches the columns of binary_labels after prepare_multilabel\n",
    "        label_names = [all_possible_label_names[i] for i in active_label_indices]\n",
    "\n",
    "        # Create the final filtered binary label vectors, keeping only the active columns\n",
    "        binary_labels = binary_labels_np[:, active_label_indices].tolist() # Convert back to list of lists\n",
    "\n",
    "        print(f\"Number of unique labels left after filtering: {len(binary_labels[0])}\")\n",
    "\n",
    "    else:\n",
    "        glycan_sequences = glycans\n",
    "        binary_labels = labels\n",
    "        print(f\"Number of unique glycans: {len(glycan_sequences)}\")\n",
    "\n",
    "    return glycan_sequences, binary_labels, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd78302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 unique individual classes/labels.\n",
      "Number of unique glycans left after filtering rare classes (size >= 6): 1458/1648\n",
      "Number of unique labels left after filtering: 18\n"
     ]
    }
   ],
   "source": [
    "glycans, labels, label_names = build_multilabel_dataset(glycan_dataset='df_disease', glycan_class='disease_association', min_class_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b77c4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Crohn_disease', 'LPS_induced_inflammation', 'Sjogrens_syndrome', 'benign_breast_tumor_tissues_vs_para_carcinoma_tissues', 'colon_cancer', 'colorectal_cancer', 'female_breast_cancer', 'glioblastoma', 'liver_cancer', 'lung_cancer', 'lung_small_cell_carcinoma', 'pancreatic_cancer', 'thymic_carcinoma', 'thyroid_gland_papillary_carcinoma', 'type_1_diabetes_mellitus', 'type_2_diabetes_mellitus', 'urinary_bladder_cancer']\n"
     ]
    }
   ],
   "source": [
    "print (label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c440705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique glycans: 1458\n",
      "Number of label vectors: 1458\n",
      "Shape of first label vector (number of members in class): 60\n",
      "\n",
      "First 5 glycans:\n",
      "['Neu5Ac(a2-?)Gal(b1-4)GlcNAc(b1-2)Man(a1-3)[Gal(b1-4)GlcNAc(b1-2)Man(a1-6)][GlcNAc(b1-4)]Man(b1-4)GlcNAc(b1-4)GlcNAc', 'Neu5Ac(a2-?)Gal(b1-4)GlcNAc(b1-2)[Neu5Ac(a2-?)Gal(b1-4)GlcNAc(b1-4)]Man(a1-3)[Neu5Ac(a2-?)Gal(b1-4)[Fuc(a1-3)]GlcNAc(b1-2)Man(a1-6)]Man(b1-4)GlcNAc(b1-4)[Fuc(a1-6)]GlcNAc', 'Fuc(a1-2)[GalNAc(a1-3)]Gal(b1-4)GlcNAc(b1-3)Gal(b1-4)GlcNAc(b1-6)[Gal(b1-3)]GalNAc', 'Fuc(a1-2)Gal(b1-?)GlcNAc(b1-6)[Fuc(a1-?)[Gal(b1-?)]GlcNAc(b1-3)]Gal(b1-3)[GlcNAc(b1-6)]GalNAc', 'Fuc(a1-2)Gal(b1-4)[Fuc(a1-3)]GlcNAc(b1-3)Gal(b1-3)GalNAc']\n",
      "\n",
      "First 5 label vectors:\n",
      "[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "# quick inspection of the data\n",
    "\n",
    "print(f\"Number of unique glycans: {len(glycans)}\")\n",
    "print(f\"Number of label vectors: {len(labels)}\")\n",
    "print(f\"Shape of first label vector (number of members in class): {len(labels[0])}\")\n",
    "print(f\"\\nFirst 5 glycans:\\n{glycans[:5]}\")\n",
    "print(f\"\\nFirst 5 label vectors:\\n{labels[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91ff9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and testing sets using StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import List, Union\n",
    "\n",
    "def multilabel_split(glycans: List[str], # list of IUPAC-condensed glycans\n",
    "                 labels: List[Union[float, int, str]], # list of prediction labels\n",
    "                 train_size: float = 0.7, # size of train set, the rest is split into validation and test sets\n",
    "                 random_state: int = 42 # random state for reproducibility\n",
    "                )-> Tuple[List[str], List[str], List[str], List[List[float]], List[List[float]], List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets using StratifiedShuffleSplit.\n",
    "    \n",
    "    Args:\n",
    "        glycans: List of glycans.\n",
    "        labels: List of label vectors.\n",
    "        train_size: Proportion of the dataset to include in the validation and test split.\n",
    "        random_state: Controls the randomness of the split.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - train_glycans: Training set of glycans.\n",
    "            - val_glycans: Validation set of glycans.\n",
    "            - test_glycans: Testing set of glycans.\n",
    "            - train_labels: Training set of labels.\n",
    "            - val_labels: Validation set of labels.\n",
    "            - test_labels: Testing set of labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert labels to a suitable format for stratification (string representation)\n",
    "    label_strings = [''.join(map(str, label)) for label in labels]\n",
    "\n",
    "    # Initial split for train vs. (val + test)\n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = 1 - train_size, random_state = random_state)\n",
    "    train_index, temp_index = next(sss.split(glycans, label_strings))\n",
    "    train_glycans = [glycans[i] for i in train_index]\n",
    "    train_labels = [labels[i] for i in train_index]\n",
    "    temp_glycans = [glycans[i] for i in temp_index]\n",
    "    temp_labels = [labels[i] for i in temp_index]\n",
    "\n",
    "    # Split the remaining (val + test) into validation and test sets\n",
    "    sss_val_test = StratifiedShuffleSplit(n_splits = 1, test_size = 0.5, random_state = random_state)\n",
    "    val_index, test_index = next(sss_val_test.split(temp_glycans, [''.join(map(str, label)) for label in temp_labels]))\n",
    "    val_glycans = [temp_glycans[i] for i in val_index]\n",
    "    val_labels = [temp_labels[i] for i in val_index]\n",
    "    test_glycans = [temp_glycans[i] for i in test_index]\n",
    "    test_labels = [temp_labels[i] for i in test_index]\n",
    "\n",
    "    print(\"Split complete!\")\n",
    "    print(f\"Train set size: {len(train_glycans)}\")\n",
    "    print(f\"Validation set size: {len(val_glycans)}\")\n",
    "    print(f\"Test set size: {len(test_glycans)}\")\n",
    "        \n",
    "    return train_glycans, val_glycans, test_glycans, train_labels, val_labels, test_labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4723022a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete!\n",
      "Train set size: 1020\n",
      "Validation set size: 219\n",
      "Test set size: 219\n"
     ]
    }
   ],
   "source": [
    "# Lets use the split function\n",
    "train_glycans, val_glycans, test_glycans, train_labels, val_labels, test_labels = multilabel_split(glycans, labels, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "068f4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms IUPAC into graphs and makes the data loaders for the training and validation sets\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "\n",
    "glycan_loaders = split_data_to_train(\n",
    "    glycan_list_train = train_glycans,\n",
    "    glycan_list_val = val_glycans,\n",
    "    labels_train = train_labels,\n",
    "    labels_val = val_labels,\n",
    "    batch_size = 128,  # 32 or 128 seem to work well on this system\n",
    "    drop_last = False,\n",
    "    augment_prob = 0.0,  # Adjust if you want augmentation for training\n",
    "    generalization_prob = 0.2  # Adjust if you want generalization for training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae2f77",
   "metadata": {},
   "source": [
    "### GIFFLAR Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a5ce2",
   "metadata": {},
   "source": [
    "#### Load GIFFLAR Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the GIFFLAR dataset for the Taxonomy Kingdom (takes a long time to run(~40m), just use the file it generated)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../GIFFLAR') \n",
    "\n",
    "from gifflar.benchmarks import get_dataset\n",
    "import pathlib\n",
    "\n",
    "data_config_kingdom = {\"name\": \"Taxonomy_Kingdom\"}\n",
    "root_dir = pathlib.Path(\"./data_gifflar\")  # Choose a directory to save the data\n",
    "root_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "kingdom_dataset_config = get_dataset(data_config_kingdom, root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c4b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Taxonomy_Kingdom', 'filepath': WindowsPath('data_gifflar/taxonomy_Kingdom.tsv')}\n"
     ]
    }
   ],
   "source": [
    "# Print the dataset configuration\n",
    "print(kingdom_dataset_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a488a",
   "metadata": {},
   "source": [
    "#### Load and transform GIFFLAR Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "339c831f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               IUPAC  Amoebozoa  Animalia  \\\n",
      "0  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "1  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "2  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "3  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "4  3,6-Anhydro-L-Gal(a1-3)GalOMe(b1-4)3,6-Anhydro...          0         0   \n",
      "\n",
      "   Bacteria  Bamfordvirae  Chromista  Euryarchaeota  Excavata  Fungi  \\\n",
      "0         1             0          0              0         0      0   \n",
      "1         1             0          0              0         0      0   \n",
      "2         1             0          0              0         0      0   \n",
      "3         1             0          0              0         0      0   \n",
      "4         1             0          0              0         0      0   \n",
      "\n",
      "   Heunggongvirae  Metazoa  Orthornavirae  Pararnavirae  Plantae  Protista  \\\n",
      "0               0        0              0             0        0         0   \n",
      "1               0        0              0             0        0         0   \n",
      "2               0        0              0             0        0         0   \n",
      "3               0        0              0             0        0         0   \n",
      "4               0        0              0             0        0         0   \n",
      "\n",
      "   Riboviria  split  \n",
      "0          0  train  \n",
      "1          0  train  \n",
      "2          0  train  \n",
      "3          0    val  \n",
      "4          0   test  \n",
      "Shape of the DataFrame: (16452, 17)\n"
     ]
    }
   ],
   "source": [
    "# Lets load the dataset into a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "filepath = 'data_gifflar/taxonomy_Kingdom.tsv'\n",
    "#filepath = kingdom_dataset_config['filepath'] #If you've loaded it recently, which you shouldn't\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "multilabel_kingdom_df = pd.read_csv(filepath, sep=\"\\t\")\n",
    "# Display the first few rows of the DataFrame\n",
    "print(multilabel_kingdom_df.head())\n",
    "# Display the shape of the DataFrame\n",
    "print(f\"Shape of the DataFrame: {multilabel_kingdom_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b5a99",
   "metadata": {},
   "source": [
    "That looks fine\n",
    "\n",
    "Let's convert them into graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f975f366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['IUPAC', 'Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae',\n",
      "       'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae',\n",
      "       'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista',\n",
      "       'Riboviria', 'split'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(multilabel_kingdom_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f37ce3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's try the prepare multilabel function from the train_test_split module\n",
    "\n",
    "from glycowork.ml.train_test_split import prepare_multilabel\n",
    "\n",
    "# Prepare the multilabel dataset using the melt function\n",
    "# The melt function is used to transform the DataFrame from wide format to long format\n",
    "kingdom_df_melted = multilabel_kingdom_df.melt(\n",
    "    id_vars=['IUPAC', 'split'],\n",
    "    value_vars=['Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae', 'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae', 'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista', 'Riboviria'],\n",
    "    var_name='Kingdom',\n",
    "    value_name='Association'\n",
    ")\n",
    "\n",
    "# Filter for associations where the glycan belongs to the kingdom (Association == 1)\n",
    "kingdom_df_melted = kingdom_df_melted[kingdom_df_melted['Association'] == 1]\n",
    "\n",
    "# Splitting the dataset using the 'split' column\n",
    "train_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'train'].drop(columns=['split'])\n",
    "val_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'val'].drop(columns=['split'])\n",
    "test_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'test'].drop(columns=['split'])\n",
    "\n",
    "# Finally using the prepare_multilabel function to prepare the data for training\n",
    "glycan_train, label_train = prepare_multilabel(train_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "glycan_val, label_val = prepare_multilabel(val_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "glycan_test, label_test = prepare_multilabel(test_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b68ad291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make them into graphs again, but hyper efficiently this time\n",
    "\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "\n",
    "multilabel_kingdom_loaders = split_data_to_train(\n",
    "    glycan_list_train=glycan_train,\n",
    "    glycan_list_val=glycan_val,\n",
    "    labels_train=label_train,\n",
    "    labels_val=label_val,\n",
    "    batch_size=32,  # Adjust as needed\n",
    "    drop_last=False,\n",
    "    augment_prob=0.0,  # Adjust if you want augmentation for training\n",
    "    generalization_prob=0.0  # Adjust if you want generalization for training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26e9af",
   "metadata": {},
   "source": [
    "#### Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2104e79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training glycans: 11593\n",
      "Number of validation glycans: 3213\n",
      "Number of test glycans: 1646\n",
      "Shape of training labels: 11593 x 15\n",
      "Shape of validation labels: 3213 x 15\n",
      "Shape of test labels: 1646 x 15\n",
      "--- Checking example from train set (Direct) ---\n",
      "Glycan: 3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-Gal(a1-3)Gal\n",
      "Split in original data: train\n",
      "Labels in split data: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Labels in original data: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Labels match!\n",
      "--- Checking example from val set (Direct) ---\n",
      "Glycan: 3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-Gal(a1-3)GalOMe(b1-4)3,6-Anhydro-L-Gal(a1-3)Gal\n",
      "Split in original data: val\n",
      "Labels in split data: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Labels in original data: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Labels match!\n",
      "--- Checking example from test set (Direct) ---\n",
      "Glycan: 3,6-Anhydro-L-Gal(a1-3)GalOMe(b1-4)3,6-Anhydro-L-Gal(a1-3)Gal\n",
      "Split in original data: test\n",
      "Labels in split data: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Labels in original data: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Labels match!\n"
     ]
    }
   ],
   "source": [
    "# Let's validate the split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "filepath = 'data_gifflar/taxonomy_Kingdom.tsv'\n",
    "\n",
    "# 1. Load the original DataFrame\n",
    "multilabel_kingdom_df_original = pd.read_csv(filepath, sep=\"\\t\")\n",
    "kingdom_cols = ['Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae', 'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae', 'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista', 'Riboviria']\n",
    "\n",
    "# 2. Split the original DataFrame by 'split'\n",
    "train_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'train']\n",
    "val_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'val']\n",
    "test_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'test']\n",
    "\n",
    "# 3. Extract glycans and labels directly\n",
    "glycan_train_list = train_df['IUPAC'].tolist()\n",
    "label_train_list = [train_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_train_list = list(zip(*label_train_list)) # Transpose\n",
    "\n",
    "glycan_val_list = val_df['IUPAC'].tolist()\n",
    "label_val_list = [val_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_val_list = list(zip(*label_val_list)) # Transpose\n",
    "\n",
    "glycan_test_list = test_df['IUPAC'].tolist()\n",
    "label_test_list = [test_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_test_list = list(zip(*label_test_list)) # Transpose\n",
    "\n",
    "print(f\"Number of training glycans: {len(glycan_train_list)}\")\n",
    "print(f\"Number of validation glycans: {len(glycan_val_list)}\")\n",
    "print(f\"Number of test glycans: {len(glycan_test_list)}\")\n",
    "print(f\"Shape of training labels: {len(label_train_list)} x {len(label_train_list[0]) if label_train_list else 0}\")\n",
    "print(f\"Shape of validation labels: {len(label_val_list)} x {len(label_val_list[0]) if label_val_list else 0}\")\n",
    "print(f\"Shape of test labels: {len(label_test_list)} x {len(label_test_list[0]) if label_test_list else 0}\")\n",
    "\n",
    "# Now re-run the checking function (modified for this new label extraction)\n",
    "def check_example_direct(glycan_list, label_list, split_name, original_df, kingdom_cols):\n",
    "    if glycan_list:\n",
    "        example_index = 0\n",
    "        example_glycan = glycan_list[example_index]\n",
    "        example_labels_split = list(label_list[example_index])\n",
    "\n",
    "        original_row = original_df[original_df['IUPAC'] == example_glycan].iloc[0]\n",
    "        labels_original = np.array([original_row[col] for col in kingdom_cols], dtype=np.float32).tolist()\n",
    "\n",
    "        print(f\"--- Checking example from {split_name} set (Direct) ---\")\n",
    "        print(f\"Glycan: {example_glycan}\")\n",
    "        print(f\"Split in original data: {original_row['split']}\")\n",
    "        print(f\"Labels in split data: {example_labels_split}\")\n",
    "        print(f\"Labels in original data: {labels_original}\")\n",
    "        if labels_original == example_labels_split:\n",
    "            print(\"Labels match!\")\n",
    "        else:\n",
    "            print(\"Labels DO NOT match!\")\n",
    "    else:\n",
    "        print(f\"{split_name} set is empty.\")\n",
    "\n",
    "check_example_direct(glycan_train_list, label_train_list, 'train', multilabel_kingdom_df_original, kingdom_cols)\n",
    "check_example_direct(glycan_val_list, label_val_list, 'val', multilabel_kingdom_df_original, kingdom_cols)\n",
    "check_example_direct(glycan_test_list, label_test_list, 'test', multilabel_kingdom_df_original, kingdom_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0f833",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multilabel_kingdom_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmultilabel_kingdom_loader\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'multilabel_kingdom_loader' is not defined"
     ]
    }
   ],
   "source": [
    "print(multilabel_kingdom_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e0b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: DataBatch(edge_index=[2, 430], labels=[462], string_labels=[32], y=[480], num_nodes=462, x=[462, 320], batch=[462], ptr=[33])\n",
      "Number of graphs in batch: 32\n",
      "\n",
      "First graph data: Data(edge_index=[2, 8], labels=[9], string_labels=[9], y=[15], x=[9, 320], num_nodes=9)\n",
      "Node features (x): tensor([[ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        ...,\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676]])\n",
      "Edge indices (edge_index): tensor([[1, 2, 3, 4, 5, 6, 7, 8],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7]])\n",
      "Labels (y): tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "String labels: ['Rha', 'a1-3', 'Rha', 'a1-4', 'GalNAcA3Ac', 'a1-3', 'QuiNAc', 'b1-2', 'Rha']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check one of the graphs in the dataloader\n",
    "\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataLoader is called 'multilabel_kingdom_loader'\n",
    "inspected_loader = multilabel_kingdom_loaders_emb['train'] # Access the train DataLoader\n",
    "\n",
    "# Get the first batch of data\n",
    "try:\n",
    "    batch = next(iter(inspected_loader))\n",
    "    print(\"Batch:\", batch)\n",
    "    print(\"Number of graphs in batch:\", batch.num_graphs)\n",
    "\n",
    "    # Extract the first graph from the batch\n",
    "    first_graph_data = batch[2] #change to check other graphs\n",
    "    print(\"\\nFirst graph data:\", first_graph_data)\n",
    "    print(\"Node features (x):\", first_graph_data.x)\n",
    "    print(\"Edge indices (edge_index):\", first_graph_data.edge_index)\n",
    "    print(\"Labels (y):\", first_graph_data.y)\n",
    "    print(\"String labels:\", first_graph_data.string_labels)\n",
    "\n",
    "    # Convert the PyG Data object to a NetworkX graph for visualization\n",
    "    nx_graph = to_networkx(first_graph_data) \n",
    "\n",
    "    # Visualize the NetworkX graph\n",
    "    nx_graph = to_networkx(first_graph_data)\n",
    "    pos = nx.spring_layout(nx_graph)\n",
    "    nx.draw(nx_graph, pos, with_labels=True, node_size=500, node_color=\"red\", font_size=10, font_weight=\"bold\", arrows=False)\n",
    "    plt.title(\"First Graph from DataLoader (PyG)\")\n",
    "    plt.show()\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"The DataLoader is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87dc8a6",
   "metadata": {},
   "source": [
    "## Let's make a function to add embeddings to a dataloader object ||||run on kernel restart||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da9d11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: glm_embeddings_1.pkl\n",
      "Embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Let's load the embeddings again here, so I don't have to jump up and down whenever I reload the kernel)\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            glm_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72ad776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add GLM embeddings to a dictionary of dataloaders before loading them into the model\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def add_glm_embeddings_to_dataloaders(dataloaders, glm_embeddings):\n",
    "    embedded_loaders = {}\n",
    "    embedding_dim = 320\n",
    "    for split, loader in dataloaders.items():\n",
    "        embedded_data_list = []\n",
    "        for batch in loader:\n",
    "            for graph in batch.to_data_list():\n",
    "                node_embeddings = []\n",
    "                if hasattr(graph, 'string_labels'):\n",
    "                    for label in graph.string_labels:\n",
    "                        if label in glm_embeddings:\n",
    "                            embedding = glm_embeddings[label]\n",
    "                            node_embeddings.append(torch.tensor(embedding))\n",
    "                        else:\n",
    "                            node_embeddings.append(torch.zeros(embedding_dim))\n",
    "                    graph.x = torch.stack(node_embeddings).float()\n",
    "                    #print(f\"Shape of graph.x after adding embeddings: {graph.x.shape}\")\n",
    "                embedded_data_list.append(graph)\n",
    "\n",
    "        embedded_loaders[split] = DataLoader(embedded_data_list, batch_size=32)\n",
    "        print(f\"GLM embeddings added to {split} DataLoader.\")\n",
    "    return embedded_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b83ae5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multilabel_kingdom_loaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# testing the function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m multilabel_kingdom_loaders_emb = add_glm_embeddings_to_dataloaders(\u001b[43mmultilabel_kingdom_loaders\u001b[49m, glm_embeddings)\n",
      "\u001b[31mNameError\u001b[39m: name 'multilabel_kingdom_loaders' is not defined"
     ]
    }
   ],
   "source": [
    "# testing the function\n",
    "\n",
    "multilabel_kingdom_loaders_emb = add_glm_embeddings_to_dataloaders(multilabel_kingdom_loaders, glm_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef713ec2",
   "metadata": {},
   "source": [
    "## Lets look at the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "83e13ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample string_labels from the first graph:\n",
      "['6dTal', 'a1-2', 'Rhaf', 'b1-5', 'Sug']\n"
     ]
    }
   ],
   "source": [
    "# Looking at string labels\n",
    "\n",
    "train_loader = multilabel_kingdom_loaders_emb['train'] \n",
    "batch = next(iter(train_loader))\n",
    "first_graph_labels = batch[2].string_labels\n",
    "print(\"Sample string_labels from the first graph:\")\n",
    "print(first_graph_labels[:20])  # Print the first 20 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e6368a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embeddings from the first graph:\n",
      "tensor([[ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676]])\n"
     ]
    }
   ],
   "source": [
    "# looking at the embeddings themselves\n",
    "\n",
    "train_loader = multilabel_kingdom_loaders_emb['train'] \n",
    "batch = next(iter(train_loader))\n",
    "first_graph_embeddings = batch[2].x\n",
    "print(\"Sample embeddings from the first graph:\")\n",
    "print(first_graph_embeddings[:20])  # Print the first 20 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef4130ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample keys from glm_embeddings:\n",
      "['!GlcNAc', '-10', '-12', '-2', '-4', '-6', '-8', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic', '1,5-Anhydro-Man-ol', '1,5-Anhydro-ManNAc-ol', '1,5-Anhydro-Xyl-ol', '1,5-Anhydro-Xyl2F-ol', '1-1', '1-2', '1-3', '1-4', '1-5', '1-6', '1-?', '1b-4', '1dAlt-ol', '1dEry-ol', '2,3-Anhydro-All', '2,3-Anhydro-Man', '2,3-Anhydro-Rib', '2,5-Anhydro-D-Alt-ol', '2,5-Anhydro-D-Alt3S-ol', '2,5-Anhydro-D-Tal', '2,5-Anhydro-Glc', '2,5-Anhydro-L-Man-ol', '2,5-Anhydro-Man', '2,5-Anhydro-Man-ol', '2,5-Anhydro-Man1S-ol', '2,5-Anhydro-Man3S-ol', '2,5-Anhydro-Man6S', '2,5-Anhydro-Tal-ol', '2,5-Anhydro-Tal6P', '2,6-Anhydro-Glc5NAc-ol', '2,6-Anhydro-L-Gul-ol', '2,6-Anhydro-L-Gul-onic', '2,6-Anhydro-Man-ol', '2,6-Anhydro-Tal5NAc-ol', '2,7-Anhydro-Kdo', '2,7-Anhydro-Kdof', '2-3', '2-4', '2-5', '2-6', '2dAraHexA', '3,6-Anhydro-Fruf', '3,6-Anhydro-Gal', '3,6-Anhydro-Gal2S', '3,6-Anhydro-Glc', '3,6-Anhydro-L-Gal', '3,6-Anhydro-L-Gal2Me', '3-1', '3-5', '3-Anhydro-Gal', '3-Anhydro-Gal2S', '3dFuc', '3dGal', '3dLyxHep-ulosaric', '4,7-Anhydro-Kdo', '4,7-Anhydro-KdoOPEtN', '4,8-Anhydro-Kdo', '4-1', '4-5', '4d8dNeu5Ac', '4dAraHex', '4dEry-ol', '4dFuc', '4dGal', '4dNeu5Ac', '4dThrHexNAcA4en', '4eLeg5Ac7Ac', '5-1', '5-2', '5-3', '5-4', '5-5', '5-6', '5dAraf', '5dAraf3Me', '5dLyxf3CFo', '5dLyxf3CMe', '5dPenf3CFo', '6-1', '6-3', '6-4', '6dAll', '6dAll3Me', '6dAlt', '6dAltNAc', '6dAltNAc1PP4N', '6dAltNAc1PP4NAc', '6dAltNAc3PCho', '6dAltOAc', '6dAltf', '6dAltfOAc', '6dFruf', '6dGal', '6dGalNAc', '6dGul', '6dHex', '6dHexN', '6dHexNAc4NAc', '6dManHep', '6dTal', '6dTal1PP', '6dTal2Ac', '6dTal2Ac3Ac', '6dTal2Ac3Ac4Ac', '6dTal2Ac3Me', '6dTal2Ac3Me4Ac', '6dTal2Ac4Ac', '6dTal2Me', '6dTal2Me4Ac', '6dTal3Me', '6dTal4Ac', '6dTalNAc', '6dTalNAc1PP', '6dTalNAc4Ac', '6dTalNAcOAc', '6dTalOAc', '6dTalOAcOAc', '6dTalOAcOMe', '6dTalOMe', '6dTalOMe-ol', '6dTalf', '7dNeu5Ac', '8dNeu5Ac', '8eAci5Ac7Ac', '8eLeg', '8eLeg5Ac7Ac', '8eLeg5Ac7Ac8Ac', '8eLeg5Ac7AcGro', '8eLeg5But7Ac', '8eLegNAcNBut', '8ePse5Ac7Ac', '9dNeu5Ac', '?1-2', '?1-3', '?1-4', '?1-6', '?1-?', '?2-?', 'Abe', 'Abe1PP', 'Abe2Ac', 'AbeOAc', 'Acarbose', 'AcefA', 'Aci5Ac7Ac', 'AcoNAc', 'All', 'All-ol', 'All1S2S3S4S', 'All2Ac3Ac', 'All2S3S4S', 'All3Ac', 'All6Ac', 'AllN', 'AllN1P', 'AllNAc', 'AllNAc6Me', 'AllOMe', 'Alt', 'AltA', 'AltA2N', 'AltA2S', 'AltAN', 'AltNAc', 'AltNAcA', 'AltNAcA1Prop', 'Altf', 'AltfOAc', 'Amikacin', 'Api', 'ApiOAc', 'ApiOMe-ol', 'Apif', 'Ara', 'Ara-ol', 'Ara1Cer2Ac', 'Ara1Me', 'Ara1N4P', 'Ara1P4N', 'Ara1PP']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample keys from glm_embeddings:\")\n",
    "print(list(glm_embeddings.keys())[:200])  # Print the first 20 keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b56aa7",
   "metadata": {},
   "source": [
    "## Copies of glycowork functions for experimentation ||||run on kernel restart||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5658e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SweetNet class\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Union, Literal\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.nn import GraphConv\n",
    "    from torch_geometric.nn import global_mean_pool as gap\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "  raise ImportError(\"<torch or torch_geometric missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from glycowork.glycan_data.loader import lib, download_model \n",
    "\n",
    "class SweetNet(torch.nn.Module):\n",
    "    def __init__(self, lib_size: int, # number of unique tokens for graph nodes\n",
    "                 num_classes: int = 1, # number of output classes (>1 for multilabel)\n",
    "                 hidden_dim: int = 320, # dimension of hidden layers (changed from 128 to 320)\n",
    "                 use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "                ) -> None:\n",
    "        \"given glycan graphs as input, predicts properties via a graph neural network\"\n",
    "        #print(\"Using SweetNet from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "        super(SweetNet, self).__init__()\n",
    "        self.use_external_embeddings = use_external_embeddings\n",
    "        # Convolution operations on the graph\n",
    "        self.conv1 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GraphConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Node embedding\n",
    "        if use_external_embeddings:\n",
    "            self.embedding_dim = hidden_dim\n",
    "        else:\n",
    "            self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    " \n",
    "\n",
    "        #self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    "        # Fully connected part\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, 1024)\n",
    "        self.lin2 = torch.nn.Linear(1024, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, num_classes)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(1024)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.act1 = torch.nn.LeakyReLU()\n",
    "        self.act2 = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor,\n",
    "                inference: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \n",
    "        # Getting node features\n",
    "        if self.use_external_embeddings:\n",
    "           # Use external embeddings (already in x)\n",
    "            pass # x is already the embeddings\n",
    "        else:\n",
    "             # Use internal embedding\n",
    "            x = self.item_embedding(x).squeeze(1)\n",
    "            \n",
    "        \n",
    "        # Graph convolution operations (now using GLM embeddings as x)\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = gap(x, batch)\n",
    "\n",
    "        # Fully connected part\n",
    "        x = self.act1(self.bn1(self.lin1(x)))\n",
    "        x_out = self.bn2(self.lin2(x))\n",
    "        x = F.dropout(self.act2(x_out), p = 0.5, training = self.training)\n",
    "\n",
    "        x = self.lin3(x).squeeze(1)\n",
    "\n",
    "        if inference:\n",
    "          return x, x_out\n",
    "        else:\n",
    "          return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03900750",
   "metadata": {},
   "source": [
    "### Other functions I might modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6eccb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model function\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import torch\n",
    "    # Choose the correct computing architecture\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "    raise ImportError(\"<torch missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, mean_squared_error, \\\n",
    "    label_ranking_average_precision_score, ndcg_score, roc_auc_score, mean_absolute_error, r2_score\n",
    "from glycowork.motif.annotate import annotate_dataset\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 7, # epochs to wait after last improvement\n",
    "                 verbose: bool = False # whether to print messages\n",
    "                ) -> None:\n",
    "        \"Early stops the training if validation loss doesn't improve after a given patience\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = 0\n",
    "\n",
    "    def __call__(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), 'drive/My Drive/checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def sigmoid(x: float # input value\n",
    "          ) -> float: # sigmoid transformed value\n",
    "    \"Apply sigmoid transformation to input\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def disable_running_stats(model: torch.nn.Module # model to disable batch norm\n",
    "                       ) -> None:\n",
    "    \"Disable batch normalization running statistics\"\n",
    "\n",
    "    def _disable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "\n",
    "def enable_running_stats(model: torch.nn.Module # model to enable batch norm\n",
    "                      ) -> None:\n",
    "    \"Enable batch normalization running statistics\"\n",
    "\n",
    "    def _enable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def train_model(model: torch.nn.Module, # graph neural network for analyzing glycans\n",
    "               dataloaders: Dict[str, torch.utils.data.DataLoader], # dict with 'train' and 'val' loaders\n",
    "               criterion: torch.nn.Module, # PyTorch loss function\n",
    "               optimizer: torch.optim.Optimizer, # PyTorch optimizer, has to be SAM if mode != \"regression\"\n",
    "               scheduler: torch.optim.lr_scheduler._LRScheduler, # PyTorch learning rate decay\n",
    "               num_epochs: int = 25, # number of epochs for training\n",
    "               patience: int = 50, # epochs without improvement until early stop\n",
    "               mode: str = 'classification', # 'classification', 'multilabel', or 'regression'\n",
    "               mode2: str = 'multi', # 'multi' or 'binary' classification\n",
    "               return_metrics: bool = False, # whether to return metrics\n",
    "               use_external_embeddings: bool = False # whether to use external embeddings\n",
    "              ) -> Union[torch.nn.Module, tuple[torch.nn.Module, dict[str, dict[str, list[float]]]]]: # best model from training and the training and validation metrics\n",
    "    \"trains a deep learning model on predicting glycan properties\"\n",
    "\n",
    "    since = time.time()\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "    best_lead_metric = float(\"inf\")\n",
    "\n",
    "    if mode == 'classification':\n",
    "         # Removed auroc from metrics to avoid calculation issues for now\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": []}\n",
    "    elif mode == 'multilabel':\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": [], \"lrap\": [], \"ndcg\": []}\n",
    "    else:\n",
    "        blank_metrics = {\"loss\": [], \"mse\": [], \"mae\": [], \"r2\": []}\n",
    "\n",
    "    metrics = {\"train\": copy.deepcopy(blank_metrics), \"val\": copy.deepcopy(blank_metrics)}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_metrics = copy.deepcopy(blank_metrics)\n",
    "            running_metrics[\"weights\"] = []\n",
    "\n",
    "            for data in dataloaders[phase]:\n",
    "                # Get all relevant node attributes\n",
    "                print(f\"Phase: {phase}, Data: {data}\")\n",
    "                print(f\"Phase: {phase}, Data.x: {getattr(data, 'x', None)}\") # Check if x exists and its value\n",
    "                if use_external_embeddings:\n",
    "                    x, y, edge_index, batch = data.x, data.y, data.edge_index, data.batch\n",
    "                else:\n",
    "                    x, y, edge_index, batch = data.labels, data.y, data.edge_index, data.batch\n",
    "                prot = getattr(data, 'train_idx', None)\n",
    "                if prot is not None:\n",
    "                    prot = prot.view(max(batch) + 1, -1).to(device)\n",
    "                x = x.to(device)\n",
    "                if mode == 'multilabel':\n",
    "                    y = y.view(max(batch) + 1, -1).to(device)\n",
    "                else:\n",
    "                    y = y.to(device)\n",
    "                y = y.view(-1, 1) if mode == 'regression' else y\n",
    "                edge_index = edge_index.to(device)\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # First forward pass\n",
    "                    if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                        enable_running_stats(model)\n",
    "                    pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                    loss = criterion(pred, y)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                            optimizer.first_step(zero_grad = True)\n",
    "                            # Second forward pass\n",
    "                            disable_running_stats(model)\n",
    "                            second_pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                            criterion(second_pred, y).backward()\n",
    "                            optimizer.second_step(zero_grad = True)\n",
    "                        else:\n",
    "                            optimizer.step()\n",
    "                \n",
    "                # Check for single-class batches\n",
    "                unique_classes = torch.unique(y).cpu().numpy()\n",
    "                if len(unique_classes) == 1:\n",
    "                    print(f\"WARNING: Single-class batch detected in {phase} phase at epoch {epoch}!\")\n",
    "                    print(f\"Batch labels: {unique_classes}\")\n",
    "\n",
    "                # Collecting relevant metrics\n",
    "                running_metrics[\"loss\"].append(loss.item())\n",
    "                running_metrics[\"weights\"].append(batch.max().cpu() + 1)\n",
    "\n",
    "                y_det = y.detach().cpu().numpy()\n",
    "                pred_det = pred.cpu().detach().numpy()\n",
    "                if mode == 'classification':\n",
    "                    if mode2 == 'multi':\n",
    "                        pred_proba = np.exp(pred_det) / np.sum(np.exp(pred_det), axis = 1, keepdims = True)  # numpy softmax\n",
    "                        pred2 = np.argmax(pred_det, axis = 1)\n",
    "                    else:\n",
    "                        pred_proba = sigmoid(pred_det)\n",
    "                        pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det, pred2))\n",
    "                    # commented out auroc because it was throwing errors\n",
    "                    #running_metrics[\"auroc\"].append(roc_auc_score(y_det.astype(int), pred_proba if mode2 == 'binary' else pred_proba[:, 1]))\n",
    "                elif mode == 'multilabel':\n",
    "                    pred_proba = sigmoid(pred_det)\n",
    "                    pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det.flatten(), pred2.flatten()))\n",
    "                    running_metrics[\"lrap\"].append(label_ranking_average_precision_score(y_det.astype(int), pred_proba))\n",
    "                    running_metrics[\"ndcg\"].append(ndcg_score(y_det.astype(int), pred_proba))\n",
    "                else:\n",
    "                    running_metrics[\"mse\"].append(mean_squared_error(y_det, pred_det))\n",
    "                    running_metrics[\"mae\"].append(mean_absolute_error(y_det, pred_det))\n",
    "                    running_metrics[\"r2\"].append(r2_score(y_det, pred_det))\n",
    "\n",
    "            # Averaging metrics at end of epoch\n",
    "            for key in running_metrics:\n",
    "                if key == \"weights\":\n",
    "                    continue\n",
    "                metrics[phase][key].append(np.average(running_metrics[key], weights = running_metrics[\"weights\"]))\n",
    "\n",
    "            if mode == 'classification':\n",
    "                print('{} Loss: {:.4f} Accuracy: {:.4f} MCC: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            elif mode == 'multilabel':\n",
    "                print('{} Loss: {:.4f} LRAP: {:.4f} NDCG: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            else:\n",
    "                print('{} Loss: {:.4f} MSE: {:.4f} MAE: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"mse\"][-1], metrics[phase][\"mae\"][-1]))\n",
    "\n",
    "            # Keep best model state_dict\n",
    "            if phase == \"val\":\n",
    "                if metrics[phase][\"loss\"][-1] <= best_loss:\n",
    "                    best_loss = metrics[phase][\"loss\"][-1]\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                    # Extract the lead metric (ACC, LRAP, or MSE) of the new best model\n",
    "                    if mode == 'classification':\n",
    "                        best_lead_metric = metrics[phase][\"acc\"][-1]\n",
    "                    elif mode == 'multilabel':\n",
    "                        best_lead_metric = metrics[phase][\"lrap\"][-1]\n",
    "                    else:\n",
    "                        best_lead_metric = metrics[phase][\"mse\"][-1]\n",
    "\n",
    "                # Check Early Stopping & adjust learning rate if needed\n",
    "                early_stopping(metrics[phase][\"loss\"][-1], model)\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(metrics[phase][\"loss\"][-1])\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    if mode == 'classification':\n",
    "        print('Best val loss: {:4f}, best Accuracy score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    elif mode == 'multilabel':\n",
    "        print('Best val loss: {:4f}, best LRAP score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    else:\n",
    "        print('Best val loss: {:4f}, best MSE score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    if return_metrics:\n",
    "        return model, metrics\n",
    "\n",
    "    # Plot loss & score over the course of training\n",
    "    _, _ = plt.subplots(nrows=2, ncols=1)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(epoch + 1), metrics[\"val\"][\"loss\"])\n",
    "    plt.title('Model Training')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.legend(['Validation Loss'], loc='best')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    if mode == 'classification':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"acc\"])\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "        plt.legend(['Validation Accuracy'], loc='best')\n",
    "    elif mode == 'multilabel':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"lrap\"])\n",
    "        plt.ylabel('Validation LRAP')\n",
    "        plt.legend(['Validation LRAP'], loc='best')\n",
    "    else:\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"mse\"])\n",
    "        plt.ylabel('Validation MSE')\n",
    "        plt.legend(['Validation MSE'], loc='best')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57a375b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init_weights function\n",
    "\n",
    "def init_weights(model: torch.nn.Module, # neural network for analyzing glycans\n",
    "                mode: str = 'sparse', # initialization algorithm: 'sparse', 'kaiming', 'xavier'\n",
    "                sparsity: float = 0.1 # proportion of sparsity after initialization\n",
    "               ) -> None:\n",
    "    \"initializes linear layers of PyTorch model with a weight initialization\"\n",
    "    #print(\"Using init_weights from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if isinstance(model, torch.nn.Linear):\n",
    "        if mode == 'sparse':\n",
    "            torch.nn.init.sparse_(model.weight, sparsity = sparsity)\n",
    "        elif mode == 'kaiming':\n",
    "            torch.nn.init.kaiming_uniform_(model.weight)\n",
    "        elif mode == 'xavier':\n",
    "            torch.nn.init.xavier_uniform_(model.weight)\n",
    "        else:\n",
    "            print(\"This initialization option is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a059da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_model function\n",
    "\n",
    "def prep_model(model_type: Literal[\"SweetNet\", \"LectinOracle\", \"LectinOracle_flex\", \"NSequonPred\"], # type of model to create\n",
    "              num_classes: int, # number of unique classes for classification\n",
    "              libr: Optional[Dict[str, int]] = None, # dictionary of form glycoletter:index\n",
    "              trained: bool = False, # whether to use pretrained model\n",
    "              # set hidden_dim to 320 rather than 128 for the pretrained model\n",
    "              # but 128 is the default for the model in the paper\n",
    "              hidden_dim: int = 320, # hidden dimension for the model (SweetNet/LectinOracle only)\n",
    "              use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "             ) -> torch.nn.Module: # initialized PyTorch model\n",
    "    \"wrapper to instantiate model, initialize it, and put it on the GPU\"\n",
    "    #print(\"Using prep_model from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if libr is None:\n",
    "      libr = lib\n",
    "    if model_type == 'SweetNet':\n",
    "      model = SweetNet(len(libr), num_classes = num_classes, hidden_dim = hidden_dim, use_external_embeddings = use_external_embeddings)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'sparse'))\n",
    "      if trained:\n",
    "        if hidden_dim != 128:\n",
    "          raise ValueError(\"Hidden dimension must be 128 for pretrained model\")\n",
    "        model_path = download_model(\"glycowork_sweetnet_species.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle':\n",
    "      model = LectinOracle(len(libr), num_classes = num_classes, input_size_prot = int(10*hidden_dim))\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle_flex':\n",
    "      model = LectinOracle_flex(len(libr), num_classes = num_classes)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle_flex.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'NSequonPred':\n",
    "      model = NSequonPred()\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"NSequonPred_batch32.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    else:\n",
    "      print(\"Invalid Model Type\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbbca33",
   "metadata": {},
   "source": [
    "## Testing using same framework as iteration 0 (basic kingdom sweetnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# testing the modified SweetNet model on the GlycoWork dataset \n",
    "from glycowork.glycan_data.loader import df_species\n",
    "from glycowork.ml.train_test_split import hierarchy_filter\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "from glycowork.ml import model_training\n",
    "\n",
    "# silence the avalanche of \"undefined\" warnings\n",
    "#import warnings\n",
    "#from sklearn.exceptions import UndefinedMetricWarning\n",
    "#warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "train_x, val_x, train_y, val_y, id_val, class_list, class_converter = hierarchy_filter(df_species,\n",
    "                                                                                       rank = 'Kingdom')\n",
    "\n",
    "dataloaders = split_data_to_train(train_x, val_x, train_y, val_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets split out the training code so I don't have to load the data each time\n",
    "\n",
    "model = prep_model('SweetNet', len(class_list))\n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = len(class_list))\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'classification',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4382d5b4",
   "metadata": {},
   "source": [
    "### Extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdade799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the Kingdoms in the original data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "kingdom_counts = df_species['Kingdom'].value_counts()\n",
    "print(\"Kingdom distribution in the original data:\")\n",
    "print(kingdom_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f112562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Print the first few rows to get an overview\n",
    "print(df_species.head())\n",
    "\n",
    "# Print the data types of the columns\n",
    "print(df_species.info())\n",
    "\n",
    "# Check for duplicated glycans\n",
    "print(f\"Number of unique glycans: {df_species['glycan'].nunique()}\")\n",
    "print(f\"Total number of rows: {len(df_species)}\")\n",
    "\n",
    "# If you suspect a column contains multiple kingdoms:\n",
    "if 'Kingdom' in df_species.columns:  # Replace 'Kingdom' with the actual column name\n",
    "    # Check the first few values of that column\n",
    "    print(df_species['Kingdom'].head())\n",
    "    # If it's a string with delimiters, count the delimiters\n",
    "    if isinstance(df_species['Kingdom'][0], str) and ',' in df_species['Kingdom'][0]: #assuming ',' is the delimiter\n",
    "        print(f\"Example value: {df_species['Kingdom'][0]}\")\n",
    "        print(f\"Number of commas in the first value: {df_species['Kingdom'][0].count(',')}\")\n",
    "\n",
    "    # Check for multiple columns representing kingdoms\n",
    "    for col in df_species.columns:\n",
    "        if col in ['Animalia', 'Bacteria', 'Plantae']:  # Add all possible kingdom column names\n",
    "            print(f\"Column '{col}': Data type = {df_species[col].dtype}, Unique values = {df_species[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set()\n",
    "duplicates_count = 0\n",
    "\n",
    "for item in train_x:\n",
    "    if item in seen:\n",
    "        duplicates_count += 1\n",
    "    else:\n",
    "        seen.add(item)\n",
    "\n",
    "print(f\"Number of duplicates: {duplicates_count}\")\n",
    "print(f\"Number of unique items: {len(seen)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e20752",
   "metadata": {},
   "source": [
    "## Time to train a multi-class multi-label Sweetnet on our properly loaded data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b59cf29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 3.7156 LRAP: 0.0029 NDCG: 0.1865\n",
      "val Loss: 3.8115 LRAP: 0.0000 NDCG: 0.1584\n",
      "Validation loss decreased (0.000000 --> 3.811537).  Saving model ...\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 3.6113 LRAP: 0.0039 NDCG: 0.2871\n",
      "val Loss: 3.7843 LRAP: 0.0000 NDCG: 0.1722\n",
      "Validation loss decreased (3.811537 --> 3.784271).  Saving model ...\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 3.5082 LRAP: 0.0020 NDCG: 0.3097\n",
      "val Loss: 3.7401 LRAP: 0.0000 NDCG: 0.2228\n",
      "Validation loss decreased (3.784271 --> 3.740058).  Saving model ...\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 3.4643 LRAP: 0.0088 NDCG: 0.3387\n",
      "val Loss: 3.6607 LRAP: 0.0000 NDCG: 0.2863\n",
      "Validation loss decreased (3.740058 --> 3.660745).  Saving model ...\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 3.3407 LRAP: 0.0098 NDCG: 0.4065\n",
      "val Loss: 3.5756 LRAP: 0.0183 NDCG: 0.3901\n",
      "Validation loss decreased (3.660745 --> 3.575584).  Saving model ...\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 3.2455 LRAP: 0.0118 NDCG: 0.4205\n",
      "val Loss: 3.4510 LRAP: 0.0183 NDCG: 0.3925\n",
      "Validation loss decreased (3.575584 --> 3.450967).  Saving model ...\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 3.1265 LRAP: 0.0265 NDCG: 0.4651\n",
      "val Loss: 3.3499 LRAP: 0.0365 NDCG: 0.4785\n",
      "Validation loss decreased (3.450967 --> 3.349923).  Saving model ...\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 2.9546 LRAP: 0.0402 NDCG: 0.5085\n",
      "val Loss: 2.8955 LRAP: 0.0274 NDCG: 0.5070\n",
      "Validation loss decreased (3.349923 --> 2.895508).  Saving model ...\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 2.7894 LRAP: 0.0471 NDCG: 0.5242\n",
      "val Loss: 2.7321 LRAP: 0.0411 NDCG: 0.5372\n",
      "Validation loss decreased (2.895508 --> 2.732119).  Saving model ...\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 2.6087 LRAP: 0.0529 NDCG: 0.5352\n",
      "val Loss: 2.6733 LRAP: 0.0411 NDCG: 0.5325\n",
      "Validation loss decreased (2.732119 --> 2.673282).  Saving model ...\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 2.3750 LRAP: 0.0510 NDCG: 0.5483\n",
      "val Loss: 2.4466 LRAP: 0.0274 NDCG: 0.5528\n",
      "Validation loss decreased (2.673282 --> 2.446596).  Saving model ...\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 2.1080 LRAP: 0.0373 NDCG: 0.5559\n",
      "val Loss: 2.1991 LRAP: 0.1142 NDCG: 0.5738\n",
      "Validation loss decreased (2.446596 --> 2.199097).  Saving model ...\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 1.8781 LRAP: 0.0735 NDCG: 0.5751\n",
      "val Loss: 1.8853 LRAP: 0.1644 NDCG: 0.5865\n",
      "Validation loss decreased (2.199097 --> 1.885319).  Saving model ...\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 1.7295 LRAP: 0.1686 NDCG: 0.5960\n",
      "val Loss: 1.8675 LRAP: 0.2922 NDCG: 0.6306\n",
      "Validation loss decreased (1.885319 --> 1.867511).  Saving model ...\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 1.6575 LRAP: 0.2637 NDCG: 0.6197\n",
      "val Loss: 1.6567 LRAP: 0.4521 NDCG: 0.6627\n",
      "Validation loss decreased (1.867511 --> 1.656747).  Saving model ...\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 1.6591 LRAP: 0.3804 NDCG: 0.6406\n",
      "val Loss: 1.6075 LRAP: 0.5388 NDCG: 0.6864\n",
      "Validation loss decreased (1.656747 --> 1.607486).  Saving model ...\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 1.6292 LRAP: 0.4304 NDCG: 0.6497\n",
      "val Loss: 1.6880 LRAP: 0.4155 NDCG: 0.6511\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 1.6159 LRAP: 0.4039 NDCG: 0.6412\n",
      "val Loss: 1.6001 LRAP: 0.5571 NDCG: 0.6943\n",
      "Validation loss decreased (1.607486 --> 1.600107).  Saving model ...\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 1.6012 LRAP: 0.3814 NDCG: 0.6340\n",
      "val Loss: 1.5542 LRAP: 0.4201 NDCG: 0.6588\n",
      "Validation loss decreased (1.600107 --> 1.554165).  Saving model ...\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 1.5702 LRAP: 0.3245 NDCG: 0.6256\n",
      "val Loss: 1.5571 LRAP: 0.3699 NDCG: 0.6418\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 1.5725 LRAP: 0.3382 NDCG: 0.6370\n",
      "val Loss: 1.5378 LRAP: 0.2831 NDCG: 0.6199\n",
      "Validation loss decreased (1.554165 --> 1.537804).  Saving model ...\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 1.5618 LRAP: 0.3245 NDCG: 0.6344\n",
      "val Loss: 1.5180 LRAP: 0.2831 NDCG: 0.6241\n",
      "Validation loss decreased (1.537804 --> 1.518043).  Saving model ...\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 1.5494 LRAP: 0.3441 NDCG: 0.6342\n",
      "val Loss: 1.4972 LRAP: 0.3470 NDCG: 0.6398\n",
      "Validation loss decreased (1.518043 --> 1.497248).  Saving model ...\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 1.5375 LRAP: 0.3559 NDCG: 0.6374\n",
      "val Loss: 1.5065 LRAP: 0.3059 NDCG: 0.6288\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 1.5360 LRAP: 0.3549 NDCG: 0.6363\n",
      "val Loss: 1.5095 LRAP: 0.3881 NDCG: 0.6514\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 1.5361 LRAP: 0.4363 NDCG: 0.6565\n",
      "val Loss: 1.4832 LRAP: 0.4749 NDCG: 0.6731\n",
      "Validation loss decreased (1.497248 --> 1.483165).  Saving model ...\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 1.5117 LRAP: 0.4549 NDCG: 0.6605\n",
      "val Loss: 1.4751 LRAP: 0.4521 NDCG: 0.6628\n",
      "Validation loss decreased (1.483165 --> 1.475139).  Saving model ...\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 1.4969 LRAP: 0.4294 NDCG: 0.6583\n",
      "val Loss: 1.4609 LRAP: 0.4749 NDCG: 0.6746\n",
      "Validation loss decreased (1.475139 --> 1.460872).  Saving model ...\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 1.4982 LRAP: 0.3990 NDCG: 0.6485\n",
      "val Loss: 1.4661 LRAP: 0.4749 NDCG: 0.6649\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 1.4929 LRAP: 0.4255 NDCG: 0.6543\n",
      "val Loss: 1.4891 LRAP: 0.4749 NDCG: 0.6596\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 1.4836 LRAP: 0.4294 NDCG: 0.6585\n",
      "val Loss: 1.4534 LRAP: 0.5160 NDCG: 0.6822\n",
      "Validation loss decreased (1.460872 --> 1.453421).  Saving model ...\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 1.4702 LRAP: 0.4510 NDCG: 0.6608\n",
      "val Loss: 1.4477 LRAP: 0.4932 NDCG: 0.6727\n",
      "Validation loss decreased (1.453421 --> 1.447739).  Saving model ...\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 1.4706 LRAP: 0.4275 NDCG: 0.6611\n",
      "val Loss: 1.4424 LRAP: 0.4475 NDCG: 0.6620\n",
      "Validation loss decreased (1.447739 --> 1.442382).  Saving model ...\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 1.4553 LRAP: 0.4373 NDCG: 0.6622\n",
      "val Loss: 1.4422 LRAP: 0.4292 NDCG: 0.6509\n",
      "Validation loss decreased (1.442382 --> 1.442162).  Saving model ...\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 1.4550 LRAP: 0.4422 NDCG: 0.6640\n",
      "val Loss: 1.4298 LRAP: 0.4886 NDCG: 0.6753\n",
      "Validation loss decreased (1.442162 --> 1.429804).  Saving model ...\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 1.4263 LRAP: 0.4608 NDCG: 0.6623\n",
      "val Loss: 1.4181 LRAP: 0.4840 NDCG: 0.6733\n",
      "Validation loss decreased (1.429804 --> 1.418057).  Saving model ...\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 1.4221 LRAP: 0.4686 NDCG: 0.6712\n",
      "val Loss: 1.4313 LRAP: 0.5023 NDCG: 0.6751\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 1.4112 LRAP: 0.4775 NDCG: 0.6734\n",
      "val Loss: 1.4075 LRAP: 0.4795 NDCG: 0.6712\n",
      "Validation loss decreased (1.418057 --> 1.407519).  Saving model ...\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 1.3979 LRAP: 0.5137 NDCG: 0.6780\n",
      "val Loss: 1.3982 LRAP: 0.5297 NDCG: 0.6762\n",
      "Validation loss decreased (1.407519 --> 1.398221).  Saving model ...\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 1.3824 LRAP: 0.5245 NDCG: 0.6786\n",
      "val Loss: 1.3742 LRAP: 0.5434 NDCG: 0.6799\n",
      "Validation loss decreased (1.398221 --> 1.374241).  Saving model ...\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 1.3848 LRAP: 0.5363 NDCG: 0.6776\n",
      "val Loss: 1.3822 LRAP: 0.5342 NDCG: 0.6772\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 1.3592 LRAP: 0.5294 NDCG: 0.6819\n",
      "val Loss: 1.3758 LRAP: 0.5434 NDCG: 0.6808\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 1.3555 LRAP: 0.5412 NDCG: 0.6767\n",
      "val Loss: 1.3678 LRAP: 0.5571 NDCG: 0.6907\n",
      "Validation loss decreased (1.374241 --> 1.367785).  Saving model ...\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 1.3538 LRAP: 0.5559 NDCG: 0.6826\n",
      "val Loss: 1.3810 LRAP: 0.4977 NDCG: 0.6626\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 1.3402 LRAP: 0.5529 NDCG: 0.6787\n",
      "val Loss: 1.3520 LRAP: 0.5571 NDCG: 0.6841\n",
      "Validation loss decreased (1.367785 --> 1.351983).  Saving model ...\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 1.3328 LRAP: 0.5549 NDCG: 0.6847\n",
      "val Loss: 1.3346 LRAP: 0.5479 NDCG: 0.6812\n",
      "Validation loss decreased (1.351983 --> 1.334578).  Saving model ...\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 1.3207 LRAP: 0.5559 NDCG: 0.6860\n",
      "val Loss: 1.3385 LRAP: 0.5890 NDCG: 0.6925\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 1.3036 LRAP: 0.5706 NDCG: 0.6816\n",
      "val Loss: 1.3256 LRAP: 0.5388 NDCG: 0.6732\n",
      "Validation loss decreased (1.334578 --> 1.325634).  Saving model ...\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 1.2980 LRAP: 0.5588 NDCG: 0.6816\n",
      "val Loss: 1.3289 LRAP: 0.5388 NDCG: 0.6704\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 1.2939 LRAP: 0.5706 NDCG: 0.6839\n",
      "val Loss: 1.3122 LRAP: 0.5479 NDCG: 0.6656\n",
      "Validation loss decreased (1.325634 --> 1.312230).  Saving model ...\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 1.2880 LRAP: 0.5676 NDCG: 0.6836\n",
      "val Loss: 1.3254 LRAP: 0.5799 NDCG: 0.6680\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 1.2838 LRAP: 0.5696 NDCG: 0.6835\n",
      "val Loss: 1.3312 LRAP: 0.5799 NDCG: 0.6756\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 1.2961 LRAP: 0.5676 NDCG: 0.6811\n",
      "val Loss: 1.3175 LRAP: 0.5616 NDCG: 0.6771\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 1.2760 LRAP: 0.5853 NDCG: 0.6882\n",
      "val Loss: 1.3114 LRAP: 0.5753 NDCG: 0.6923\n",
      "Validation loss decreased (1.312230 --> 1.311386).  Saving model ...\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 1.2647 LRAP: 0.5882 NDCG: 0.6906\n",
      "val Loss: 1.3263 LRAP: 0.5434 NDCG: 0.6801\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 1.2617 LRAP: 0.5902 NDCG: 0.6938\n",
      "val Loss: 1.3098 LRAP: 0.5708 NDCG: 0.6829\n",
      "Validation loss decreased (1.311386 --> 1.309808).  Saving model ...\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 1.2518 LRAP: 0.6010 NDCG: 0.6906\n",
      "val Loss: 1.3044 LRAP: 0.5708 NDCG: 0.6768\n",
      "Validation loss decreased (1.309808 --> 1.304363).  Saving model ...\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 1.2426 LRAP: 0.5980 NDCG: 0.6944\n",
      "val Loss: 1.2909 LRAP: 0.5616 NDCG: 0.6702\n",
      "Validation loss decreased (1.304363 --> 1.290922).  Saving model ...\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 1.2504 LRAP: 0.6039 NDCG: 0.6861\n",
      "val Loss: 1.2919 LRAP: 0.5845 NDCG: 0.6797\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 1.2520 LRAP: 0.6118 NDCG: 0.6958\n",
      "val Loss: 1.3073 LRAP: 0.5845 NDCG: 0.6786\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 1.2568 LRAP: 0.6078 NDCG: 0.6910\n",
      "val Loss: 1.3077 LRAP: 0.5799 NDCG: 0.6830\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 1.2346 LRAP: 0.6245 NDCG: 0.7004\n",
      "val Loss: 1.2862 LRAP: 0.5753 NDCG: 0.6853\n",
      "Validation loss decreased (1.290922 --> 1.286226).  Saving model ...\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 1.2327 LRAP: 0.6049 NDCG: 0.6846\n",
      "val Loss: 1.2865 LRAP: 0.5845 NDCG: 0.6927\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 1.2302 LRAP: 0.6157 NDCG: 0.6930\n",
      "val Loss: 1.3122 LRAP: 0.5845 NDCG: 0.6892\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 1.2168 LRAP: 0.6314 NDCG: 0.7022\n",
      "val Loss: 1.2975 LRAP: 0.5890 NDCG: 0.6846\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 1.2079 LRAP: 0.6206 NDCG: 0.6959\n",
      "val Loss: 1.2721 LRAP: 0.5845 NDCG: 0.6789\n",
      "Validation loss decreased (1.286226 --> 1.272071).  Saving model ...\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 1.2184 LRAP: 0.6127 NDCG: 0.6954\n",
      "val Loss: 1.2880 LRAP: 0.5662 NDCG: 0.6765\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 1.1922 LRAP: 0.6216 NDCG: 0.6972\n",
      "val Loss: 1.2924 LRAP: 0.5890 NDCG: 0.6830\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 1.2057 LRAP: 0.6186 NDCG: 0.7022\n",
      "val Loss: 1.2841 LRAP: 0.5708 NDCG: 0.6783\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 1.1969 LRAP: 0.6225 NDCG: 0.6979\n",
      "val Loss: 1.2881 LRAP: 0.5616 NDCG: 0.6738\n",
      "EarlyStopping counter: 4 out of 50\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 1.2014 LRAP: 0.6108 NDCG: 0.6930\n",
      "val Loss: 1.2725 LRAP: 0.5799 NDCG: 0.6753\n",
      "EarlyStopping counter: 5 out of 50\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 1.1932 LRAP: 0.6225 NDCG: 0.7001\n",
      "val Loss: 1.2711 LRAP: 0.5753 NDCG: 0.6755\n",
      "Validation loss decreased (1.272071 --> 1.271069).  Saving model ...\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 1.1794 LRAP: 0.6098 NDCG: 0.6975\n",
      "val Loss: 1.2747 LRAP: 0.5753 NDCG: 0.6735\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 1.1891 LRAP: 0.6196 NDCG: 0.6964\n",
      "val Loss: 1.2743 LRAP: 0.5845 NDCG: 0.6827\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 1.1887 LRAP: 0.6225 NDCG: 0.6968\n",
      "val Loss: 1.2703 LRAP: 0.5845 NDCG: 0.6832\n",
      "Validation loss decreased (1.271069 --> 1.270327).  Saving model ...\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 1.1849 LRAP: 0.6245 NDCG: 0.6943\n",
      "val Loss: 1.2671 LRAP: 0.5799 NDCG: 0.6814\n",
      "Validation loss decreased (1.270327 --> 1.267070).  Saving model ...\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 1.1804 LRAP: 0.6294 NDCG: 0.7032\n",
      "val Loss: 1.2649 LRAP: 0.5799 NDCG: 0.6814\n",
      "Validation loss decreased (1.267070 --> 1.264935).  Saving model ...\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 1.1819 LRAP: 0.6255 NDCG: 0.6995\n",
      "val Loss: 1.2624 LRAP: 0.5799 NDCG: 0.6814\n",
      "Validation loss decreased (1.264935 --> 1.262359).  Saving model ...\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 1.1874 LRAP: 0.6216 NDCG: 0.6991\n",
      "val Loss: 1.2612 LRAP: 0.5799 NDCG: 0.6814\n",
      "Validation loss decreased (1.262359 --> 1.261219).  Saving model ...\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 1.1814 LRAP: 0.6304 NDCG: 0.7015\n",
      "val Loss: 1.2629 LRAP: 0.5845 NDCG: 0.6834\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 1.1774 LRAP: 0.6314 NDCG: 0.7019\n",
      "val Loss: 1.2648 LRAP: 0.5845 NDCG: 0.6869\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 1.1859 LRAP: 0.6186 NDCG: 0.6962\n",
      "val Loss: 1.2677 LRAP: 0.5845 NDCG: 0.6902\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 1.1738 LRAP: 0.6314 NDCG: 0.7037\n",
      "val Loss: 1.2646 LRAP: 0.5799 NDCG: 0.6829\n",
      "EarlyStopping counter: 4 out of 50\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 1.1738 LRAP: 0.6176 NDCG: 0.6984\n",
      "val Loss: 1.2645 LRAP: 0.5799 NDCG: 0.6841\n",
      "EarlyStopping counter: 5 out of 50\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 1.1702 LRAP: 0.6294 NDCG: 0.7041\n",
      "val Loss: 1.2646 LRAP: 0.5845 NDCG: 0.6851\n",
      "EarlyStopping counter: 6 out of 50\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 1.1662 LRAP: 0.6235 NDCG: 0.7015\n",
      "val Loss: 1.2638 LRAP: 0.5845 NDCG: 0.6851\n",
      "EarlyStopping counter: 7 out of 50\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 1.1688 LRAP: 0.6343 NDCG: 0.7048\n",
      "val Loss: 1.2637 LRAP: 0.5845 NDCG: 0.6851\n",
      "EarlyStopping counter: 8 out of 50\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 1.1763 LRAP: 0.6245 NDCG: 0.6983\n",
      "val Loss: 1.2634 LRAP: 0.5845 NDCG: 0.6851\n",
      "EarlyStopping counter: 9 out of 50\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 1.1765 LRAP: 0.6245 NDCG: 0.7009\n",
      "val Loss: 1.2628 LRAP: 0.5845 NDCG: 0.6837\n",
      "EarlyStopping counter: 10 out of 50\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 1.1690 LRAP: 0.6353 NDCG: 0.6998\n",
      "val Loss: 1.2627 LRAP: 0.5845 NDCG: 0.6827\n",
      "EarlyStopping counter: 11 out of 50\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 1.1649 LRAP: 0.6304 NDCG: 0.7053\n",
      "val Loss: 1.2618 LRAP: 0.5845 NDCG: 0.6837\n",
      "EarlyStopping counter: 12 out of 50\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 1.1698 LRAP: 0.6216 NDCG: 0.6969\n",
      "val Loss: 1.2625 LRAP: 0.5845 NDCG: 0.6837\n",
      "EarlyStopping counter: 13 out of 50\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 1.1782 LRAP: 0.6333 NDCG: 0.7062\n",
      "val Loss: 1.2620 LRAP: 0.5845 NDCG: 0.6837\n",
      "EarlyStopping counter: 14 out of 50\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 1.1646 LRAP: 0.6284 NDCG: 0.7037\n",
      "val Loss: 1.2617 LRAP: 0.5845 NDCG: 0.6837\n",
      "EarlyStopping counter: 15 out of 50\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 1.1700 LRAP: 0.6196 NDCG: 0.7023\n",
      "val Loss: 1.2622 LRAP: 0.5845 NDCG: 0.6837\n",
      "EarlyStopping counter: 16 out of 50\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 1.1728 LRAP: 0.6255 NDCG: 0.7026\n",
      "val Loss: 1.2615 LRAP: 0.5845 NDCG: 0.6837\n",
      "EarlyStopping counter: 17 out of 50\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 1.1708 LRAP: 0.6225 NDCG: 0.7001\n",
      "val Loss: 1.2612 LRAP: 0.5845 NDCG: 0.6837\n",
      "EarlyStopping counter: 18 out of 50\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 1.1705 LRAP: 0.6206 NDCG: 0.7018\n",
      "val Loss: 1.2608 LRAP: 0.5845 NDCG: 0.6827\n",
      "Validation loss decreased (1.261219 --> 1.260792).  Saving model ...\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 1.1747 LRAP: 0.6245 NDCG: 0.6985\n",
      "val Loss: 1.2613 LRAP: 0.5845 NDCG: 0.6827\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 1.1757 LRAP: 0.6235 NDCG: 0.6980\n",
      "val Loss: 1.2614 LRAP: 0.5845 NDCG: 0.6827\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Training complete in 0m 53s\n",
      "Best val loss: 1.260792, best LRAP score: 0.8934\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders\n",
    "model = prep_model('SweetNet', classes) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes)\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'multilabel',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Try with my new glm-infused data\n",
    "\n",
    "classes = 15 # 15 kingdoms in the dataset, should read that from the data instead\n",
    "dataloaders = multilabel_kingdom_loaders_emb\n",
    "model = prep_model('SweetNet', classes, use_external_embeddings = True) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes) #changed to 0.01 from 0.005 from 0.0005\n",
    "model_ft = train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 1, mode = 'multilabel', use_external_embeddings = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5025834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweetnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
