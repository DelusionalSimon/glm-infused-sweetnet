{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2416a1b",
   "metadata": {},
   "source": [
    "# GLM-Infused SweetNet Experiments\n",
    "\n",
    "Experimenting with a modified version of SweetNet that allows it to take pre-trained embeddings as input. To get there I need a way to take the embeddings Iâ€™ve gotten from roman and transform them into nice inputs for the model, and a way to set the initial features using these inputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652c8f8",
   "metadata": {},
   "source": [
    "## Importing and exploring the GLM Embedding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0485fe3",
   "metadata": {},
   "source": [
    "### Exploring embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick thing to load a pickle file\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            loaded_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do some quick exploration\n",
    "\n",
    "# --- Explore the loaded data ---\n",
    "print(f\"Type of loaded object: {type(loaded_embeddings)}\")\n",
    "\n",
    "# Common formats for embeddings: dictionary or numpy array\n",
    "if isinstance(loaded_embeddings, dict):\n",
    "    print(f\"Number of items (if dictionary): {len(loaded_embeddings)}\")\n",
    "    # print some keys to see what they look like\n",
    "    print(f\"Example keys (first 5): {list(loaded_embeddings.keys())[:5]}\")\n",
    "elif hasattr(loaded_embeddings, 'shape'):\n",
    "    print(f\"Shape (if array/tensor): {loaded_embeddings.shape}\")\n",
    "    if hasattr(loaded_embeddings, 'dtype'):\n",
    "         print(f\"Data type (if array/tensor): {loaded_embeddings.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e6aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(loaded_embeddings.keys())[5:30]) # Print more keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_key = '!GlcNAc' \n",
    "if example_key in loaded_embeddings:\n",
    "    embedding_vector = loaded_embeddings[example_key]\n",
    "    print(f\"Type of value for '{example_key}': {type(embedding_vector)}\")\n",
    "    if hasattr(embedding_vector, 'shape'):\n",
    "        print(f\"Shape of value: {embedding_vector.shape}\") # This gives dimensionality!\n",
    "        print(f\"Dtype of value: {embedding_vector.dtype}\")\n",
    "    print(embedding_vector) # Print the vector itself if it's not too long\n",
    "else:\n",
    "    print(f\"Key '{example_key}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the keys a bit more closely\n",
    "\n",
    "import collections\n",
    "\n",
    "key_types = collections.defaultdict(int)\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        key_types['linkage_or_modification'] += 1\n",
    "    elif key[0].isalpha():\n",
    "        key_types['monosaccharide'] += 1\n",
    "    else:\n",
    "        key_types['other'] += 1\n",
    "\n",
    "print(key_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore those Other keys \n",
    "\n",
    "other_keys = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        other_keys.append(key)\n",
    "\n",
    "print(f\"Number of 'other' keys: {len(other_keys)}\")\n",
    "print(f\"Examples of 'other' keys: {other_keys[:20]}\") # Print the first 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398bd9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at 50 more keys\n",
    "\n",
    "print(f\"More Examples of 'other' keys: {other_keys[20:70]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore those monosaccharide keys\n",
    "monosaccharide = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        monosaccharide.append(key)\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'monosaccharide' keys: {len(monosaccharide)}\")\n",
    "print(f\"Examples of 'monosaccharide' keys: {monosaccharide[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ddd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be throughough, let's look at 50 Linkage or Modification keys as well\n",
    "linkage_or_modification = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        linkage_or_modification.append(key)\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'linkage_or_modification' keys: {len(linkage_or_modification)}\")\n",
    "print(f\"Examples of 'linkage_or_modification' keys: {linkage_or_modification[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df823d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait, a couple of weeks later and I couldn't figure out why my glm-infused model wasn't converging, \n",
    "# and I just realised that the embeddings were all the same, no wonder it didn't work. \n",
    "# I should have looked at the embeddings themselves, not just the keys.\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    loaded_embeddings = pickle.load(f)\n",
    "\n",
    "first_embedding = None\n",
    "all_same = True\n",
    "if loaded_embeddings:\n",
    "    first_key = next(iter(loaded_embeddings))\n",
    "    first_embedding = loaded_embeddings[first_key]\n",
    "    for key, embedding in loaded_embeddings.items():\n",
    "        if not (embedding == first_embedding).all():\n",
    "            all_same = False\n",
    "            print(f\"Found a different embedding for key: {key}\")\n",
    "            break\n",
    "\n",
    "if all_same and first_embedding is not None:\n",
    "    print(\"All embeddings in the loaded dictionary appear to be the same.\")\n",
    "elif first_embedding is None:\n",
    "    print(\"The embedding dictionary is empty.\")\n",
    "else:\n",
    "    print(\"Embeddings in the dictionary are not all the same.\")\n",
    "\n",
    "print(f\"Number of embeddings in the dictionary: {len(loaded_embeddings)}\")\n",
    "if first_embedding is not None:\n",
    "    print(f\"First embedding:\")\n",
    "    print(first_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9ad5a",
   "metadata": {},
   "source": [
    "### Load the glycowork libr\n",
    "\n",
    "I'll load the glycowork library and compare it to the keys in the embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b14506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glycowork.glycan_data import loader\n",
    "\n",
    "glycowork_vocabulary = loader.lib\n",
    "\n",
    "print(f\"Number of items in glycowork vocabulary: {len(glycowork_vocabulary)}\")\n",
    "print(f\"Example keys from glycowork vocabulary (first 20): {list(glycowork_vocabulary.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e1f3d",
   "metadata": {},
   "source": [
    "Nice, they seem to correspond one to one!\n",
    "\n",
    "That saves me a lot of work down the line (Thanks Roman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at one of the keys in the glycowork vocabulary to see what they return\n",
    "example_glycowork_key = '-10'\n",
    "if example_glycowork_key in glycowork_vocabulary:\n",
    "    glycowork_value = glycowork_vocabulary[example_glycowork_key]\n",
    "    print(f\"Type of value for '{example_glycowork_key}': {type(glycowork_value)}\")\n",
    "    print(glycowork_value)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f827a0",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54413ae2",
   "metadata": {},
   "source": [
    "### Load, filter, and transform glycowork data into glycan_loaders ||||run on kernel restart||||\n",
    "This is used to load and filter glycowork data for a specific prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b4d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the full sugarbase dataset and make a custom dataframe for the problem you want to tackle\n",
    "\n",
    "from glycowork.glycan_data.loader import build_custom_df, df_glycan\n",
    "from glycowork.ml.train_test_split import prepare_multilabel\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "def build_multilabel_dataset(glycan_dataset: str = 'df_species',\n",
    "                          glycan_class: str = 'Kingdom',\n",
    "                          min_class_size: int = 6) -> Tuple[List[str], List[List[float]], List[str]]:\n",
    "    \"\"\"\n",
    "    Loads glycan data, prepares it for multi-label classification, and filters it.\n",
    "\n",
    "    Removes glycans with rare label combinations and filters out individual\n",
    "    labels that have no positive examples in the remaining glycans.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    glycan_dataset : str, optional, default = 'df_species'\n",
    "        The glycowork dataset to use. Options include:\n",
    "        - 'df_species'\n",
    "        - 'df_tissue'\n",
    "        - 'df_disease'\n",
    "    glycan_class : str, optional, default = 'Kingdom'\n",
    "        The class to predict from the chosen dataset. Options depend on\n",
    "        `glycan_dataset`:\n",
    "        - 'df_species': 'Species', 'Genus', 'Family', 'Order', 'Class', 'Phylum', 'Kingdom', 'Domain', 'ref'\n",
    "        - 'df_tissue': 'tissue_sample', 'tissue_species', 'tissue_id', 'tissue_ref'\n",
    "        - 'df_disease': 'disease_association', 'disease_sample', 'disease_direction', 'disease_species', 'disease_id', 'disease_ref'\n",
    "    min_class_size : int, optional, default = 6\n",
    "        Minimum number of samples required for a specific multi-label combination\n",
    "        to be included. Set to 1 to include all combinations. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[str], List[List[float]], List[str]]\n",
    "        A tuple containing:\n",
    "        - glycan_sequences (List[str]): List of glycan strings after filtering.\n",
    "        - binary_labels_filtered (List[List[float]]): List of corresponding\n",
    "          multi-label binary vectors with columns for inactive labels removed.\n",
    "        - label_names_filtered (List[str]): The ordered list of names for each\n",
    "          position in the binary vectors, containing only labels with at\n",
    "          least one positive example.\n",
    "\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    all_glycan_data = df_glycan\n",
    "\n",
    "    # Build custom dataframe\n",
    "    custom_glycan_df = build_custom_df(all_glycan_data, glycan_dataset)\n",
    "\n",
    "    # Extract the list of unique individual labels from the chosen class from the custom_glycan_df\n",
    "    # These are used to dechipher the labels when the model is used for prediction\n",
    "    all_possible_label_names = sorted(list(custom_glycan_df[glycan_class].unique()))\n",
    "    print(f\"Found {len(all_possible_label_names)} unique individual classes/labels.\")\n",
    "\n",
    "\n",
    "    # Prepare for multi-label prediction\n",
    "    glycans, labels = prepare_multilabel(custom_glycan_df, glycan_class)\n",
    "\n",
    "    # if needed, removes classes with fewer than min_class_size samples.\n",
    "    if(min_class_size > 1):\n",
    "\n",
    "        # Convert labels to string representation for counting\n",
    "        label_strings = [''.join(map(str, label)) for label in labels]\n",
    "\n",
    "        # Count occurrences of each label combination\n",
    "        label_counts = Counter(label_strings)\n",
    "\n",
    "        # Filter glycans and labels based on class size\n",
    "        glycan_sequences = [glycans[i] for i, label_str in enumerate(label_strings) if label_counts[label_str] >= min_class_size]\n",
    "        binary_labels_unfiltered = [labels[i] for i, label_str in enumerate(label_strings) if label_counts[label_str] >= min_class_size]\n",
    "        print(f\"Number of unique glycans left after filtering rare classes (size >= {min_class_size}): {len(glycan_sequences)}/{len(glycans)}\")\n",
    "        \n",
    "        # Filter out individual labels with no positive examples after glycan filtering\n",
    "\n",
    "        # Convert binary_labels to numpy array for easier column manipulation\n",
    "        binary_labels_np = np.array(binary_labels_unfiltered)\n",
    "\n",
    "        # Find indices of labels with at least one positive example\n",
    "        # Sum across rows (axis=0) to get count for each label\n",
    "        label_sums = binary_labels_np.sum(axis=0)\n",
    "        active_label_indices = np.where(label_sums > 0)[0]\n",
    "\n",
    "        # Create the final list of label names using the active indices\n",
    "        # Use the initially generated sorted list (all_possible_label_names)\n",
    "        # because its order matches the columns of binary_labels after prepare_multilabel\n",
    "        label_names = [all_possible_label_names[i] for i in active_label_indices]\n",
    "\n",
    "        # Create the final filtered binary label vectors, keeping only the active columns\n",
    "        binary_labels = binary_labels_np[:, active_label_indices].tolist() # Convert back to list of lists\n",
    "\n",
    "        print(f\"Number of unique labels left after filtering: {len(binary_labels[0])}\")\n",
    "\n",
    "    else:\n",
    "        glycan_sequences = glycans\n",
    "        binary_labels = labels\n",
    "        print(f\"Number of unique glycans: {len(glycan_sequences)}\")\n",
    "\n",
    "    return glycan_sequences, binary_labels, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glycans, labels, label_names = build_multilabel_dataset(glycan_dataset='df_disease', glycan_class='disease_association', min_class_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b77c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c440705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick inspection of the data\n",
    "\n",
    "print(f\"Number of unique glycans: {len(glycans)}\")\n",
    "print(f\"Number of label vectors: {len(labels)}\")\n",
    "print(f\"Shape of first label vector (number of members in class): {len(labels[0])}\")\n",
    "print(f\"\\nFirst 5 glycans:\\n{glycans[:5]}\")\n",
    "print(f\"\\nFirst 5 label vectors:\\n{labels[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and testing sets using StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import List, Union\n",
    "\n",
    "def multilabel_split(glycans: List[str], # list of IUPAC-condensed glycans\n",
    "                 labels: List[Union[float, int, str]], # list of prediction labels\n",
    "                 train_size: float = 0.7, # size of train set, the rest is split into validation and test sets\n",
    "                 random_state: int = 42 # random state for reproducibility\n",
    "                )-> Tuple[List[str], List[str], List[str], List[List[float]], List[List[float]], List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and testing sets using StratifiedShuffleSplit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    glycans : List[str]\n",
    "        List of glycan strings (IUPAC-condensed).\n",
    "    labels : List[Union[float, int, str]]\n",
    "        List of label vectors or single labels for stratification. \n",
    "    train_size : float, optional, default = 0.7\n",
    "        Proportion of the dataset to include in the training split. The remaining\n",
    "        data is split equally into validation and test sets\n",
    "    random_state : int, optional, default = 42\n",
    "        Controls the randomness of the split for reproducibility. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[str], List[str], List[str], List[List[float]], List[List[float]], List[List[float]]]\n",
    "        A tuple containing:\n",
    "        - train_glycans (List[str]): Glycans for the training set.\n",
    "        - val_glycans (List[str]): Glycans for the validation set.\n",
    "        - test_glycans (List[str]): Glycans for the testing set.\n",
    "        - train_labels (List[List[float]]): Labels for the training set.\n",
    "        - val_labels (List[List[float]]): Labels for the validation set.\n",
    "        - test_labels (List[List[float]]): Labels for the testing set.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert labels to a suitable format for stratification (string representation)\n",
    "    label_strings = [''.join(map(str, label)) for label in labels]\n",
    "\n",
    "    # Initial split for train vs. (val + test)\n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = 1 - train_size, random_state = random_state)\n",
    "    train_index, temp_index = next(sss.split(glycans, label_strings))\n",
    "    train_glycans = [glycans[i] for i in train_index]\n",
    "    train_labels = [labels[i] for i in train_index]\n",
    "    temp_glycans = [glycans[i] for i in temp_index]\n",
    "    temp_labels = [labels[i] for i in temp_index]\n",
    "\n",
    "    # Split the remaining (val + test) into validation and test sets\n",
    "    sss_val_test = StratifiedShuffleSplit(n_splits = 1, test_size = 0.5, random_state = random_state)\n",
    "    val_index, test_index = next(sss_val_test.split(temp_glycans, [''.join(map(str, label)) for label in temp_labels]))\n",
    "    val_glycans = [temp_glycans[i] for i in val_index]\n",
    "    val_labels = [temp_labels[i] for i in val_index]\n",
    "    test_glycans = [temp_glycans[i] for i in test_index]\n",
    "    test_labels = [temp_labels[i] for i in test_index]\n",
    "\n",
    "    print(\"Split complete!\")\n",
    "    print(f\"Train set size: {len(train_glycans)}\")\n",
    "    print(f\"Validation set size: {len(val_glycans)}\")\n",
    "    print(f\"Test set size: {len(test_glycans)}\")\n",
    "        \n",
    "    return train_glycans, val_glycans, test_glycans, train_labels, val_labels, test_labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use the split function\n",
    "train_glycans, val_glycans, test_glycans, train_labels, val_labels, test_labels = multilabel_split(glycans, labels, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms IUPAC into graphs and makes the data loaders for the training and validation sets\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "\n",
    "glycan_loaders = split_data_to_train(\n",
    "    glycan_list_train = train_glycans,\n",
    "    glycan_list_val = val_glycans,\n",
    "    labels_train = train_labels,\n",
    "    labels_val = val_labels,\n",
    "    batch_size = 128,  # 32 or 128 seem to work well on this system\n",
    "    drop_last = False,\n",
    "    augment_prob = 0.0,  # Adjust if you want augmentation for training\n",
    "    generalization_prob = 0.2  # Adjust if you want generalization for training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae2f77",
   "metadata": {},
   "source": [
    "### GIFFLAR Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a5ce2",
   "metadata": {},
   "source": [
    "#### Load GIFFLAR Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the GIFFLAR dataset for the Taxonomy Kingdom (takes a long time to run(~40m), just use the file it generated)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../GIFFLAR') \n",
    "\n",
    "from gifflar.benchmarks import get_dataset\n",
    "import pathlib\n",
    "\n",
    "data_config_kingdom = {\"name\": \"Taxonomy_Kingdom\"}\n",
    "root_dir = pathlib.Path(\"./data_gifflar\")  # Choose a directory to save the data\n",
    "root_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "kingdom_dataset_config = get_dataset(data_config_kingdom, root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataset configuration\n",
    "print(kingdom_dataset_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a488a",
   "metadata": {},
   "source": [
    "#### Load and transform GIFFLAR Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339c831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load the dataset into a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "filepath = 'data_gifflar/taxonomy_Kingdom.tsv'\n",
    "#filepath = kingdom_dataset_config['filepath'] #If you've loaded it recently, which you shouldn't\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "multilabel_kingdom_df = pd.read_csv(filepath, sep=\"\\t\")\n",
    "# Display the first few rows of the DataFrame\n",
    "print(multilabel_kingdom_df.head())\n",
    "# Display the shape of the DataFrame\n",
    "print(f\"Shape of the DataFrame: {multilabel_kingdom_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b5a99",
   "metadata": {},
   "source": [
    "That looks fine\n",
    "\n",
    "Let's convert them into graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multilabel_kingdom_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ce3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's try the prepare multilabel function from the train_test_split module\n",
    "\n",
    "from glycowork.ml.train_test_split import prepare_multilabel\n",
    "\n",
    "# Prepare the multilabel dataset using the melt function\n",
    "# The melt function is used to transform the DataFrame from wide format to long format\n",
    "kingdom_df_melted = multilabel_kingdom_df.melt(\n",
    "    id_vars=['IUPAC', 'split'],\n",
    "    value_vars=['Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae', 'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae', 'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista', 'Riboviria'],\n",
    "    var_name='Kingdom',\n",
    "    value_name='Association'\n",
    ")\n",
    "\n",
    "# Filter for associations where the glycan belongs to the kingdom (Association == 1)\n",
    "kingdom_df_melted = kingdom_df_melted[kingdom_df_melted['Association'] == 1]\n",
    "\n",
    "# Splitting the dataset using the 'split' column\n",
    "train_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'train'].drop(columns=['split'])\n",
    "val_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'val'].drop(columns=['split'])\n",
    "test_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'test'].drop(columns=['split'])\n",
    "\n",
    "# Finally using the prepare_multilabel function to prepare the data for training\n",
    "glycan_train, label_train = prepare_multilabel(train_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "glycan_val, label_val = prepare_multilabel(val_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "glycan_test, label_test = prepare_multilabel(test_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ad291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make them into graphs again, but hyper efficiently this time\n",
    "\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "\n",
    "multilabel_kingdom_loaders = split_data_to_train(\n",
    "    glycan_list_train=glycan_train,\n",
    "    glycan_list_val=glycan_val,\n",
    "    labels_train=label_train,\n",
    "    labels_val=label_val,\n",
    "    batch_size=32,  # Adjust as needed\n",
    "    drop_last=False,\n",
    "    augment_prob=0.0,  # Adjust if you want augmentation for training\n",
    "    generalization_prob=0.0  # Adjust if you want generalization for training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26e9af",
   "metadata": {},
   "source": [
    "#### Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2104e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's validate the split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "filepath = 'data_gifflar/taxonomy_Kingdom.tsv'\n",
    "\n",
    "# 1. Load the original DataFrame\n",
    "multilabel_kingdom_df_original = pd.read_csv(filepath, sep=\"\\t\")\n",
    "kingdom_cols = ['Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae', 'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae', 'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista', 'Riboviria']\n",
    "\n",
    "# 2. Split the original DataFrame by 'split'\n",
    "train_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'train']\n",
    "val_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'val']\n",
    "test_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'test']\n",
    "\n",
    "# 3. Extract glycans and labels directly\n",
    "glycan_train_list = train_df['IUPAC'].tolist()\n",
    "label_train_list = [train_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_train_list = list(zip(*label_train_list)) # Transpose\n",
    "\n",
    "glycan_val_list = val_df['IUPAC'].tolist()\n",
    "label_val_list = [val_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_val_list = list(zip(*label_val_list)) # Transpose\n",
    "\n",
    "glycan_test_list = test_df['IUPAC'].tolist()\n",
    "label_test_list = [test_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_test_list = list(zip(*label_test_list)) # Transpose\n",
    "\n",
    "print(f\"Number of training glycans: {len(glycan_train_list)}\")\n",
    "print(f\"Number of validation glycans: {len(glycan_val_list)}\")\n",
    "print(f\"Number of test glycans: {len(glycan_test_list)}\")\n",
    "print(f\"Shape of training labels: {len(label_train_list)} x {len(label_train_list[0]) if label_train_list else 0}\")\n",
    "print(f\"Shape of validation labels: {len(label_val_list)} x {len(label_val_list[0]) if label_val_list else 0}\")\n",
    "print(f\"Shape of test labels: {len(label_test_list)} x {len(label_test_list[0]) if label_test_list else 0}\")\n",
    "\n",
    "# Now re-run the checking function (modified for this new label extraction)\n",
    "def check_example_direct(glycan_list, label_list, split_name, original_df, kingdom_cols):\n",
    "    if glycan_list:\n",
    "        example_index = 0\n",
    "        example_glycan = glycan_list[example_index]\n",
    "        example_labels_split = list(label_list[example_index])\n",
    "\n",
    "        original_row = original_df[original_df['IUPAC'] == example_glycan].iloc[0]\n",
    "        labels_original = np.array([original_row[col] for col in kingdom_cols], dtype=np.float32).tolist()\n",
    "\n",
    "        print(f\"--- Checking example from {split_name} set (Direct) ---\")\n",
    "        print(f\"Glycan: {example_glycan}\")\n",
    "        print(f\"Split in original data: {original_row['split']}\")\n",
    "        print(f\"Labels in split data: {example_labels_split}\")\n",
    "        print(f\"Labels in original data: {labels_original}\")\n",
    "        if labels_original == example_labels_split:\n",
    "            print(\"Labels match!\")\n",
    "        else:\n",
    "            print(\"Labels DO NOT match!\")\n",
    "    else:\n",
    "        print(f\"{split_name} set is empty.\")\n",
    "\n",
    "check_example_direct(glycan_train_list, label_train_list, 'train', multilabel_kingdom_df_original, kingdom_cols)\n",
    "check_example_direct(glycan_val_list, label_val_list, 'val', multilabel_kingdom_df_original, kingdom_cols)\n",
    "check_example_direct(glycan_test_list, label_test_list, 'test', multilabel_kingdom_df_original, kingdom_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multilabel_kingdom_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check one of the graphs in the dataloader\n",
    "\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataLoader is called 'multilabel_kingdom_loader'\n",
    "inspected_loader = multilabel_kingdom_loaders_emb['train'] # Access the train DataLoader\n",
    "\n",
    "# Get the first batch of data\n",
    "try:\n",
    "    batch = next(iter(inspected_loader))\n",
    "    print(\"Batch:\", batch)\n",
    "    print(\"Number of graphs in batch:\", batch.num_graphs)\n",
    "\n",
    "    # Extract the first graph from the batch\n",
    "    first_graph_data = batch[2] #change to check other graphs\n",
    "    print(\"\\nFirst graph data:\", first_graph_data)\n",
    "    print(\"Node features (x):\", first_graph_data.x)\n",
    "    print(\"Edge indices (edge_index):\", first_graph_data.edge_index)\n",
    "    print(\"Labels (y):\", first_graph_data.y)\n",
    "    print(\"String labels:\", first_graph_data.string_labels)\n",
    "\n",
    "    # Convert the PyG Data object to a NetworkX graph for visualization\n",
    "    nx_graph = to_networkx(first_graph_data) \n",
    "\n",
    "    # Visualize the NetworkX graph\n",
    "    nx_graph = to_networkx(first_graph_data)\n",
    "    pos = nx.spring_layout(nx_graph)\n",
    "    nx.draw(nx_graph, pos, with_labels=True, node_size=500, node_color=\"red\", font_size=10, font_weight=\"bold\", arrows=False)\n",
    "    plt.title(\"First Graph from DataLoader (PyG)\")\n",
    "    plt.show()\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"The DataLoader is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87dc8a6",
   "metadata": {},
   "source": [
    "## Let's make a function to add embeddings to a dataloader object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the embeddings again here, so I don't have to jump up and down whenever I reload the kernel)\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            glm_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add GLM embeddings to a dictionary of dataloaders before loading them into the model\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def add_glm_embeddings_to_dataloaders(dataloaders, glm_embeddings):\n",
    "    embedded_loaders = {}\n",
    "    embedding_dim = 320\n",
    "    for split, loader in dataloaders.items():\n",
    "        embedded_data_list = []\n",
    "        for batch in loader:\n",
    "            for graph in batch.to_data_list():\n",
    "                node_embeddings = []\n",
    "                if hasattr(graph, 'string_labels'):\n",
    "                    for label in graph.string_labels:\n",
    "                        if label in glm_embeddings:\n",
    "                            embedding = glm_embeddings[label]\n",
    "                            node_embeddings.append(torch.tensor(embedding))\n",
    "                        else:\n",
    "                            node_embeddings.append(torch.zeros(embedding_dim))\n",
    "                    graph.x = torch.stack(node_embeddings).float()\n",
    "                    #print(f\"Shape of graph.x after adding embeddings: {graph.x.shape}\")\n",
    "                embedded_data_list.append(graph)\n",
    "\n",
    "        embedded_loaders[split] = DataLoader(embedded_data_list, batch_size=32)\n",
    "        print(f\"GLM embeddings added to {split} DataLoader.\")\n",
    "    return embedded_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b83ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the function\n",
    "\n",
    "glycan_loaders_emb = add_glm_embeddings_to_dataloaders(glycan_loaders, glm_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef713ec2",
   "metadata": {},
   "source": [
    "## Lets look at the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e13ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at string labels\n",
    "\n",
    "train_loader = multilabel_kingdom_loaders_emb['train'] \n",
    "batch = next(iter(train_loader))\n",
    "first_graph_labels = batch[2].string_labels\n",
    "print(\"Sample string_labels from the first graph:\")\n",
    "print(first_graph_labels[:20])  # Print the first 20 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6368a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the embeddings themselves\n",
    "\n",
    "train_loader = multilabel_kingdom_loaders_emb['train'] \n",
    "batch = next(iter(train_loader))\n",
    "first_graph_embeddings = batch[2].x\n",
    "print(\"Sample embeddings from the first graph:\")\n",
    "print(first_graph_embeddings[:20])  # Print the first 20 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4130ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample keys from glm_embeddings:\")\n",
    "print(list(glm_embeddings.keys())[:200])  # Print the first 20 keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fafdb1",
   "metadata": {},
   "source": [
    "## Copies of glycowork functions edited to use embeddings directly ||||run on kernel restart||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original SweetNet class from the glycowork package\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Union, Literal\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.nn import GraphConv\n",
    "    from torch_geometric.nn import global_mean_pool as gap\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "  raise ImportError(\"<torch or torch_geometric missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from glycowork.glycan_data.loader import lib, download_model \n",
    "\n",
    "class SweetNet(torch.nn.Module):\n",
    "    def __init__(self, lib_size: int, # number of unique tokens for graph nodes\n",
    "                 num_classes: int = 1, # number of output classes (>1 for multilabel)\n",
    "                 hidden_dim: int = 128 # dimension of hidden layers\n",
    "                ) -> None:\n",
    "        \"given glycan graphs as input, predicts properties via a graph neural network\"\n",
    "        super(SweetNet, self).__init__()\n",
    "        # Convolution operations on the graph\n",
    "        self.conv1 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GraphConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Node embedding\n",
    "        self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "        # Fully connected part\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, 1024)\n",
    "        self.lin2 = torch.nn.Linear(1024, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, num_classes)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(1024)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.act1 = torch.nn.LeakyReLU()\n",
    "        self.act2 = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor,\n",
    "                inference: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        # Getting node features\n",
    "        x = self.item_embedding(x)\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        # Graph convolution operations\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = gap(x, batch)\n",
    "\n",
    "        # Fully connected part\n",
    "        x = self.act1(self.bn1(self.lin1(x)))\n",
    "        x_out = self.bn2(self.lin2(x))\n",
    "        x = F.dropout(self.act2(x_out), p = 0.5, training = self.training)\n",
    "\n",
    "        x = self.lin3(x).squeeze(1)\n",
    "\n",
    "        if inference:\n",
    "          return x, x_out\n",
    "        else:\n",
    "          return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333ffcc",
   "metadata": {},
   "source": [
    "### Other functions I might modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model function\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import torch\n",
    "    # Choose the correct computing architecture\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "    raise ImportError(\"<torch missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, mean_squared_error, \\\n",
    "    label_ranking_average_precision_score, ndcg_score, roc_auc_score, mean_absolute_error, r2_score\n",
    "from glycowork.motif.annotate import annotate_dataset\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 7, # epochs to wait after last improvement\n",
    "                 verbose: bool = False # whether to print messages\n",
    "                ) -> None:\n",
    "        \"Early stops the training if validation loss doesn't improve after a given patience\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = 0\n",
    "\n",
    "    def __call__(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), 'drive/My Drive/checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def sigmoid(x: float # input value\n",
    "          ) -> float: # sigmoid transformed value\n",
    "    \"Apply sigmoid transformation to input\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def disable_running_stats(model: torch.nn.Module # model to disable batch norm\n",
    "                       ) -> None:\n",
    "    \"Disable batch normalization running statistics\"\n",
    "\n",
    "    def _disable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "\n",
    "def enable_running_stats(model: torch.nn.Module # model to enable batch norm\n",
    "                      ) -> None:\n",
    "    \"Enable batch normalization running statistics\"\n",
    "\n",
    "    def _enable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def train_model(model: torch.nn.Module, # graph neural network for analyzing glycans\n",
    "               dataloaders: Dict[str, torch.utils.data.DataLoader], # dict with 'train' and 'val' loaders\n",
    "               criterion: torch.nn.Module, # PyTorch loss function\n",
    "               optimizer: torch.optim.Optimizer, # PyTorch optimizer, has to be SAM if mode != \"regression\"\n",
    "               scheduler: torch.optim.lr_scheduler._LRScheduler, # PyTorch learning rate decay\n",
    "               num_epochs: int = 25, # number of epochs for training\n",
    "               patience: int = 50, # epochs without improvement until early stop\n",
    "               mode: str = 'classification', # 'classification', 'multilabel', or 'regression'\n",
    "               mode2: str = 'multi', # 'multi' or 'binary' classification\n",
    "               return_metrics: bool = False, # whether to return metrics\n",
    "               use_external_embeddings: bool = False # whether to use external embeddings\n",
    "              ) -> Union[torch.nn.Module, tuple[torch.nn.Module, dict[str, dict[str, list[float]]]]]: # best model from training and the training and validation metrics\n",
    "    \"trains a deep learning model on predicting glycan properties\"\n",
    "\n",
    "    since = time.time()\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "    best_lead_metric = float(\"inf\")\n",
    "\n",
    "    if mode == 'classification':\n",
    "         # Removed auroc from metrics to avoid calculation issues for now\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": []}\n",
    "    elif mode == 'multilabel':\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": [], \"lrap\": [], \"ndcg\": []}\n",
    "    else:\n",
    "        blank_metrics = {\"loss\": [], \"mse\": [], \"mae\": [], \"r2\": []}\n",
    "\n",
    "    metrics = {\"train\": copy.deepcopy(blank_metrics), \"val\": copy.deepcopy(blank_metrics)}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_metrics = copy.deepcopy(blank_metrics)\n",
    "            running_metrics[\"weights\"] = []\n",
    "\n",
    "            for data in dataloaders[phase]:\n",
    "                # Get all relevant node attributes\n",
    "                #print(f\"Phase: {phase}, Data: {data}\")\n",
    "                #print(f\"Phase: {phase}, Data.x: {getattr(data, 'x', None)}\") # Check if x exists and its value\n",
    "                if use_external_embeddings:\n",
    "                    x, y, edge_index, batch = data.x, data.y, data.edge_index, data.batch\n",
    "                else:\n",
    "                    x, y, edge_index, batch = data.labels, data.y, data.edge_index, data.batch\n",
    "                prot = getattr(data, 'train_idx', None)\n",
    "                if prot is not None:\n",
    "                    prot = prot.view(max(batch) + 1, -1).to(device)\n",
    "                x = x.to(device)\n",
    "                if mode == 'multilabel':\n",
    "                    y = y.view(max(batch) + 1, -1).to(device)\n",
    "                else:\n",
    "                    y = y.to(device)\n",
    "                y = y.view(-1, 1) if mode == 'regression' else y\n",
    "                edge_index = edge_index.to(device)\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # First forward pass\n",
    "                    if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                        enable_running_stats(model)\n",
    "                    pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                    loss = criterion(pred, y)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                            optimizer.first_step(zero_grad = True)\n",
    "                            # Second forward pass\n",
    "                            disable_running_stats(model)\n",
    "                            second_pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                            criterion(second_pred, y).backward()\n",
    "                            optimizer.second_step(zero_grad = True)\n",
    "                        else:\n",
    "                            optimizer.step()\n",
    "                \n",
    "                # Check for single-class batches\n",
    "                unique_classes = torch.unique(y).cpu().numpy()\n",
    "                if len(unique_classes) == 1:\n",
    "                    print(f\"WARNING: Single-class batch detected in {phase} phase at epoch {epoch}!\")\n",
    "                    print(f\"Batch labels: {unique_classes}\")\n",
    "\n",
    "                # Collecting relevant metrics\n",
    "                running_metrics[\"loss\"].append(loss.item())\n",
    "                running_metrics[\"weights\"].append(batch.max().cpu() + 1)\n",
    "\n",
    "                y_det = y.detach().cpu().numpy()\n",
    "                pred_det = pred.cpu().detach().numpy()\n",
    "                if mode == 'classification':\n",
    "                    if mode2 == 'multi':\n",
    "                        pred_proba = np.exp(pred_det) / np.sum(np.exp(pred_det), axis = 1, keepdims = True)  # numpy softmax\n",
    "                        pred2 = np.argmax(pred_det, axis = 1)\n",
    "                    else:\n",
    "                        pred_proba = sigmoid(pred_det)\n",
    "                        pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det, pred2))\n",
    "                    # commented out auroc because it was throwing errors\n",
    "                    #running_metrics[\"auroc\"].append(roc_auc_score(y_det.astype(int), pred_proba if mode2 == 'binary' else pred_proba[:, 1]))\n",
    "                elif mode == 'multilabel':\n",
    "                    pred_proba = sigmoid(pred_det)\n",
    "                    pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det.flatten(), pred2.flatten()))\n",
    "                    running_metrics[\"lrap\"].append(label_ranking_average_precision_score(y_det.astype(int), pred_proba))\n",
    "                    running_metrics[\"ndcg\"].append(ndcg_score(y_det.astype(int), pred_proba))\n",
    "                else:\n",
    "                    running_metrics[\"mse\"].append(mean_squared_error(y_det, pred_det))\n",
    "                    running_metrics[\"mae\"].append(mean_absolute_error(y_det, pred_det))\n",
    "                    running_metrics[\"r2\"].append(r2_score(y_det, pred_det))\n",
    "\n",
    "            # Averaging metrics at end of epoch\n",
    "            for key in running_metrics:\n",
    "                if key == \"weights\":\n",
    "                    continue\n",
    "                metrics[phase][key].append(np.average(running_metrics[key], weights = running_metrics[\"weights\"]))\n",
    "\n",
    "            if mode == 'classification':\n",
    "                print('{} Loss: {:.4f} Accuracy: {:.4f} MCC: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            elif mode == 'multilabel':\n",
    "                print('{} Loss: {:.4f} LRAP: {:.4f} NDCG: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            else:\n",
    "                print('{} Loss: {:.4f} MSE: {:.4f} MAE: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"mse\"][-1], metrics[phase][\"mae\"][-1]))\n",
    "\n",
    "            # Keep best model state_dict\n",
    "            if phase == \"val\":\n",
    "                if metrics[phase][\"loss\"][-1] <= best_loss:\n",
    "                    best_loss = metrics[phase][\"loss\"][-1]\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                    # Extract the lead metric (ACC, LRAP, or MSE) of the new best model\n",
    "                    if mode == 'classification':\n",
    "                        best_lead_metric = metrics[phase][\"acc\"][-1]\n",
    "                    elif mode == 'multilabel':\n",
    "                        best_lead_metric = metrics[phase][\"lrap\"][-1]\n",
    "                    else:\n",
    "                        best_lead_metric = metrics[phase][\"mse\"][-1]\n",
    "\n",
    "                # Check Early Stopping & adjust learning rate if needed\n",
    "                early_stopping(metrics[phase][\"loss\"][-1], model)\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(metrics[phase][\"loss\"][-1])\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "        if mode == 'classification':\n",
    "            print('Best val loss: {:4f}, best Accuracy score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "        elif mode == 'multilabel':\n",
    "            print('Best val loss: {:4f}, best LRAP score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "        else:\n",
    "            print('Best val loss: {:4f}, best MSE score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "        model.load_state_dict(best_model_wts)\n",
    "\n",
    "        if return_metrics:\n",
    "            return model, metrics\n",
    "\n",
    "    # Plot loss & score over the course of training\n",
    "    _, _ = plt.subplots(nrows=2, ncols=1)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(epoch + 1), metrics[\"val\"][\"loss\"])\n",
    "    plt.title('Model Training')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.legend(['Validation Loss'], loc='best')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    if mode == 'classification':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"acc\"])\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "        plt.legend(['Validation Accuracy'], loc='best')\n",
    "    elif mode == 'multilabel':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"lrap\"])\n",
    "        plt.ylabel('Validation LRAP')\n",
    "        plt.legend(['Validation LRAP'], loc='best')\n",
    "    else:\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"mse\"])\n",
    "        plt.ylabel('Validation MSE')\n",
    "        plt.legend(['Validation MSE'], loc='best')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init_weights function\n",
    "\n",
    "def init_weights(model: torch.nn.Module, # neural network for analyzing glycans\n",
    "                mode: str = 'sparse', # initialization algorithm: 'sparse', 'kaiming', 'xavier'\n",
    "                sparsity: float = 0.1 # proportion of sparsity after initialization\n",
    "               ) -> None:\n",
    "    \"initializes linear layers of PyTorch model with a weight initialization\"\n",
    "    #print(\"Using init_weights from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if isinstance(model, torch.nn.Linear):\n",
    "        if mode == 'sparse':\n",
    "            torch.nn.init.sparse_(model.weight, sparsity = sparsity)\n",
    "        elif mode == 'kaiming':\n",
    "            torch.nn.init.kaiming_uniform_(model.weight)\n",
    "        elif mode == 'xavier':\n",
    "            torch.nn.init.xavier_uniform_(model.weight)\n",
    "        else:\n",
    "            print(\"This initialization option is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e02dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_model function\n",
    "\n",
    "def prep_model(model_type: Literal[\"SweetNet\", \"LectinOracle\", \"LectinOracle_flex\", \"NSequonPred\"], # type of model to create\n",
    "              num_classes: int, # number of unique classes for classification\n",
    "              libr: Optional[Dict[str, int]] = None, # dictionary of form glycoletter:index\n",
    "              trained: bool = False, # whether to use pretrained model\n",
    "              # set hidden_dim to 320 rather than 128 for the pretrained model\n",
    "              # but 128 is the default for the model in the paper\n",
    "              hidden_dim: int = 320, # hidden dimension for the model (SweetNet/LectinOracle only)\n",
    "              use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "             ) -> torch.nn.Module: # initialized PyTorch model\n",
    "    \"wrapper to instantiate model, initialize it, and put it on the GPU\"\n",
    "    #print(\"Using prep_model from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if libr is None:\n",
    "      libr = lib\n",
    "    if model_type == 'SweetNet':\n",
    "      model = SweetNet(len(libr), num_classes = num_classes, hidden_dim = hidden_dim, use_external_embeddings = use_external_embeddings)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'sparse'))\n",
    "      if trained:\n",
    "        if hidden_dim != 128:\n",
    "          raise ValueError(\"Hidden dimension must be 128 for pretrained model\")\n",
    "        model_path = download_model(\"glycowork_sweetnet_species.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle':\n",
    "      model = LectinOracle(len(libr), num_classes = num_classes, input_size_prot = int(10*hidden_dim))\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle_flex':\n",
    "      model = LectinOracle_flex(len(libr), num_classes = num_classes)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle_flex.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'NSequonPred':\n",
    "      model = NSequonPred()\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"NSequonPred_batch32.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    else:\n",
    "      print(\"Invalid Model Type\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2749ab38",
   "metadata": {},
   "source": [
    "## Copies of glycowork functions edited for glm embedded dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SweetNet class\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Union, Literal\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.nn import GraphConv\n",
    "    from torch_geometric.nn import global_mean_pool as gap\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "  raise ImportError(\"<torch or torch_geometric missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from glycowork.glycan_data.loader import lib, download_model \n",
    "\n",
    "class SweetNet(torch.nn.Module):\n",
    "    def __init__(self, lib_size: int, # number of unique tokens for graph nodes\n",
    "                 num_classes: int = 1, # number of output classes (>1 for multilabel)\n",
    "                 hidden_dim: int = 320, # dimension of hidden layers (changed from 128 to 320)\n",
    "                 use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "                ) -> None:\n",
    "        \"given glycan graphs as input, predicts properties via a graph neural network\"\n",
    "        #print(\"Using SweetNet from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "        super(SweetNet, self).__init__()\n",
    "        self.use_external_embeddings = use_external_embeddings\n",
    "        # Convolution operations on the graph\n",
    "        self.conv1 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GraphConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Node embedding\n",
    "        if use_external_embeddings:\n",
    "            self.embedding_dim = hidden_dim\n",
    "        else:\n",
    "            self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    " \n",
    "\n",
    "        #self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    "        # Fully connected part\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, 1024)\n",
    "        self.lin2 = torch.nn.Linear(1024, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, num_classes)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(1024)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.act1 = torch.nn.LeakyReLU()\n",
    "        self.act2 = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor,\n",
    "                inference: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \n",
    "        # Getting node features\n",
    "        if self.use_external_embeddings:\n",
    "           # Use external embeddings (already in x)\n",
    "            pass # x is already the embeddings\n",
    "        else:\n",
    "             # Use internal embedding\n",
    "            x = self.item_embedding(x).squeeze(1)\n",
    "            \n",
    "        \n",
    "        # Graph convolution operations (now using GLM embeddings as x)\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = gap(x, batch)\n",
    "\n",
    "        # Fully connected part\n",
    "        x = self.act1(self.bn1(self.lin1(x)))\n",
    "        x_out = self.bn2(self.lin2(x))\n",
    "        x = F.dropout(self.act2(x_out), p = 0.5, training = self.training)\n",
    "\n",
    "        x = self.lin3(x).squeeze(1)\n",
    "\n",
    "        if inference:\n",
    "          return x, x_out\n",
    "        else:\n",
    "          return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03900750",
   "metadata": {},
   "source": [
    "### Other functions I might modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6eccb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model function\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import torch\n",
    "    # Choose the correct computing architecture\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "    raise ImportError(\"<torch missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, mean_squared_error, \\\n",
    "    label_ranking_average_precision_score, ndcg_score, roc_auc_score, mean_absolute_error, r2_score\n",
    "from glycowork.motif.annotate import annotate_dataset\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 7, # epochs to wait after last improvement\n",
    "                 verbose: bool = False # whether to print messages\n",
    "                ) -> None:\n",
    "        \"Early stops the training if validation loss doesn't improve after a given patience\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = 0\n",
    "\n",
    "    def __call__(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), 'drive/My Drive/checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def sigmoid(x: float # input value\n",
    "          ) -> float: # sigmoid transformed value\n",
    "    \"Apply sigmoid transformation to input\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def disable_running_stats(model: torch.nn.Module # model to disable batch norm\n",
    "                       ) -> None:\n",
    "    \"Disable batch normalization running statistics\"\n",
    "\n",
    "    def _disable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "\n",
    "def enable_running_stats(model: torch.nn.Module # model to enable batch norm\n",
    "                      ) -> None:\n",
    "    \"Enable batch normalization running statistics\"\n",
    "\n",
    "    def _enable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def train_model(model: torch.nn.Module, # graph neural network for analyzing glycans\n",
    "               dataloaders: Dict[str, torch.utils.data.DataLoader], # dict with 'train' and 'val' loaders\n",
    "               criterion: torch.nn.Module, # PyTorch loss function\n",
    "               optimizer: torch.optim.Optimizer, # PyTorch optimizer, has to be SAM if mode != \"regression\"\n",
    "               scheduler: torch.optim.lr_scheduler._LRScheduler, # PyTorch learning rate decay\n",
    "               num_epochs: int = 25, # number of epochs for training\n",
    "               patience: int = 50, # epochs without improvement until early stop\n",
    "               mode: str = 'classification', # 'classification', 'multilabel', or 'regression'\n",
    "               mode2: str = 'multi', # 'multi' or 'binary' classification\n",
    "               return_metrics: bool = False, # whether to return metrics\n",
    "               use_external_embeddings: bool = False # whether to use external embeddings\n",
    "              ) -> Union[torch.nn.Module, tuple[torch.nn.Module, dict[str, dict[str, list[float]]]]]: # best model from training and the training and validation metrics\n",
    "    \"trains a deep learning model on predicting glycan properties\"\n",
    "\n",
    "    since = time.time()\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "    best_lead_metric = float(\"inf\")\n",
    "\n",
    "    if mode == 'classification':\n",
    "         # Removed auroc from metrics to avoid calculation issues for now\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": []}\n",
    "    elif mode == 'multilabel':\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": [], \"lrap\": [], \"ndcg\": []}\n",
    "    else:\n",
    "        blank_metrics = {\"loss\": [], \"mse\": [], \"mae\": [], \"r2\": []}\n",
    "\n",
    "    metrics = {\"train\": copy.deepcopy(blank_metrics), \"val\": copy.deepcopy(blank_metrics)}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_metrics = copy.deepcopy(blank_metrics)\n",
    "            running_metrics[\"weights\"] = []\n",
    "\n",
    "            for data in dataloaders[phase]:\n",
    "                # Get all relevant node attributes\n",
    "                #print(f\"Phase: {phase}, Data: {data}\")\n",
    "                #print(f\"Phase: {phase}, Data.x: {getattr(data, 'x', None)}\") # Check if x exists and its value\n",
    "                if use_external_embeddings:\n",
    "                    x, y, edge_index, batch = data.x, data.y, data.edge_index, data.batch\n",
    "                else:\n",
    "                    x, y, edge_index, batch = data.labels, data.y, data.edge_index, data.batch\n",
    "                prot = getattr(data, 'train_idx', None)\n",
    "                if prot is not None:\n",
    "                    prot = prot.view(max(batch) + 1, -1).to(device)\n",
    "                x = x.to(device)\n",
    "                if mode == 'multilabel':\n",
    "                    y = y.view(max(batch) + 1, -1).to(device)\n",
    "                else:\n",
    "                    y = y.to(device)\n",
    "                y = y.view(-1, 1) if mode == 'regression' else y\n",
    "                edge_index = edge_index.to(device)\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # First forward pass\n",
    "                    if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                        enable_running_stats(model)\n",
    "                    pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                    loss = criterion(pred, y)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                            optimizer.first_step(zero_grad = True)\n",
    "                            # Second forward pass\n",
    "                            disable_running_stats(model)\n",
    "                            second_pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                            criterion(second_pred, y).backward()\n",
    "                            optimizer.second_step(zero_grad = True)\n",
    "                        else:\n",
    "                            optimizer.step()\n",
    "                \n",
    "                # Check for single-class batches\n",
    "                unique_classes = torch.unique(y).cpu().numpy()\n",
    "                if len(unique_classes) == 1:\n",
    "                    print(f\"WARNING: Single-class batch detected in {phase} phase at epoch {epoch}!\")\n",
    "                    print(f\"Batch labels: {unique_classes}\")\n",
    "\n",
    "                # Collecting relevant metrics\n",
    "                running_metrics[\"loss\"].append(loss.item())\n",
    "                running_metrics[\"weights\"].append(batch.max().cpu() + 1)\n",
    "\n",
    "                y_det = y.detach().cpu().numpy()\n",
    "                pred_det = pred.cpu().detach().numpy()\n",
    "                if mode == 'classification':\n",
    "                    if mode2 == 'multi':\n",
    "                        pred_proba = np.exp(pred_det) / np.sum(np.exp(pred_det), axis = 1, keepdims = True)  # numpy softmax\n",
    "                        pred2 = np.argmax(pred_det, axis = 1)\n",
    "                    else:\n",
    "                        pred_proba = sigmoid(pred_det)\n",
    "                        pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det, pred2))\n",
    "                    # commented out auroc because it was throwing errors\n",
    "                    #running_metrics[\"auroc\"].append(roc_auc_score(y_det.astype(int), pred_proba if mode2 == 'binary' else pred_proba[:, 1]))\n",
    "                elif mode == 'multilabel':\n",
    "                    pred_proba = sigmoid(pred_det)\n",
    "                    pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det.flatten(), pred2.flatten()))\n",
    "                    running_metrics[\"lrap\"].append(label_ranking_average_precision_score(y_det.astype(int), pred_proba))\n",
    "                    running_metrics[\"ndcg\"].append(ndcg_score(y_det.astype(int), pred_proba))\n",
    "                else:\n",
    "                    running_metrics[\"mse\"].append(mean_squared_error(y_det, pred_det))\n",
    "                    running_metrics[\"mae\"].append(mean_absolute_error(y_det, pred_det))\n",
    "                    running_metrics[\"r2\"].append(r2_score(y_det, pred_det))\n",
    "\n",
    "            # Averaging metrics at end of epoch\n",
    "            for key in running_metrics:\n",
    "                if key == \"weights\":\n",
    "                    continue\n",
    "                metrics[phase][key].append(np.average(running_metrics[key], weights = running_metrics[\"weights\"]))\n",
    "\n",
    "            if mode == 'classification':\n",
    "                print('{} Loss: {:.4f} Accuracy: {:.4f} MCC: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            elif mode == 'multilabel':\n",
    "                print('{} Loss: {:.4f} LRAP: {:.4f} NDCG: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            else:\n",
    "                print('{} Loss: {:.4f} MSE: {:.4f} MAE: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"mse\"][-1], metrics[phase][\"mae\"][-1]))\n",
    "\n",
    "            # Keep best model state_dict\n",
    "            if phase == \"val\":\n",
    "                if metrics[phase][\"loss\"][-1] <= best_loss:\n",
    "                    best_loss = metrics[phase][\"loss\"][-1]\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                    # Extract the lead metric (ACC, LRAP, or MSE) of the new best model\n",
    "                    if mode == 'classification':\n",
    "                        best_lead_metric = metrics[phase][\"acc\"][-1]\n",
    "                    elif mode == 'multilabel':\n",
    "                        best_lead_metric = metrics[phase][\"lrap\"][-1]\n",
    "                    else:\n",
    "                        best_lead_metric = metrics[phase][\"mse\"][-1]\n",
    "\n",
    "                # Check Early Stopping & adjust learning rate if needed\n",
    "                early_stopping(metrics[phase][\"loss\"][-1], model)\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(metrics[phase][\"loss\"][-1])\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    if mode == 'classification':\n",
    "        print('Best val loss: {:4f}, best Accuracy score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    elif mode == 'multilabel':\n",
    "        print('Best val loss: {:4f}, best LRAP score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    else:\n",
    "        print('Best val loss: {:4f}, best MSE score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    if return_metrics:\n",
    "        return model, metrics\n",
    "\n",
    "    # Plot loss & score over the course of training\n",
    "    _, _ = plt.subplots(nrows=2, ncols=1)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(epoch + 1), metrics[\"val\"][\"loss\"])\n",
    "    plt.title('Model Training')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.legend(['Validation Loss'], loc='best')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    if mode == 'classification':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"acc\"])\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "        plt.legend(['Validation Accuracy'], loc='best')\n",
    "    elif mode == 'multilabel':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"lrap\"])\n",
    "        plt.ylabel('Validation LRAP')\n",
    "        plt.legend(['Validation LRAP'], loc='best')\n",
    "    else:\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"mse\"])\n",
    "        plt.ylabel('Validation MSE')\n",
    "        plt.legend(['Validation MSE'], loc='best')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a375b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init_weights function\n",
    "\n",
    "def init_weights(model: torch.nn.Module, # neural network for analyzing glycans\n",
    "                mode: str = 'sparse', # initialization algorithm: 'sparse', 'kaiming', 'xavier'\n",
    "                sparsity: float = 0.1 # proportion of sparsity after initialization\n",
    "               ) -> None:\n",
    "    \"initializes linear layers of PyTorch model with a weight initialization\"\n",
    "    #print(\"Using init_weights from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if isinstance(model, torch.nn.Linear):\n",
    "        if mode == 'sparse':\n",
    "            torch.nn.init.sparse_(model.weight, sparsity = sparsity)\n",
    "        elif mode == 'kaiming':\n",
    "            torch.nn.init.kaiming_uniform_(model.weight)\n",
    "        elif mode == 'xavier':\n",
    "            torch.nn.init.xavier_uniform_(model.weight)\n",
    "        else:\n",
    "            print(\"This initialization option is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_model function\n",
    "\n",
    "def prep_model(model_type: Literal[\"SweetNet\", \"LectinOracle\", \"LectinOracle_flex\", \"NSequonPred\"], # type of model to create\n",
    "              num_classes: int, # number of unique classes for classification\n",
    "              libr: Optional[Dict[str, int]] = None, # dictionary of form glycoletter:index\n",
    "              trained: bool = False, # whether to use pretrained model\n",
    "              # set hidden_dim to 320 rather than 128 for the pretrained model\n",
    "              # but 128 is the default for the model in the paper\n",
    "              hidden_dim: int = 320, # hidden dimension for the model (SweetNet/LectinOracle only)\n",
    "              use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "             ) -> torch.nn.Module: # initialized PyTorch model\n",
    "    \"wrapper to instantiate model, initialize it, and put it on the GPU\"\n",
    "    #print(\"Using prep_model from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if libr is None:\n",
    "      libr = lib\n",
    "    if model_type == 'SweetNet':\n",
    "      model = SweetNet(len(libr), num_classes = num_classes, hidden_dim = hidden_dim, use_external_embeddings = use_external_embeddings)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'sparse'))\n",
    "      if trained:\n",
    "        if hidden_dim != 128:\n",
    "          raise ValueError(\"Hidden dimension must be 128 for pretrained model\")\n",
    "        model_path = download_model(\"glycowork_sweetnet_species.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle':\n",
    "      model = LectinOracle(len(libr), num_classes = num_classes, input_size_prot = int(10*hidden_dim))\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle_flex':\n",
    "      model = LectinOracle_flex(len(libr), num_classes = num_classes)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle_flex.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'NSequonPred':\n",
    "      model = NSequonPred()\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"NSequonPred_batch32.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    else:\n",
    "      print(\"Invalid Model Type\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbbca33",
   "metadata": {},
   "source": [
    "## Testing using same framework as iteration 0 (basic kingdom sweetnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# testing the modified SweetNet model on the GlycoWork dataset \n",
    "from glycowork.glycan_data.loader import df_species\n",
    "from glycowork.ml.train_test_split import hierarchy_filter\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "from glycowork.ml import model_training\n",
    "\n",
    "# silence the avalanche of \"undefined\" warnings\n",
    "#import warnings\n",
    "#from sklearn.exceptions import UndefinedMetricWarning\n",
    "#warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "train_x, val_x, train_y, val_y, id_val, class_list, class_converter = hierarchy_filter(df_species,\n",
    "                                                                                       rank = 'Kingdom')\n",
    "\n",
    "dataloaders = split_data_to_train(train_x, val_x, train_y, val_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets split out the training code so I don't have to load the data each time\n",
    "\n",
    "model = prep_model('SweetNet', len(class_list))\n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = len(class_list))\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'classification',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4382d5b4",
   "metadata": {},
   "source": [
    "### Extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdade799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the Kingdoms in the original data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "kingdom_counts = df_species['Kingdom'].value_counts()\n",
    "print(\"Kingdom distribution in the original data:\")\n",
    "print(kingdom_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f112562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Print the first few rows to get an overview\n",
    "print(df_species.head())\n",
    "\n",
    "# Print the data types of the columns\n",
    "print(df_species.info())\n",
    "\n",
    "# Check for duplicated glycans\n",
    "print(f\"Number of unique glycans: {df_species['glycan'].nunique()}\")\n",
    "print(f\"Total number of rows: {len(df_species)}\")\n",
    "\n",
    "# If you suspect a column contains multiple kingdoms:\n",
    "if 'Kingdom' in df_species.columns:  # Replace 'Kingdom' with the actual column name\n",
    "    # Check the first few values of that column\n",
    "    print(df_species['Kingdom'].head())\n",
    "    # If it's a string with delimiters, count the delimiters\n",
    "    if isinstance(df_species['Kingdom'][0], str) and ',' in df_species['Kingdom'][0]: #assuming ',' is the delimiter\n",
    "        print(f\"Example value: {df_species['Kingdom'][0]}\")\n",
    "        print(f\"Number of commas in the first value: {df_species['Kingdom'][0].count(',')}\")\n",
    "\n",
    "    # Check for multiple columns representing kingdoms\n",
    "    for col in df_species.columns:\n",
    "        if col in ['Animalia', 'Bacteria', 'Plantae']:  # Add all possible kingdom column names\n",
    "            print(f\"Column '{col}': Data type = {df_species[col].dtype}, Unique values = {df_species[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set()\n",
    "duplicates_count = 0\n",
    "\n",
    "for item in train_x:\n",
    "    if item in seen:\n",
    "        duplicates_count += 1\n",
    "    else:\n",
    "        seen.add(item)\n",
    "\n",
    "print(f\"Number of duplicates: {duplicates_count}\")\n",
    "print(f\"Number of unique items: {len(seen)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd3127",
   "metadata": {},
   "source": [
    "## New custom prep function for the GLM-Infused Sweetnet ||||run on kernel restart||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fdcb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the embeddings again here, so I don't have to jump up and down whenever I reload the kernel)\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            glm_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb565ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Literal \n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    from torch_geometric.nn import GraphConv\n",
    "    from torch_geometric.nn import global_mean_pool as gap\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "  raise ImportError(\"<torch or torch_geometric missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from glycowork.glycan_data.loader import lib\n",
    "\n",
    "def prep_infused_sweetnet(num_classes: int, # number of unique classes for classification\n",
    "                           embeddings_dict: Optional[Dict[str, np.ndarray]] = None, # embeddings for 'external' method\n",
    "                           initialization_method: Literal['external', 'random', 'one_hot'] = 'external', # specifies initialization method\n",
    "                           trainable_embeddings: bool = True, # whether the external embeddings should be trainable\n",
    "                           hidden_dim: int = 320, # hidden dimension for the model (be sure to match dimension of embeddings)  \n",
    "                           libr: Optional[Dict[str, int]] = None, # dictionary of form glycoletter:index\n",
    "                          ) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Instantiates and prepares a SweetNet model with specified embedding initialization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_classes : int\n",
    "        Number of unique classes for classification. (REQUIRED)\n",
    "    embeddings_dict : Optional[Dict[str, np.ndarray]], optional, default = None\n",
    "        The loaded external embeddings dictionary {glycan_word: embedding_vector}.\n",
    "        Required if initialization_method is 'external'.\n",
    "    initialization_method : {'external', 'random', 'one_hot'}, optional, default = 'external'\n",
    "        The method to initialize the embedding layer:\n",
    "        - 'external': Initialize with embeddings from embeddings_dict.\n",
    "        - 'random': Randomly initialized embeddings (train from scratch).\n",
    "        - 'one_hot': Initialize with one-hot encoded vectors.\n",
    "    trainable_embeddings : bool, optional, default = True\n",
    "        Whether the embedding layer should be trainable during training.\n",
    "    hidden_dim : int, optional, default = 320\n",
    "        Dimension of hidden layers. Must match the dimension of the embeddings\n",
    "        used if initialization_method is 'external'.\n",
    "    libr : Optional[Dict[str, int]], optional, default = None\n",
    "        Dictionary of form glycoletter:index.\n",
    "        If None, the standard glycowork library is used. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Module\n",
    "        An initialized PyTorch model (SweetNet).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        - If initialization_method is 'external' but embeddings_dict is None.\n",
    "        - If initialization_method is 'external' and embedding dimension does not match hidden_dim.\n",
    "        - If initialization_method is 'one_hot' and hidden_dim does not match library size.\n",
    "        - If initialization_method is 'random' or 'one_hot' and libr is None.\n",
    "        - If an unknown initialization_method is provided.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #  Check if libr is provided, if not, use the default library\n",
    "    if libr is None:\n",
    "        libr = lib\n",
    "\n",
    "    # Check if the required components are available in the current context\n",
    "    if 'SweetNet' not in globals() or not callable(globals()['SweetNet']) or \\\n",
    "       'init_weights' not in globals() or not callable(globals()['init_weights']):\n",
    "         raise ValueError(\"Required glycowork components (SweetNet, init_weights) not available or not callable. Please ensure they are imported correctly.\")\n",
    "\n",
    "    # Instantiate the SweetNet model\n",
    "    model = SweetNet(lib_size=len(libr), num_classes=num_classes, hidden_dim=hidden_dim)\n",
    "    print(f\"SweetNet model instantiated with lib_size={len(libr)}, num_classes={num_classes}, hidden_dim={hidden_dim}.\")\n",
    "\n",
    "\n",
    "    # Apply initial weights to all layers (embedding and non-embedding)\n",
    "    model = model.apply(lambda module: init_weights(module, mode = 'sparse')) # Experiment with 'sparse', 'xavier', or 'kaiming'\n",
    "    \n",
    "    # move model to the device (CPU or GPU)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if initialization_method == 'external':\n",
    "        print(\"Handling 'external' initialization method.\")\n",
    "        \n",
    "        # Check if embeddings_dict is provided\n",
    "        # If not, raise an error (this is a required parameter for 'external' method)\n",
    "        if embeddings_dict is None:\n",
    "            raise ValueError(\"embeddings_dict must be provided when initialization_method is 'external'.\")\n",
    "        \n",
    "        # Check that the dimension of the embeddings_dict matches hidden_dim\n",
    "        embedding_key = next(iter(embeddings_dict))\n",
    "        external_embedding_dim = embeddings_dict[embedding_key].shape[0]\n",
    "        if external_embedding_dim != hidden_dim:\n",
    "             raise ValueError(f\"External embedding dimension ({external_embedding_dim}) must match model's hidden_dim ({hidden_dim}).\")\n",
    "        \n",
    "        # Get the tensor of the embedding layer's weights. It already has initial random values.\n",
    "        embedding_tensor_to_populate = model.item_embedding.weight.data\n",
    "\n",
    "        #\n",
    "        with torch.no_grad():\n",
    "                 # Iterate through the library (which gives us the index for each glycan word)\n",
    "                 for glycan_word, index in libr.items():\n",
    "                    if glycan_word in embeddings_dict:\n",
    "\n",
    "                        # Get the embedding vector from the dictionary\n",
    "                        embedding_vector = embeddings_dict[glycan_word] \n",
    "\n",
    "                        # Convert to tensor, ensure correct dtype, and move to the same device\n",
    "                        embedding_vector_tensor = torch.tensor(embedding_vector, dtype=torch.float32).to(embedding_tensor_to_populate.device)\n",
    "\n",
    "                        # Copy the vector into the correct row of the model's embedding tensor\n",
    "                        # Relying on index from libr being valid for embedding_tensor_to_populate size\n",
    "                        embedding_tensor_to_populate[index].copy_(embedding_vector_tensor)\n",
    "                        \n",
    "                        #print(glycan_word)\n",
    "                        #print(embedding_vector)\n",
    "                    else:\n",
    "                        # If a glycan word in libr is NOT in embeddings_dict, its initial random value is preserved (for smaller dictionaries).\n",
    "                        print(f\"{glycan_word} is not in library, keeping its initial random value.\")\n",
    "                        pass # Explicitly do nothing, keeping the initial random value\n",
    "         \n",
    "\n",
    "    elif initialization_method == 'random':\n",
    "        print(\"Handling 'random' initialization method (training from scratch).\")\n",
    "        \n",
    "        # The item_embedding layer was already initialized randomly by the standard initialization loop above.\n",
    "\n",
    "        pass \n",
    "\n",
    "        \n",
    "    elif initialization_method == 'one_hot':\n",
    "        print(\" 'one_hot' initialization method not implemented yet\")\n",
    "        \n",
    "    # either I need the hidden_dim to be the same as the number of glycoletters in the library\n",
    "    # or I need to find a way to reduce the dimensionality of the one-hot encoding to match the hidden_dim\n",
    "    #or do what Roman suggested and reduce the dictionary to the 319 most common glycowords\n",
    "    # Let's tackle this later given time \n",
    "\n",
    "\n",
    "        # Determine the required embedding dimension for one-hot encoding\n",
    "        #required_hidden_dim = len(libr) + 1\n",
    "\n",
    "        # Create the one-hot embedding matrix (identity matrix)\n",
    "        #one_hot_matrix = torch.eye(required_hidden_dim, dtype=torch.float32)\n",
    "\n",
    "        # Copy one_hot_matrix to model.item_embedding.weight.data\n",
    "        #one_hot_matrix = one_hot_matrix.to(model.item_embedding.weight.device)\n",
    "\n",
    "        # Copy the one-hot matrix into the model's item_embedding.weight.data\n",
    "        #with torch.no_grad():\n",
    "           # model.item_embedding.weight.copy_(one_hot_matrix)\n",
    "            \n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        # This case should ideally be caught by the Literal type hint and docstring,\n",
    "        # but adding a runtime check is robust.\n",
    "        raise ValueError(f\"Unknown initialization_method: {initialization_method}\")\n",
    "\n",
    "\n",
    "    # Set trainability based on trainable_embeddings flag (outside the branches)\n",
    "    # This happens AFTER the initialization logic in the branches above.\n",
    "    # The logic for setting requires_grad is the same regardless of initialization method.\n",
    "    model.item_embedding.weight.requires_grad = trainable_embeddings\n",
    "    print(f\"SweetNet item_embedding layer set to trainable: {trainable_embeddings}.\")\n",
    "\n",
    "    \n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "#test = prep_infused_sweetnet(num_classes=10,embeddings_dict=glm_embeddings, initialization_method='external')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e20752",
   "metadata": {},
   "source": [
    "## Time to train a multi-class multi-label Sweetnet on our properly loaded data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fdeaa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SweetNet model instantiated with lib_size=2565, num_classes=18, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.7284 LRAP: 0.0010 NDCG: 0.1826\n",
      "val Loss: 3.7994 LRAP: 0.0000 NDCG: 0.1129\n",
      "Validation loss decreased (0.000000 --> 3.799370).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 3.6576 LRAP: 0.0039 NDCG: 0.2534\n",
      "val Loss: 3.7782 LRAP: 0.0000 NDCG: 0.1565\n",
      "Validation loss decreased (3.799370 --> 3.778179).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 3.5778 LRAP: 0.0059 NDCG: 0.2707\n",
      "val Loss: 3.7002 LRAP: 0.0000 NDCG: 0.3391\n",
      "Validation loss decreased (3.778179 --> 3.700156).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 3.4866 LRAP: 0.0078 NDCG: 0.3182\n",
      "val Loss: 3.6578 LRAP: 0.0091 NDCG: 0.3986\n",
      "Validation loss decreased (3.700156 --> 3.657808).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 3.4033 LRAP: 0.0157 NDCG: 0.3607\n",
      "val Loss: 3.6144 LRAP: 0.0228 NDCG: 0.4963\n",
      "Validation loss decreased (3.657808 --> 3.614412).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 3.614412, best LRAP score: 0.8144\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using new prep_infused_sweetnet to generate glm-infused SweetNet model\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders\n",
    "model =  prep_infused_sweetnet(\n",
    "            initialization_method = 'random',\n",
    "            num_classes = classes,\n",
    "            embeddings_dict = glm_embeddings, \n",
    "            trainable_embeddings = True\n",
    "            ) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes)\n",
    "model_ft = train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 5, mode = 'multilabel',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd57110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using new prep_infused_sweetnet to generate glm-infused SweetNet model\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders\n",
    "model =  prep_infused_sweetnet(\n",
    "            initialization_method = 'external',\n",
    "            num_classes = classes,\n",
    "            embeddings_dict = glm_embeddings, \n",
    "            trainable_embeddings = True\n",
    "            ) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes)\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'multilabel',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.item_embedding.weight.data[3])\n",
    "print(model.item_embedding.weight.data[10])\n",
    "print(model.item_embedding.weight.data[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59cf29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using standard prep_model\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders\n",
    "model =  prep_model('SweetNet', classes) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes)\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'multilabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Try with my new glm-infused data\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders_emb\n",
    "model = prep_model('SweetNet', classes, use_external_embeddings = True) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes) #changed to 0.01 from 0.005 from 0.0005\n",
    "model_ft = train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'multilabel', use_external_embeddings = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5025834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweetnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
