{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2416a1b",
   "metadata": {},
   "source": [
    "# GLM-Infused SweetNet Experiments\n",
    "\n",
    "Experimenting with a modified version of SweetNet that allows it to take pre-trained embeddings as input. To get there I need a way to take the embeddings Iâ€™ve gotten from roman and transform them into nice inputs for the model, and a way to set the initial features using these inputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652c8f8",
   "metadata": {},
   "source": [
    "## Importing and exploring the GLM Embedding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0485fe3",
   "metadata": {},
   "source": [
    "### Exploring embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick thing to load a pickle file\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            loaded_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do some quick exploration\n",
    "\n",
    "# --- Explore the loaded data ---\n",
    "print(f\"Type of loaded object: {type(loaded_embeddings)}\")\n",
    "\n",
    "# Common formats for embeddings: dictionary or numpy array\n",
    "if isinstance(loaded_embeddings, dict):\n",
    "    print(f\"Number of items (if dictionary): {len(loaded_embeddings)}\")\n",
    "    # print some keys to see what they look like\n",
    "    print(f\"Example keys (first 5): {list(loaded_embeddings.keys())[:5]}\")\n",
    "elif hasattr(loaded_embeddings, 'shape'):\n",
    "    print(f\"Shape (if array/tensor): {loaded_embeddings.shape}\")\n",
    "    if hasattr(loaded_embeddings, 'dtype'):\n",
    "         print(f\"Data type (if array/tensor): {loaded_embeddings.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e6aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(loaded_embeddings.keys())[5:30]) # Print more keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_key = '!GlcNAc' \n",
    "if example_key in loaded_embeddings:\n",
    "    embedding_vector = loaded_embeddings[example_key]\n",
    "    print(f\"Type of value for '{example_key}': {type(embedding_vector)}\")\n",
    "    if hasattr(embedding_vector, 'shape'):\n",
    "        print(f\"Shape of value: {embedding_vector.shape}\") # This gives dimensionality!\n",
    "        print(f\"Dtype of value: {embedding_vector.dtype}\")\n",
    "    print(embedding_vector) # Print the vector itself if it's not too long\n",
    "else:\n",
    "    print(f\"Key '{example_key}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the keys a bit more closely\n",
    "\n",
    "import collections\n",
    "\n",
    "key_types = collections.defaultdict(int)\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        key_types['linkage_or_modification'] += 1\n",
    "    elif key[0].isalpha():\n",
    "        key_types['monosaccharide'] += 1\n",
    "    else:\n",
    "        key_types['other'] += 1\n",
    "\n",
    "print(key_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore those Other keys \n",
    "\n",
    "other_keys = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        other_keys.append(key)\n",
    "\n",
    "print(f\"Number of 'other' keys: {len(other_keys)}\")\n",
    "print(f\"Examples of 'other' keys: {other_keys[:20]}\") # Print the first 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398bd9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at 50 more keys\n",
    "\n",
    "print(f\"More Examples of 'other' keys: {other_keys[20:70]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore those monosaccharide keys\n",
    "monosaccharide = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        monosaccharide.append(key)\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'monosaccharide' keys: {len(monosaccharide)}\")\n",
    "print(f\"Examples of 'monosaccharide' keys: {monosaccharide[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ddd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be throughough, let's look at 50 Linkage or Modification keys as well\n",
    "linkage_or_modification = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        linkage_or_modification.append(key)\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'linkage_or_modification' keys: {len(linkage_or_modification)}\")\n",
    "print(f\"Examples of 'linkage_or_modification' keys: {linkage_or_modification[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df823d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait, a couple of weeks later and I couldn't figure out why my glm-infused model wasn't converging, \n",
    "# and I just realised that the embeddings were all the same, no wonder it didn't work. \n",
    "# I should have looked at the embeddings themselves, not just the keys.\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    loaded_embeddings = pickle.load(f)\n",
    "\n",
    "first_embedding = None\n",
    "all_same = True\n",
    "if loaded_embeddings:\n",
    "    first_key = next(iter(loaded_embeddings))\n",
    "    first_embedding = loaded_embeddings[first_key]\n",
    "    for key, embedding in loaded_embeddings.items():\n",
    "        if not (embedding == first_embedding).all():\n",
    "            all_same = False\n",
    "            print(f\"Found a different embedding for key: {key}\")\n",
    "            break\n",
    "\n",
    "if all_same and first_embedding is not None:\n",
    "    print(\"All embeddings in the loaded dictionary appear to be the same.\")\n",
    "elif first_embedding is None:\n",
    "    print(\"The embedding dictionary is empty.\")\n",
    "else:\n",
    "    print(\"Embeddings in the dictionary are not all the same.\")\n",
    "\n",
    "print(f\"Number of embeddings in the dictionary: {len(loaded_embeddings)}\")\n",
    "if first_embedding is not None:\n",
    "    print(f\"First embedding:\")\n",
    "    print(first_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9ad5a",
   "metadata": {},
   "source": [
    "### Load the glycowork libr\n",
    "\n",
    "I'll load the glycowork library and compare it to the keys in the embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b14506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glycowork.glycan_data import loader\n",
    "\n",
    "glycowork_vocabulary = loader.lib\n",
    "\n",
    "print(f\"Number of items in glycowork vocabulary: {len(glycowork_vocabulary)}\")\n",
    "print(f\"Example keys from glycowork vocabulary (first 20): {list(glycowork_vocabulary.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e1f3d",
   "metadata": {},
   "source": [
    "Nice, they seem to correspond one to one!\n",
    "\n",
    "That saves me a lot of work down the line (Thanks Roman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at one of the keys in the glycowork vocabulary to see what they return\n",
    "example_glycowork_key = '-10'\n",
    "if example_glycowork_key in glycowork_vocabulary:\n",
    "    glycowork_value = glycowork_vocabulary[example_glycowork_key]\n",
    "    print(f\"Type of value for '{example_glycowork_key}': {type(glycowork_value)}\")\n",
    "    print(glycowork_value)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f827a0",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54413ae2",
   "metadata": {},
   "source": [
    "### Load, filter, and transform glycowork data into glycan_loaders ||||run on kernel restart||||\n",
    "This is used to load and filter glycowork data for a specific prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b4d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the full sugarbase dataset and make a custom dataframe for the problem you want to tackle\n",
    "\n",
    "from glycowork.glycan_data.loader import build_custom_df, df_glycan\n",
    "from glycowork.ml.train_test_split import prepare_multilabel\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "def build_multilabel_dataset(glycan_dataset: str = 'df_species',\n",
    "                          glycan_class: str = 'Kingdom',\n",
    "                          min_class_size: int = 6) -> Tuple[List[str], List[List[float]], List[str]]:\n",
    "    \"\"\"\n",
    "    Loads glycan data, prepares it for multi-label classification, and filters it.\n",
    "\n",
    "    Removes glycans with rare label combinations and filters out individual\n",
    "    labels that have no positive examples in the remaining glycans.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    glycan_dataset : str, optional, default = 'df_species'\n",
    "        The glycowork dataset to use. Options include:\n",
    "        - 'df_species'\n",
    "        - 'df_tissue'\n",
    "        - 'df_disease'\n",
    "    glycan_class : str, optional, default = 'Kingdom'\n",
    "        The class to predict from the chosen dataset. Options depend on\n",
    "        `glycan_dataset`:\n",
    "        - 'df_species': 'Species', 'Genus', 'Family', 'Order', 'Class', 'Phylum', 'Kingdom', 'Domain', 'ref'\n",
    "        - 'df_tissue': 'tissue_sample', 'tissue_species', 'tissue_id', 'tissue_ref'\n",
    "        - 'df_disease': 'disease_association', 'disease_sample', 'disease_direction', 'disease_species', 'disease_id', 'disease_ref'\n",
    "    min_class_size : int, optional, default = 6\n",
    "        Minimum number of samples required for a specific multi-label combination\n",
    "        to be included. Set to 1 to include all combinations. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[str], List[List[float]], List[str]]\n",
    "        A tuple containing:\n",
    "        - glycan_sequences (List[str]): List of glycan strings after filtering.\n",
    "        - binary_labels_filtered (List[List[float]]): List of corresponding\n",
    "          multi-label binary vectors with columns for inactive labels removed.\n",
    "        - label_names_filtered (List[str]): The ordered list of names for each\n",
    "          position in the binary vectors, containing only labels with at\n",
    "          least one positive example.\n",
    "\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    all_glycan_data = df_glycan\n",
    "\n",
    "    # Build custom dataframe\n",
    "    custom_glycan_df = build_custom_df(all_glycan_data, glycan_dataset)\n",
    "\n",
    "    # Extract the list of unique individual labels from the chosen class from the custom_glycan_df\n",
    "    # These are used to dechipher the labels when the model is used for prediction\n",
    "    all_possible_label_names = sorted(list(custom_glycan_df[glycan_class].unique()))\n",
    "    print(f\"Found {len(all_possible_label_names)} unique individual classes/labels.\")\n",
    "\n",
    "\n",
    "    # Prepare for multi-label prediction\n",
    "    glycans, labels = prepare_multilabel(custom_glycan_df, glycan_class)\n",
    "\n",
    "    # if needed, removes classes with fewer than min_class_size samples.\n",
    "    if(min_class_size > 1):\n",
    "\n",
    "        # Convert labels to string representation for counting\n",
    "        label_strings = [''.join(map(str, label)) for label in labels]\n",
    "\n",
    "        # Count occurrences of each label combination\n",
    "        label_counts = Counter(label_strings)\n",
    "\n",
    "        # Filter glycans and labels based on class size\n",
    "        glycan_sequences = [glycans[i] for i, label_str in enumerate(label_strings) if label_counts[label_str] >= min_class_size]\n",
    "        binary_labels_unfiltered = [labels[i] for i, label_str in enumerate(label_strings) if label_counts[label_str] >= min_class_size]\n",
    "        print(f\"Number of unique glycans left after filtering rare classes (size >= {min_class_size}): {len(glycan_sequences)}/{len(glycans)}\")\n",
    "        \n",
    "        # Filter out individual labels with no positive examples after glycan filtering\n",
    "\n",
    "        # Convert binary_labels to numpy array for easier column manipulation\n",
    "        binary_labels_np = np.array(binary_labels_unfiltered)\n",
    "\n",
    "        # Find indices of labels with at least one positive example\n",
    "        # Sum across rows (axis=0) to get count for each label\n",
    "        label_sums = binary_labels_np.sum(axis=0)\n",
    "        active_label_indices = np.where(label_sums > 0)[0]\n",
    "\n",
    "        # Create the final list of label names using the active indices\n",
    "        # Use the initially generated sorted list (all_possible_label_names)\n",
    "        # because its order matches the columns of binary_labels after prepare_multilabel\n",
    "        label_names = [all_possible_label_names[i] for i in active_label_indices]\n",
    "\n",
    "        # Create the final filtered binary label vectors, keeping only the active columns\n",
    "        binary_labels = binary_labels_np[:, active_label_indices].tolist() # Convert back to list of lists\n",
    "\n",
    "        print(f\"Number of unique labels left after filtering: {len(binary_labels[0])}\")\n",
    "\n",
    "    else:\n",
    "        glycan_sequences = glycans\n",
    "        binary_labels = labels\n",
    "        print(f\"Number of unique glycans: {len(glycan_sequences)}\")\n",
    "\n",
    "    return glycan_sequences, binary_labels, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glycans, labels, label_names = build_multilabel_dataset(glycan_dataset='df_disease', glycan_class='disease_association', min_class_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b77c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c440705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick inspection of the data\n",
    "\n",
    "print(f\"Number of unique glycans: {len(glycans)}\")\n",
    "print(f\"Number of label vectors: {len(labels)}\")\n",
    "print(f\"Shape of first label vector (number of members in class): {len(labels[0])}\")\n",
    "print(f\"\\nFirst 5 glycans:\\n{glycans[:5]}\")\n",
    "print(f\"\\nFirst 5 label vectors:\\n{labels[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and testing sets using StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import List, Union\n",
    "\n",
    "def multilabel_split(glycans: List[str], # list of IUPAC-condensed glycans\n",
    "                 labels: List[Union[float, int, str]], # list of prediction labels\n",
    "                 train_size: float = 0.7, # size of train set, the rest is split into validation and test sets\n",
    "                 random_state: int = 42 # random state for reproducibility\n",
    "                )-> Tuple[List[str], List[str], List[str], List[List[float]], List[List[float]], List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and testing sets using StratifiedShuffleSplit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    glycans : List[str]\n",
    "        List of glycan strings (IUPAC-condensed).\n",
    "    labels : List[Union[float, int, str]]\n",
    "        List of label vectors or single labels for stratification. \n",
    "    train_size : float, optional, default = 0.7\n",
    "        Proportion of the dataset to include in the training split. The remaining\n",
    "        data is split equally into validation and test sets\n",
    "    random_state : int, optional, default = 42\n",
    "        Controls the randomness of the split for reproducibility. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[str], List[str], List[str], List[List[float]], List[List[float]], List[List[float]]]\n",
    "        A tuple containing:\n",
    "        - train_glycans (List[str]): Glycans for the training set.\n",
    "        - val_glycans (List[str]): Glycans for the validation set.\n",
    "        - test_glycans (List[str]): Glycans for the testing set.\n",
    "        - train_labels (List[List[float]]): Labels for the training set.\n",
    "        - val_labels (List[List[float]]): Labels for the validation set.\n",
    "        - test_labels (List[List[float]]): Labels for the testing set.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert labels to a suitable format for stratification (string representation)\n",
    "    label_strings = [''.join(map(str, label)) for label in labels]\n",
    "\n",
    "    # Initial split for train vs. (val + test)\n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = 1 - train_size, random_state = random_state)\n",
    "    train_index, temp_index = next(sss.split(glycans, label_strings))\n",
    "    train_glycans = [glycans[i] for i in train_index]\n",
    "    train_labels = [labels[i] for i in train_index]\n",
    "    temp_glycans = [glycans[i] for i in temp_index]\n",
    "    temp_labels = [labels[i] for i in temp_index]\n",
    "\n",
    "    # Split the remaining (val + test) into validation and test sets\n",
    "    sss_val_test = StratifiedShuffleSplit(n_splits = 1, test_size = 0.5, random_state = random_state)\n",
    "    val_index, test_index = next(sss_val_test.split(temp_glycans, [''.join(map(str, label)) for label in temp_labels]))\n",
    "    val_glycans = [temp_glycans[i] for i in val_index]\n",
    "    val_labels = [temp_labels[i] for i in val_index]\n",
    "    test_glycans = [temp_glycans[i] for i in test_index]\n",
    "    test_labels = [temp_labels[i] for i in test_index]\n",
    "\n",
    "    print(\"Split complete!\")\n",
    "    print(f\"Train set size: {len(train_glycans)}\")\n",
    "    print(f\"Validation set size: {len(val_glycans)}\")\n",
    "    print(f\"Test set size: {len(test_glycans)}\")\n",
    "        \n",
    "    return train_glycans, val_glycans, test_glycans, train_labels, val_labels, test_labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use the split function\n",
    "train_glycans, val_glycans, test_glycans, train_labels, val_labels, test_labels = multilabel_split(glycans, labels, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms IUPAC into graphs and makes the data loaders for the training and validation sets\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "\n",
    "glycan_loaders = split_data_to_train(\n",
    "    glycan_list_train = train_glycans,\n",
    "    glycan_list_val = val_glycans,\n",
    "    labels_train = train_labels,\n",
    "    labels_val = val_labels,\n",
    "    batch_size = 128,  # 32 or 128 seem to work well on this system\n",
    "    drop_last = False,\n",
    "    augment_prob = 0.0,  # Adjust if you want augmentation for training\n",
    "    generalization_prob = 0.2  # Adjust if you want generalization for training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae2f77",
   "metadata": {},
   "source": [
    "### GIFFLAR Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a5ce2",
   "metadata": {},
   "source": [
    "#### Load GIFFLAR Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the GIFFLAR dataset for the Taxonomy Kingdom (takes a long time to run(~40m), just use the file it generated)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../GIFFLAR') \n",
    "\n",
    "from gifflar.benchmarks import get_dataset\n",
    "import pathlib\n",
    "\n",
    "data_config_kingdom = {\"name\": \"Taxonomy_Kingdom\"}\n",
    "root_dir = pathlib.Path(\"./data_gifflar\")  # Choose a directory to save the data\n",
    "root_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "kingdom_dataset_config = get_dataset(data_config_kingdom, root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataset configuration\n",
    "print(kingdom_dataset_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a488a",
   "metadata": {},
   "source": [
    "#### Load and transform GIFFLAR Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339c831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load the dataset into a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "filepath = 'data_gifflar/taxonomy_Kingdom.tsv'\n",
    "#filepath = kingdom_dataset_config['filepath'] #If you've loaded it recently, which you shouldn't\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "multilabel_kingdom_df = pd.read_csv(filepath, sep=\"\\t\")\n",
    "# Display the first few rows of the DataFrame\n",
    "print(multilabel_kingdom_df.head())\n",
    "# Display the shape of the DataFrame\n",
    "print(f\"Shape of the DataFrame: {multilabel_kingdom_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b5a99",
   "metadata": {},
   "source": [
    "That looks fine\n",
    "\n",
    "Let's convert them into graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multilabel_kingdom_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ce3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's try the prepare multilabel function from the train_test_split module\n",
    "\n",
    "from glycowork.ml.train_test_split import prepare_multilabel\n",
    "\n",
    "# Prepare the multilabel dataset using the melt function\n",
    "# The melt function is used to transform the DataFrame from wide format to long format\n",
    "kingdom_df_melted = multilabel_kingdom_df.melt(\n",
    "    id_vars=['IUPAC', 'split'],\n",
    "    value_vars=['Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae', 'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae', 'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista', 'Riboviria'],\n",
    "    var_name='Kingdom',\n",
    "    value_name='Association'\n",
    ")\n",
    "\n",
    "# Filter for associations where the glycan belongs to the kingdom (Association == 1)\n",
    "kingdom_df_melted = kingdom_df_melted[kingdom_df_melted['Association'] == 1]\n",
    "\n",
    "# Splitting the dataset using the 'split' column\n",
    "train_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'train'].drop(columns=['split'])\n",
    "val_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'val'].drop(columns=['split'])\n",
    "test_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'test'].drop(columns=['split'])\n",
    "\n",
    "# Finally using the prepare_multilabel function to prepare the data for training\n",
    "glycan_train, label_train = prepare_multilabel(train_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "glycan_val, label_val = prepare_multilabel(val_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "glycan_test, label_test = prepare_multilabel(test_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ad291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make them into graphs again, but hyper efficiently this time\n",
    "\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "\n",
    "multilabel_kingdom_loaders = split_data_to_train(\n",
    "    glycan_list_train=glycan_train,\n",
    "    glycan_list_val=glycan_val,\n",
    "    labels_train=label_train,\n",
    "    labels_val=label_val,\n",
    "    batch_size=32,  # Adjust as needed\n",
    "    drop_last=False,\n",
    "    augment_prob=0.0,  # Adjust if you want augmentation for training\n",
    "    generalization_prob=0.0  # Adjust if you want generalization for training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26e9af",
   "metadata": {},
   "source": [
    "#### Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2104e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's validate the split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "filepath = 'data_gifflar/taxonomy_Kingdom.tsv'\n",
    "\n",
    "# 1. Load the original DataFrame\n",
    "multilabel_kingdom_df_original = pd.read_csv(filepath, sep=\"\\t\")\n",
    "kingdom_cols = ['Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae', 'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae', 'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista', 'Riboviria']\n",
    "\n",
    "# 2. Split the original DataFrame by 'split'\n",
    "train_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'train']\n",
    "val_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'val']\n",
    "test_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'test']\n",
    "\n",
    "# 3. Extract glycans and labels directly\n",
    "glycan_train_list = train_df['IUPAC'].tolist()\n",
    "label_train_list = [train_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_train_list = list(zip(*label_train_list)) # Transpose\n",
    "\n",
    "glycan_val_list = val_df['IUPAC'].tolist()\n",
    "label_val_list = [val_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_val_list = list(zip(*label_val_list)) # Transpose\n",
    "\n",
    "glycan_test_list = test_df['IUPAC'].tolist()\n",
    "label_test_list = [test_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_test_list = list(zip(*label_test_list)) # Transpose\n",
    "\n",
    "print(f\"Number of training glycans: {len(glycan_train_list)}\")\n",
    "print(f\"Number of validation glycans: {len(glycan_val_list)}\")\n",
    "print(f\"Number of test glycans: {len(glycan_test_list)}\")\n",
    "print(f\"Shape of training labels: {len(label_train_list)} x {len(label_train_list[0]) if label_train_list else 0}\")\n",
    "print(f\"Shape of validation labels: {len(label_val_list)} x {len(label_val_list[0]) if label_val_list else 0}\")\n",
    "print(f\"Shape of test labels: {len(label_test_list)} x {len(label_test_list[0]) if label_test_list else 0}\")\n",
    "\n",
    "# Now re-run the checking function (modified for this new label extraction)\n",
    "def check_example_direct(glycan_list, label_list, split_name, original_df, kingdom_cols):\n",
    "    if glycan_list:\n",
    "        example_index = 0\n",
    "        example_glycan = glycan_list[example_index]\n",
    "        example_labels_split = list(label_list[example_index])\n",
    "\n",
    "        original_row = original_df[original_df['IUPAC'] == example_glycan].iloc[0]\n",
    "        labels_original = np.array([original_row[col] for col in kingdom_cols], dtype=np.float32).tolist()\n",
    "\n",
    "        print(f\"--- Checking example from {split_name} set (Direct) ---\")\n",
    "        print(f\"Glycan: {example_glycan}\")\n",
    "        print(f\"Split in original data: {original_row['split']}\")\n",
    "        print(f\"Labels in split data: {example_labels_split}\")\n",
    "        print(f\"Labels in original data: {labels_original}\")\n",
    "        if labels_original == example_labels_split:\n",
    "            print(\"Labels match!\")\n",
    "        else:\n",
    "            print(\"Labels DO NOT match!\")\n",
    "    else:\n",
    "        print(f\"{split_name} set is empty.\")\n",
    "\n",
    "check_example_direct(glycan_train_list, label_train_list, 'train', multilabel_kingdom_df_original, kingdom_cols)\n",
    "check_example_direct(glycan_val_list, label_val_list, 'val', multilabel_kingdom_df_original, kingdom_cols)\n",
    "check_example_direct(glycan_test_list, label_test_list, 'test', multilabel_kingdom_df_original, kingdom_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multilabel_kingdom_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check one of the graphs in the dataloader\n",
    "\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataLoader is called 'multilabel_kingdom_loader'\n",
    "inspected_loader = multilabel_kingdom_loaders_emb['train'] # Access the train DataLoader\n",
    "\n",
    "# Get the first batch of data\n",
    "try:\n",
    "    batch = next(iter(inspected_loader))\n",
    "    print(\"Batch:\", batch)\n",
    "    print(\"Number of graphs in batch:\", batch.num_graphs)\n",
    "\n",
    "    # Extract the first graph from the batch\n",
    "    first_graph_data = batch[2] #change to check other graphs\n",
    "    print(\"\\nFirst graph data:\", first_graph_data)\n",
    "    print(\"Node features (x):\", first_graph_data.x)\n",
    "    print(\"Edge indices (edge_index):\", first_graph_data.edge_index)\n",
    "    print(\"Labels (y):\", first_graph_data.y)\n",
    "    print(\"String labels:\", first_graph_data.string_labels)\n",
    "\n",
    "    # Convert the PyG Data object to a NetworkX graph for visualization\n",
    "    nx_graph = to_networkx(first_graph_data) \n",
    "\n",
    "    # Visualize the NetworkX graph\n",
    "    nx_graph = to_networkx(first_graph_data)\n",
    "    pos = nx.spring_layout(nx_graph)\n",
    "    nx.draw(nx_graph, pos, with_labels=True, node_size=500, node_color=\"red\", font_size=10, font_weight=\"bold\", arrows=False)\n",
    "    plt.title(\"First Graph from DataLoader (PyG)\")\n",
    "    plt.show()\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"The DataLoader is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87dc8a6",
   "metadata": {},
   "source": [
    "## Let's make a function to add embeddings to a dataloader object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the embeddings again here, so I don't have to jump up and down whenever I reload the kernel)\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            glm_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add GLM embeddings to a dictionary of dataloaders before loading them into the model\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def add_glm_embeddings_to_dataloaders(dataloaders, glm_embeddings):\n",
    "    embedded_loaders = {}\n",
    "    embedding_dim = 320\n",
    "    for split, loader in dataloaders.items():\n",
    "        embedded_data_list = []\n",
    "        for batch in loader:\n",
    "            for graph in batch.to_data_list():\n",
    "                node_embeddings = []\n",
    "                if hasattr(graph, 'string_labels'):\n",
    "                    for label in graph.string_labels:\n",
    "                        if label in glm_embeddings:\n",
    "                            embedding = glm_embeddings[label]\n",
    "                            node_embeddings.append(torch.tensor(embedding))\n",
    "                        else:\n",
    "                            node_embeddings.append(torch.zeros(embedding_dim))\n",
    "                    graph.x = torch.stack(node_embeddings).float()\n",
    "                    #print(f\"Shape of graph.x after adding embeddings: {graph.x.shape}\")\n",
    "                embedded_data_list.append(graph)\n",
    "\n",
    "        embedded_loaders[split] = DataLoader(embedded_data_list, batch_size=32)\n",
    "        print(f\"GLM embeddings added to {split} DataLoader.\")\n",
    "    return embedded_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b83ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the function\n",
    "\n",
    "glycan_loaders_emb = add_glm_embeddings_to_dataloaders(glycan_loaders, glm_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef713ec2",
   "metadata": {},
   "source": [
    "## Lets look at the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e13ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at string labels\n",
    "\n",
    "train_loader = multilabel_kingdom_loaders_emb['train'] \n",
    "batch = next(iter(train_loader))\n",
    "first_graph_labels = batch[2].string_labels\n",
    "print(\"Sample string_labels from the first graph:\")\n",
    "print(first_graph_labels[:20])  # Print the first 20 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6368a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the embeddings themselves\n",
    "\n",
    "train_loader = multilabel_kingdom_loaders_emb['train'] \n",
    "batch = next(iter(train_loader))\n",
    "first_graph_embeddings = batch[2].x\n",
    "print(\"Sample embeddings from the first graph:\")\n",
    "print(first_graph_embeddings[:20])  # Print the first 20 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4130ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample keys from glm_embeddings:\")\n",
    "print(list(glm_embeddings.keys())[:200])  # Print the first 20 keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fafdb1",
   "metadata": {},
   "source": [
    "## Copies of glycowork functions edited to use embeddings directly ||||run on kernel restart||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original SweetNet class from the glycowork package\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Union, Literal\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.nn import GraphConv\n",
    "    from torch_geometric.nn import global_mean_pool as gap\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "  raise ImportError(\"<torch or torch_geometric missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from glycowork.glycan_data.loader import lib, download_model \n",
    "\n",
    "class SweetNet(torch.nn.Module):\n",
    "    def __init__(self, lib_size: int, # number of unique tokens for graph nodes\n",
    "                 num_classes: int = 1, # number of output classes (>1 for multilabel)\n",
    "                 hidden_dim: int = 128 # dimension of hidden layers\n",
    "                ) -> None:\n",
    "        \"given glycan graphs as input, predicts properties via a graph neural network\"\n",
    "        super(SweetNet, self).__init__()\n",
    "        # Convolution operations on the graph\n",
    "        self.conv1 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GraphConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Node embedding\n",
    "        self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "        # Fully connected part\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, 1024)\n",
    "        self.lin2 = torch.nn.Linear(1024, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, num_classes)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(1024)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.act1 = torch.nn.LeakyReLU()\n",
    "        self.act2 = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor,\n",
    "                inference: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        # Getting node features\n",
    "        x = self.item_embedding(x)\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        # Graph convolution operations\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = gap(x, batch)\n",
    "\n",
    "        # Fully connected part\n",
    "        x = self.act1(self.bn1(self.lin1(x)))\n",
    "        x_out = self.bn2(self.lin2(x))\n",
    "        x = F.dropout(self.act2(x_out), p = 0.5, training = self.training)\n",
    "\n",
    "        x = self.lin3(x).squeeze(1)\n",
    "\n",
    "        if inference:\n",
    "          return x, x_out\n",
    "        else:\n",
    "          return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333ffcc",
   "metadata": {},
   "source": [
    "### Other functions I might modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57f2c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model function\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import torch\n",
    "    # Choose the correct computing architecture\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "    raise ImportError(\"<torch missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, mean_squared_error, \\\n",
    "    label_ranking_average_precision_score, ndcg_score, roc_auc_score, mean_absolute_error, r2_score\n",
    "from glycowork.motif.annotate import annotate_dataset\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 7, # epochs to wait after last improvement\n",
    "                 verbose: bool = False # whether to print messages\n",
    "                ) -> None:\n",
    "        \"Early stops the training if validation loss doesn't improve after a given patience\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = 0\n",
    "\n",
    "    def __call__(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), 'drive/My Drive/checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def sigmoid(x: float # input value\n",
    "          ) -> float: # sigmoid transformed value\n",
    "    \"Apply sigmoid transformation to input\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def disable_running_stats(model: torch.nn.Module # model to disable batch norm\n",
    "                       ) -> None:\n",
    "    \"Disable batch normalization running statistics\"\n",
    "\n",
    "    def _disable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "\n",
    "def enable_running_stats(model: torch.nn.Module # model to enable batch norm\n",
    "                      ) -> None:\n",
    "    \"Enable batch normalization running statistics\"\n",
    "\n",
    "    def _enable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, mean_squared_error, \\\n",
    "    label_ranking_average_precision_score, ndcg_score, roc_auc_score, mean_absolute_error, r2_score\n",
    "\n",
    "# Assuming sigmoid, EarlyStopping, disable_running_stats, enable_running_stats are defined in this module or imported.\n",
    "# Assuming init_weights is defined in this module or imported.\n",
    "# Assuming SweetNet class is defined or imported.\n",
    "# If they are defined elsewhere in this file/cell, their definitions should be above this function.\n",
    "# If they are imported from glycowork, ensure the import lines are uncommented at the top.\n",
    "\n",
    "# Example imports from glycowork.ml if needed and not defined locally (uncomment if you need them):\n",
    "# from glycowork.ml.model_training import EarlyStopping, sigmoid, disable_running_stats, enable_running_stats, training_setup\n",
    "# from glycowork.ml.model_training import init_weights\n",
    "# from glycowork.ml.models import SweetNet\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "# Assuming EarlyStopping, sigmoid, disable_running_stats, enable_running_stats are available or defined above this function.\n",
    "# Their definitions should be present in the code block you paste into your notebook.\n",
    "\n",
    "def train_model(model: torch.nn.Module,\n",
    "               dataloaders: Dict[str, torch.utils.data.DataLoader],\n",
    "               criterion: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "               num_epochs: int = 25,\n",
    "               patience: int = 50,\n",
    "               mode: str = 'classification',\n",
    "               mode2: str = 'multi',\n",
    "               return_metrics: bool = False,\n",
    "               use_external_embeddings: bool = False # Keep original flag if it's there\n",
    "              ) -> Union[torch.nn.Module, tuple[torch.nn.Module, dict[str, dict[str, list[float]]]]]:\n",
    "    \"Trains a deep learning model on predicting glycan properties.\"\n",
    "\n",
    "    since = time.time()\n",
    "    # Ensure EarlyStopping is available (its definition should be in the code you paste)\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True) # Use verbose=True for printing\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "    # Initialize best_lead_metric based on mode (higher is better for LRAP/Acc, lower for MSE)\n",
    "    best_lead_metric = -float(\"inf\") if mode in ['multilabel', 'classification'] else float(\"inf\")\n",
    "\n",
    "\n",
    "    # Define the metrics dictionary structure based on mode (keep original structure)\n",
    "    if mode == 'classification':\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": []}\n",
    "        if mode2 == 'binary':\n",
    "             blank_metrics[\"auroc\"] = []\n",
    "    elif mode == 'multilabel':\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": [], \"lrap\": [], \"ndcg\": []}\n",
    "    elif mode == 'regression':\n",
    "        blank_metrics = {\"loss\": [], \"mse\": [], \"mae\": [], \"r2\": []}\n",
    "    else:\n",
    "         print(f\"Warning: Unknown mode '{mode}'. Using default blank metrics.\")\n",
    "         blank_metrics = {\"loss\": []}\n",
    "\n",
    "    metrics = {\"train\": copy.deepcopy(blank_metrics), \"val\": copy.deepcopy(blank_metrics)}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                # --- Train phase: Keep original batch-wise metric calculation and averaging ---\n",
    "                # This part calculates and averages metrics per batch for the training phase.\n",
    "                # This is kept as per the likely original structure.\n",
    "                running_metrics = copy.deepcopy(blank_metrics)\n",
    "                running_metrics[\"weights\"] = []\n",
    "\n",
    "                for data in dataloaders[phase]:\n",
    "                    # Original data loading, moving to device, getting inputs (keep your existing code here)\n",
    "                    # Ensure your existing code gets x, y, edge_index, batch, prot and moves them to 'device' correctly\n",
    "                    # Example for SweetNet and PyG DataBatch (keep your actual code):\n",
    "                    # data = data.to(device)\n",
    "                    # x, y, edge_index, batch = data.x, data.y, data.edge_index, data.batch\n",
    "                    # prot = getattr(data, 'train_idx', None)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(True): # Always enable gradients in train phase\n",
    "                         # Forward pass (keep your existing code)\n",
    "                         # Ensure inputs to model are on the correct device.\n",
    "                         # Example: pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                         pass # Your forward pass and loss calculation logic here\n",
    "\n",
    "                         # Example loss calculation: loss = criterion(pred, y)\n",
    "\n",
    "\n",
    "                         if phase == 'train':\n",
    "                               # Backward pass and optimizer step (keep your existing code)\n",
    "                               # Example: loss.backward(); optimizer.step()\n",
    "                               pass # Your backward and optimizer step here\n",
    "\n",
    "\n",
    "                    # Collecting relevant metrics for train phase (keep original logic)\n",
    "                    # This calculates metrics per batch for the training phase.\n",
    "                    # Example (keep your actual code):\n",
    "                    # if loss is not None: running_metrics[\"loss\"].append(loss.item())\n",
    "                    # if batch is not None: running_metrics[\"weights\"].append(batch.max().item() + 1) # Or y.size(0)\n",
    "\n",
    "\n",
    "                    # Example metric calculations per batch:\n",
    "                    # y_det = y.detach().cpu().numpy() if y is not None else None\n",
    "                    # pred_det = pred.cpu().detach().numpy() if pred is not None else None\n",
    "                    # if y_det is not None and pred_det is not None:\n",
    "                    #     if mode == 'multilabel':\n",
    "                    #          pred_proba = sigmoid(pred_det)\n",
    "                    #          running_metrics[\"lrap\"].append(label_ranking_average_precision_score(y_det.astype(float), pred_proba))\n",
    "                    #          # ... other metrics like ndcg, acc, mcc\n",
    "                    pass # Your metric calculation logic here\n",
    "\n",
    "                # Averaging metrics at end of train phase (keep original logic)\n",
    "                # This averages the batch-wise metrics collected above.\n",
    "                for key in running_metrics:\n",
    "                    if key == \"weights\": continue\n",
    "                    if running_metrics[key]:\n",
    "                        metrics[phase][key].append(np.average(running_metrics[key], weights = running_metrics[\"weights\"] if running_metrics[\"weights\"] else None))\n",
    "                    else:\n",
    "                         metrics[phase][key].append(np.nan) # Append NaN if no data\n",
    "\n",
    "\n",
    "            else: # Validation phase - MINIMAL MODIFICATION to calculate metrics once per epoch over the whole set\n",
    "                model.eval() # Set model to evaluation mode\n",
    "\n",
    "                # --- MINIMAL FIX: Initialize lists to collect data for epoch-level metrics ---\n",
    "                all_val_preds = [] # List to collect predictions from all batches\n",
    "                all_val_labels = [] # List to collect true labels from all batches\n",
    "                val_running_loss = 0.0 # Accumulate loss across batches\n",
    "                total_val_samples = 0 # Count total samples across batches\n",
    "\n",
    "                for data in dataloaders[phase]:\n",
    "                     # Original data loading, moving to device, getting inputs (keep your existing code here)\n",
    "                     # Ensure your existing code gets x, y, edge_index, batch, prot and moves them to 'device' correctly.\n",
    "                     # Example for SweetNet and PyG DataBatch (keep your actual code):\n",
    "                     # data = data.to(device)\n",
    "                     # x, y, edge_index, batch = data.x, data.y, data.edge_index, data.batch\n",
    "                     # prot = getattr(data, 'train_idx', None)\n",
    "\n",
    "                     # --- MINIMAL FIX: Ensure necessary inputs are available and on device ---\n",
    "                     # Add basic checks if your existing data loading might result in None or wrong device\n",
    "                     if x is None or y is None or edge_index is None or batch is None:\n",
    "                         continue # Skip batch if essential data is missing\n",
    "\n",
    "                     # Assuming x, y, edge_index, batch are on 'device' here\n",
    "\n",
    "                     with torch.no_grad(): # Ensure no gradients in validation\n",
    "                          # Forward pass (keep your existing code)\n",
    "                          # Ensure inputs to model are on the same device.\n",
    "                          # Example: pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                          pass # Your forward pass and loss calculation logic here\n",
    "\n",
    "                          # Example loss calculation: loss = criterion(pred, y)\n",
    "\n",
    "\n",
    "                     # --- MINIMAL FIX: Collect predictions and labels, accumulate loss and samples ---\n",
    "                     if loss is not None:\n",
    "                         # Accumulate loss (loss.item() gets the scalar value)\n",
    "                         val_running_loss += loss.item() * y.size(0) # Accumulate sum of losses (loss is usually mean over batch)\n",
    "\n",
    "                         # Accumulate number of samples\n",
    "                         total_val_samples += y.size(0) # Assuming y.size(0) gives the number of samples in the batch\n",
    "\n",
    "\n",
    "                     if pred is not None and y is not None:\n",
    "                          # Store predictions and labels (move to CPU and convert to numpy)\n",
    "                          # Ensure shapes are compatible for concatenation later (e.g., (batch_size, num_classes))\n",
    "                          if mode == 'multilabel':\n",
    "                               # Get probabilities for LRAP/NDCG\n",
    "                               val_pred_proba_batch = sigmoid(pred.cpu().numpy()) # Get probabilities on CPU\n",
    "                               y_labels_batch = y.cpu().numpy() # Get labels on CPU\n",
    "\n",
    "                               # Ensure shapes are (batch_size, num_classes) for storage\n",
    "                               if val_pred_proba_batch.ndim == 1: val_pred_proba_batch = val_pred_proba_batch.reshape(-1, 1)\n",
    "                               if y_labels_batch.ndim == 1: y_labels_batch = y_labels_batch.reshape(-1, 1)\n",
    "\n",
    "\n",
    "                               all_val_preds.append(val_pred_proba_batch) # Append probabilities\n",
    "                               all_val_labels.append(y_labels_batch) # Append labels\n",
    "                          # ... (add elif for other modes if needed, collect predictions/labels in numpy) ...\n",
    "\n",
    "\n",
    "                # --- After iterating through all batches in the validation set ---\n",
    "\n",
    "                # --- MINIMAL FIX: Calculate metrics once over the entire validation set ---\n",
    "                val_metrics = {} # Dictionary to store epoch-level validation metrics\n",
    "\n",
    "                # Calculate epoch-level validation loss (average over collected samples)\n",
    "                val_epoch_loss = val_running_loss / total_val_samples if total_val_samples > 0 else np.nan\n",
    "                val_metrics[\"loss\"] = val_epoch_loss # Store the epoch loss\n",
    "\n",
    "\n",
    "                if all_val_preds and all_val_labels: # Check if any data was collected\n",
    "                    try:\n",
    "                         # Concatenate predictions and labels from all batches collected\n",
    "                         all_val_preds = np.concatenate(all_val_preds, axis=0)\n",
    "                         all_val_labels = np.concatenate(all_val_labels, axis=0)\n",
    "\n",
    "                         # --- Calculate metrics over the full validation set ---\n",
    "                         if mode == 'multilabel':\n",
    "                              # all_val_preds are already probabilities (shape N_total, num_classes)\n",
    "                              # all_val_labels are true labels (shape N_total, num_classes)\n",
    "\n",
    "                              # Calculate LRAP and NDCG over the full validation set\n",
    "                              # Ensure label type is correct for metrics (float for LRAP, int for NDCG)\n",
    "                              # Ensure shapes are (n_samples, n_labels) for sklearn metrics\n",
    "                              if all_val_labels.ndim == 2 and all_val_preds.ndim == 2 and all_val_labels.shape[0] == all_val_preds.shape[0] and all_val_labels.shape[1] == all_val_preds.shape[1]:\n",
    "                                   val_metrics[\"lrap\"] = label_ranking_average_precision_score(all_val_labels.astype(float), all_val_preds)\n",
    "                                   val_metrics[\"ndcg\"] = ndcg_score(all_val_labels.astype(int), all_val_preds)\n",
    "\n",
    "                                   # Keep acc/mcc calculation if in blank_metrics and you need them calculated over the whole set\n",
    "                                   # Example: val_pred_binary = (all_val_preds >= 0.5).astype(int)\n",
    "                                   # if \"acc\" in blank_metrics: val_metrics[\"acc\"] = accuracy_score(all_val_labels.astype(int), val_pred_binary)\n",
    "                                   # if \"mcc\" in blank_metrics: val_metrics[\"mcc\"] = matthews_corrcoef(all_val_labels.flatten().astype(int), val_pred_binary.flatten().astype(int))\n",
    "\n",
    "                              else:\n",
    "                                   # Shape mismatch after concatenation, store NaNs\n",
    "                                   val_metrics[\"lrap\"] = np.nan\n",
    "                                   val_metrics[\"ndcg\"] = np.nan\n",
    "                                   # if \"acc\" in blank_metrics: val_metrics[\"acc\"] = np.nan\n",
    "                                   # if \"mcc\" in blank_metrics: val_metrics[\"mcc\"] = np.nan\n",
    "                                   print(f\"Warning: Shape mismatch after concatenating data for val metrics in epoch {epoch}. Skipping metrics.\")\n",
    "\n",
    "\n",
    "                         # ... (add elif for other modes like 'classification' or 'regression' if needed, calculate metrics over all_val_labels/all_val_preds) ...\n",
    "\n",
    "                    except Exception as e:\n",
    "                         # Handle errors during concatenation or metric calculation over the whole set\n",
    "                         print(f\"Warning: Error calculating validation metrics over full epoch data for epoch {epoch}: {e}. Storing NaNs.\")\n",
    "                         # Ensure all metrics expected in blank_metrics for this mode get a value (NaN)\n",
    "                         for key in blank_metrics.keys():\n",
    "                              if key not in val_metrics: val_metrics[key] = np.nan # Avoid overwriting loss if calculated\n",
    "\n",
    "\n",
    "                else:\n",
    "                     # Handle case where all_val_preds or all_val_labels are empty (e.g., all batches skipped)\n",
    "                     print(f\"Warning: No data processed in validation phase for epoch {epoch}. Storing NaNs for metrics.\")\n",
    "                     # Ensure all metrics expected in blank_metrics for this mode get a value (NaN)\n",
    "                     for key in blank_metrics.keys():\n",
    "                          if key not in val_metrics: metrics[phase][key] = [] # Ensure list exists\n",
    "                          metrics[phase][key].append(np.nan) # Append NaN if no data collected\n",
    "\n",
    "\n",
    "                # Store the epoch-level validation metrics\n",
    "                # Append values from val_metrics to metrics[phase][key] lists.\n",
    "                for key, value in val_metrics.items():\n",
    "                    if key not in metrics[phase]: # Ensure list exists for keys\n",
    "                         metrics[phase][key] = []\n",
    "                    metrics[phase][key].append(value)\n",
    "\n",
    "\n",
    "                # --- Print epoch-level validation metrics ---\n",
    "                # This print statement uses metrics[phase][\"lrap\"][-1], which is now the correctly calculated epoch-level LRAP\n",
    "                if mode == 'multilabel':\n",
    "                     # Print the epoch-level calculated metrics\n",
    "                     print('{} Loss: {:.4f} LRAP: {:.4f} NDCG: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"lrap\"][-1], metrics[phase][\"ndcg\"][-1]))\n",
    "\n",
    "                elif mode == 'classification':\n",
    "                     # Print epoch-level classification metrics (assuming calculated and stored)\n",
    "                     print('{} Loss: {:.4f} Accuracy: {:.4f} MCC: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "                elif mode == 'regression':\n",
    "                     # Print epoch-level regression metrics (assuming calculated and stored)\n",
    "                     print('{} Loss: {:.4f} MSE: {:.4f} MAE: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"mse\"][-1], metrics[phase][\"mae\"][-1]))\n",
    "                else:\n",
    "                     # Fallback print if mode is unknown\n",
    "                     # print(f\"Warning: Unknown mode '{mode}' in print statement.\") # Debug print\n",
    "                     print('{} Loss: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1]))\n",
    "\n",
    "\n",
    "                # Keep best model state_dict based on loss (keep original logic)\n",
    "                # Check if validation loss is available and not NaN\n",
    "                if metrics[phase][\"loss\"] and not np.isnan(metrics[phase][\"loss\"][-1]):\n",
    "                     if metrics[phase][\"loss\"][-1] <= best_loss:\n",
    "                         best_loss = metrics[phase][\"loss\"][-1]\n",
    "                         best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                         # Extract the lead metric corresponding to the best loss epoch (keep original logic)\n",
    "                         # Use the accurately calculated epoch-level metric for the best loss epoch\n",
    "                         if mode == 'multilabel':\n",
    "                             # Ensure metrics[phase][\"lrap\"] is available and not NaN for this epoch\n",
    "                             if metrics[phase][\"lrap\"] and not np.isnan(metrics[phase][\"lrap\"][-1]):\n",
    "                                  best_lead_metric = metrics[phase][\"lrap\"][-1]\n",
    "                             elif metrics[phase][\"lrap\"]: pass # If list exists but value is NaN, keep previous best\n",
    "                             else: best_lead_metric = -float(\"inf\") # Reset if list is empty (shouldn't happen with append)\n",
    "\n",
    "                         elif mode == 'classification':\n",
    "                             if metrics[phase][\"acc\"] and not np.isnan(metrics[phase][\"acc\"][-1]):\n",
    "                                  best_lead_metric = metrics[phase][\"acc\"][-1]\n",
    "                             elif metrics[phase][\"acc\"]: pass\n",
    "                             else: best_lead_metric = -float(\"inf\") # For higher is better\n",
    "\n",
    "                         elif mode == 'regression':\n",
    "                             if metrics[phase][\"mse\"] and not np.isnan(metrics[phase][\"mse\"][-1]):\n",
    "                                  best_lead_metric = metrics[phase][\"mse\"][-1]\n",
    "                             elif metrics[phase][\"mse\"]: pass\n",
    "                             else: best_lead_metric = float(\"inf\") # For lower is better\n",
    "\n",
    "\n",
    "                # Check Early Stopping & adjust learning rate (keep original logic)\n",
    "                # Ensure early_stopping is not None and validation loss is available and not NaN\n",
    "                if early_stopping is not None and metrics[phase][\"loss\"] and not np.isnan(metrics[phase][\"loss\"][-1]):\n",
    "                    # Early stopping uses val_loss, which is the epoch-level calculated loss\n",
    "                    early_stopping(metrics[phase][\"loss\"][-1], model)\n",
    "                    # Scheduler step (keep original logic)\n",
    "                    # Ensure scheduler is not None\n",
    "                    if isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler):\n",
    "                        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                             # ReduceLROnPlateau needs the metric (loss in this case)\n",
    "                             scheduler.step(metrics[phase][\"loss\"][-1])\n",
    "                        elif scheduler is not None: # Step other schedulers every epoch regardless of early stopping/loss if not ReduceLROnPlateau\n",
    "                             scheduler.step()\n",
    "                # If early_stopping is None, scheduler step happens here (keep original logic)\n",
    "                # Also step scheduler if early stopping did NOT trigger early_stop\n",
    "                elif early_stopping is None and isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler):\n",
    "                     if scheduler is not None:\n",
    "                         scheduler.step()\n",
    "\n",
    "\n",
    "        # --- End of train/val phase loop ---\n",
    "\n",
    "        # Check if early stopping is triggered (keep original logic)\n",
    "        # This check is after the inner train/val loop, which is correct.\n",
    "        if early_stopping is not None and early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break # Break the main epoch loop\n",
    "\n",
    "\n",
    "        # Print blank line after each epoch's phase reports\n",
    "        print()\n",
    "\n",
    "\n",
    "    # --- Final reporting ---\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    # Print the best metrics found during training (should be accurate now)\n",
    "    if mode == 'multilabel':\n",
    "        # Print the best LRAP recorded (corresponding to the best loss epoch)\n",
    "        # Handle case where best_lead_metric might be NaN\n",
    "        if not np.isnan(best_lead_metric):\n",
    "             print('Best val loss: {:4f}, best LRAP score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "        elif metrics[\"val\"][\"lrap\"]: # Fallback if best_lead_metric is NaN but some LRAP values exist\n",
    "             print('Best val loss: {:4f}, best LRAP score: {:.4f} (Note: Best LRAP based on loss was NaN, showing max available)'.format(best_loss, np.nanmax(metrics[\"val\"][\"lrap\"]))) # Print max non-NaN LRAP if available\n",
    "        else:\n",
    "             print('Best val loss: {:4f}, best LRAP score: NaN (No valid LRAP recorded)'.format(best_loss))\n",
    "\n",
    "\n",
    "    elif mode == 'classification':\n",
    "         if not np.isnan(best_lead_metric):\n",
    "             print('Best val loss: {:4f}, best Accuracy score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "         elif metrics[\"val\"][\"acc\"]:\n",
    "              print('Best val loss: {:4f}, best Accuracy score: {:.4f} (Note: Best Acc based on loss was NaN, showing max available)'.format(best_loss, np.nanmax(metrics[\"val\"][\"acc\"])))\n",
    "         else:\n",
    "              print('Best val loss: {:4f}, best Accuracy score: NaN (No valid Acc recorded)'.format(best_loss))\n",
    "\n",
    "    elif mode == 'regression':\n",
    "        if not np.isnan(best_lead_metric):\n",
    "             print('Best val loss: {:4f}, best MSE score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "        elif metrics[\"val\"][\"mse\"]:\n",
    "              print('Best val loss: {:4f}, best MSE score: {:.4f} (Note: Best MSE based on loss was NaN, showing min available)'.format(best_loss, np.nanmin(metrics[\"val\"][\"mse\"]))) # Use nanmin for MSE\n",
    "        else:\n",
    "             print('Best val loss: {:4f}, best MSE score: NaN (No valid MSE recorded)'.format(best_loss))\n",
    "\n",
    "    else:\n",
    "        print('Best val loss: {:4f}, best Lead Metric score: {:.4f} (Mode not explicitly handled in final print)'.format(best_loss, best_lead_metric)) # Fallback\n",
    "\n",
    "    model.load_state_dict(best_model_wts) # Load the model weights from the best epoch (based on loss)\n",
    "\n",
    "\n",
    "    # --- Return model and/or metrics ---\n",
    "    if return_metrics:\n",
    "        return model, metrics # Return the best model and the collected epoch-level metrics\n",
    "    else:\n",
    "        return model # Return the best model\n",
    "\n",
    "\n",
    "    # --- Plotting code (commented out as per previous decision) ---\n",
    "    # If return_metrics is False, the plotting code would execute here.\n",
    "    # ... (commented out plotting code) ...\n",
    "\n",
    "    # Plot loss & score over the course of training\n",
    "    _, _ = plt.subplots(nrows=2, ncols=1)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(epoch + 1), metrics[\"val\"][\"loss\"])\n",
    "    plt.title('Model Training')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.legend(['Validation Loss'], loc='best')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    if mode == 'classification':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"acc\"])\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "        plt.legend(['Validation Accuracy'], loc='best')\n",
    "    elif mode == 'multilabel':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"lrap\"])\n",
    "        plt.ylabel('Validation LRAP')\n",
    "        plt.legend(['Validation LRAP'], loc='best')\n",
    "    else:\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"mse\"])\n",
    "        plt.ylabel('Validation MSE')\n",
    "        plt.legend(['Validation MSE'], loc='best')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init_weights function\n",
    "\n",
    "def init_weights(model: torch.nn.Module, # neural network for analyzing glycans\n",
    "                mode: str = 'sparse', # initialization algorithm: 'sparse', 'kaiming', 'xavier'\n",
    "                sparsity: float = 0.1 # proportion of sparsity after initialization\n",
    "               ) -> None:\n",
    "    \"initializes linear layers of PyTorch model with a weight initialization\"\n",
    "    #print(\"Using init_weights from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if isinstance(model, torch.nn.Linear):\n",
    "        if mode == 'sparse':\n",
    "            torch.nn.init.sparse_(model.weight, sparsity = sparsity)\n",
    "        elif mode == 'kaiming':\n",
    "            torch.nn.init.kaiming_uniform_(model.weight)\n",
    "        elif mode == 'xavier':\n",
    "            torch.nn.init.xavier_uniform_(model.weight)\n",
    "        else:\n",
    "            print(\"This initialization option is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e02dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_model function\n",
    "\n",
    "def prep_model(model_type: Literal[\"SweetNet\", \"LectinOracle\", \"LectinOracle_flex\", \"NSequonPred\"], # type of model to create\n",
    "              num_classes: int, # number of unique classes for classification\n",
    "              libr: Optional[Dict[str, int]] = None, # dictionary of form glycoletter:index\n",
    "              trained: bool = False, # whether to use pretrained model\n",
    "              # set hidden_dim to 320 rather than 128 for the pretrained model\n",
    "              # but 128 is the default for the model in the paper\n",
    "              hidden_dim: int = 320, # hidden dimension for the model (SweetNet/LectinOracle only)\n",
    "              use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "             ) -> torch.nn.Module: # initialized PyTorch model\n",
    "    \"wrapper to instantiate model, initialize it, and put it on the GPU\"\n",
    "    #print(\"Using prep_model from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if libr is None:\n",
    "      libr = lib\n",
    "    if model_type == 'SweetNet':\n",
    "      model = SweetNet(len(libr), num_classes = num_classes, hidden_dim = hidden_dim, use_external_embeddings = use_external_embeddings)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'sparse'))\n",
    "      if trained:\n",
    "        if hidden_dim != 128:\n",
    "          raise ValueError(\"Hidden dimension must be 128 for pretrained model\")\n",
    "        model_path = download_model(\"glycowork_sweetnet_species.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle':\n",
    "      model = LectinOracle(len(libr), num_classes = num_classes, input_size_prot = int(10*hidden_dim))\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle_flex':\n",
    "      model = LectinOracle_flex(len(libr), num_classes = num_classes)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle_flex.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'NSequonPred':\n",
    "      model = NSequonPred()\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"NSequonPred_batch32.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    else:\n",
    "      print(\"Invalid Model Type\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2749ab38",
   "metadata": {},
   "source": [
    "## Copies of glycowork functions edited for glm embedded dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SweetNet class\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Union, Literal\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.nn import GraphConv\n",
    "    from torch_geometric.nn import global_mean_pool as gap\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "  raise ImportError(\"<torch or torch_geometric missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from glycowork.glycan_data.loader import lib, download_model \n",
    "\n",
    "class SweetNet(torch.nn.Module):\n",
    "    def __init__(self, lib_size: int, # number of unique tokens for graph nodes\n",
    "                 num_classes: int = 1, # number of output classes (>1 for multilabel)\n",
    "                 hidden_dim: int = 320, # dimension of hidden layers (changed from 128 to 320)\n",
    "                 use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "                ) -> None:\n",
    "        \"given glycan graphs as input, predicts properties via a graph neural network\"\n",
    "        #print(\"Using SweetNet from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "        super(SweetNet, self).__init__()\n",
    "        self.use_external_embeddings = use_external_embeddings\n",
    "        # Convolution operations on the graph\n",
    "        self.conv1 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GraphConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Node embedding\n",
    "        if use_external_embeddings:\n",
    "            self.embedding_dim = hidden_dim\n",
    "        else:\n",
    "            self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    " \n",
    "\n",
    "        #self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    "        # Fully connected part\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, 1024)\n",
    "        self.lin2 = torch.nn.Linear(1024, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, num_classes)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(1024)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.act1 = torch.nn.LeakyReLU()\n",
    "        self.act2 = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor,\n",
    "                inference: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \n",
    "        # Getting node features\n",
    "        if self.use_external_embeddings:\n",
    "           # Use external embeddings (already in x)\n",
    "            pass # x is already the embeddings\n",
    "        else:\n",
    "             # Use internal embedding\n",
    "            x = self.item_embedding(x).squeeze(1)\n",
    "            \n",
    "        \n",
    "        # Graph convolution operations (now using GLM embeddings as x)\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = gap(x, batch)\n",
    "\n",
    "        # Fully connected part\n",
    "        x = self.act1(self.bn1(self.lin1(x)))\n",
    "        x_out = self.bn2(self.lin2(x))\n",
    "        x = F.dropout(self.act2(x_out), p = 0.5, training = self.training)\n",
    "\n",
    "        x = self.lin3(x).squeeze(1)\n",
    "\n",
    "        if inference:\n",
    "          return x, x_out\n",
    "        else:\n",
    "          return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03900750",
   "metadata": {},
   "source": [
    "### Other functions I might modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6eccb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model function\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import torch\n",
    "    # Choose the correct computing architecture\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "    raise ImportError(\"<torch missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, mean_squared_error, \\\n",
    "    label_ranking_average_precision_score, ndcg_score, roc_auc_score, mean_absolute_error, r2_score\n",
    "from glycowork.motif.annotate import annotate_dataset\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 7, # epochs to wait after last improvement\n",
    "                 verbose: bool = False # whether to print messages\n",
    "                ) -> None:\n",
    "        \"Early stops the training if validation loss doesn't improve after a given patience\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = 0\n",
    "\n",
    "    def __call__(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), 'drive/My Drive/checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def sigmoid(x: float # input value\n",
    "          ) -> float: # sigmoid transformed value\n",
    "    \"Apply sigmoid transformation to input\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def disable_running_stats(model: torch.nn.Module # model to disable batch norm\n",
    "                       ) -> None:\n",
    "    \"Disable batch normalization running statistics\"\n",
    "\n",
    "    def _disable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "\n",
    "def enable_running_stats(model: torch.nn.Module # model to enable batch norm\n",
    "                      ) -> None:\n",
    "    \"Enable batch normalization running statistics\"\n",
    "\n",
    "    def _enable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def train_model(model: torch.nn.Module, # graph neural network for analyzing glycans\n",
    "               dataloaders: Dict[str, torch.utils.data.DataLoader], # dict with 'train' and 'val' loaders\n",
    "               criterion: torch.nn.Module, # PyTorch loss function\n",
    "               optimizer: torch.optim.Optimizer, # PyTorch optimizer, has to be SAM if mode != \"regression\"\n",
    "               scheduler: torch.optim.lr_scheduler._LRScheduler, # PyTorch learning rate decay\n",
    "               num_epochs: int = 25, # number of epochs for training\n",
    "               patience: int = 50, # epochs without improvement until early stop\n",
    "               mode: str = 'classification', # 'classification', 'multilabel', or 'regression'\n",
    "               mode2: str = 'multi', # 'multi' or 'binary' classification\n",
    "               return_metrics: bool = False, # whether to return metrics\n",
    "               use_external_embeddings: bool = False # whether to use external embeddings\n",
    "              ) -> Union[torch.nn.Module, tuple[torch.nn.Module, dict[str, dict[str, list[float]]]]]: # best model from training and the training and validation metrics\n",
    "    \"trains a deep learning model on predicting glycan properties\"\n",
    "\n",
    "    since = time.time()\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "    best_lead_metric = float(\"inf\")\n",
    "\n",
    "    if mode == 'classification':\n",
    "         # Removed auroc from metrics to avoid calculation issues for now\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": []}\n",
    "    elif mode == 'multilabel':\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": [], \"lrap\": [], \"ndcg\": []}\n",
    "    else:\n",
    "        blank_metrics = {\"loss\": [], \"mse\": [], \"mae\": [], \"r2\": []}\n",
    "\n",
    "    metrics = {\"train\": copy.deepcopy(blank_metrics), \"val\": copy.deepcopy(blank_metrics)}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_metrics = copy.deepcopy(blank_metrics)\n",
    "            running_metrics[\"weights\"] = []\n",
    "\n",
    "            for data in dataloaders[phase]:\n",
    "                # Get all relevant node attributes\n",
    "                #print(f\"Phase: {phase}, Data: {data}\")\n",
    "                #print(f\"Phase: {phase}, Data.x: {getattr(data, 'x', None)}\") # Check if x exists and its value\n",
    "                if use_external_embeddings:\n",
    "                    x, y, edge_index, batch = data.x, data.y, data.edge_index, data.batch\n",
    "                else:\n",
    "                    x, y, edge_index, batch = data.labels, data.y, data.edge_index, data.batch\n",
    "                prot = getattr(data, 'train_idx', None)\n",
    "                if prot is not None:\n",
    "                    prot = prot.view(max(batch) + 1, -1).to(device)\n",
    "                x = x.to(device)\n",
    "                if mode == 'multilabel':\n",
    "                    y = y.view(max(batch) + 1, -1).to(device)\n",
    "                else:\n",
    "                    y = y.to(device)\n",
    "                y = y.view(-1, 1) if mode == 'regression' else y\n",
    "                edge_index = edge_index.to(device)\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # First forward pass\n",
    "                    if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                        enable_running_stats(model)\n",
    "                    pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                    loss = criterion(pred, y)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                            optimizer.first_step(zero_grad = True)\n",
    "                            # Second forward pass\n",
    "                            disable_running_stats(model)\n",
    "                            second_pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                            criterion(second_pred, y).backward()\n",
    "                            optimizer.second_step(zero_grad = True)\n",
    "                        else:\n",
    "                            optimizer.step()\n",
    "                \n",
    "                # Check for single-class batches\n",
    "                unique_classes = torch.unique(y).cpu().numpy()\n",
    "                if len(unique_classes) == 1:\n",
    "                    print(f\"WARNING: Single-class batch detected in {phase} phase at epoch {epoch}!\")\n",
    "                    print(f\"Batch labels: {unique_classes}\")\n",
    "\n",
    "                # Collecting relevant metrics\n",
    "                running_metrics[\"loss\"].append(loss.item())\n",
    "                running_metrics[\"weights\"].append(batch.max().cpu() + 1)\n",
    "\n",
    "                y_det = y.detach().cpu().numpy()\n",
    "                pred_det = pred.cpu().detach().numpy()\n",
    "                if mode == 'classification':\n",
    "                    if mode2 == 'multi':\n",
    "                        pred_proba = np.exp(pred_det) / np.sum(np.exp(pred_det), axis = 1, keepdims = True)  # numpy softmax\n",
    "                        pred2 = np.argmax(pred_det, axis = 1)\n",
    "                    else:\n",
    "                        pred_proba = sigmoid(pred_det)\n",
    "                        pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det, pred2))\n",
    "                    # commented out auroc because it was throwing errors\n",
    "                    #running_metrics[\"auroc\"].append(roc_auc_score(y_det.astype(int), pred_proba if mode2 == 'binary' else pred_proba[:, 1]))\n",
    "                elif mode == 'multilabel':\n",
    "                    pred_proba = sigmoid(pred_det)\n",
    "                    pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det.flatten(), pred2.flatten()))\n",
    "                    running_metrics[\"lrap\"].append(label_ranking_average_precision_score(y_det.astype(int), pred_proba))\n",
    "                    running_metrics[\"ndcg\"].append(ndcg_score(y_det.astype(int), pred_proba))\n",
    "                else:\n",
    "                    running_metrics[\"mse\"].append(mean_squared_error(y_det, pred_det))\n",
    "                    running_metrics[\"mae\"].append(mean_absolute_error(y_det, pred_det))\n",
    "                    running_metrics[\"r2\"].append(r2_score(y_det, pred_det))\n",
    "\n",
    "            # Averaging metrics at end of epoch\n",
    "            for key in running_metrics:\n",
    "                if key == \"weights\":\n",
    "                    continue\n",
    "                metrics[phase][key].append(np.average(running_metrics[key], weights = running_metrics[\"weights\"]))\n",
    "\n",
    "            if mode == 'classification':\n",
    "                print('{} Loss: {:.4f} Accuracy: {:.4f} MCC: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            elif mode == 'multilabel':\n",
    "                print('{} Loss: {:.4f} LRAP: {:.4f} NDCG: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            else:\n",
    "                print('{} Loss: {:.4f} MSE: {:.4f} MAE: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"mse\"][-1], metrics[phase][\"mae\"][-1]))\n",
    "\n",
    "            # Keep best model state_dict\n",
    "            if phase == \"val\":\n",
    "                if metrics[phase][\"loss\"][-1] <= best_loss:\n",
    "                    best_loss = metrics[phase][\"loss\"][-1]\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                    # Extract the lead metric (ACC, LRAP, or MSE) of the new best model\n",
    "                    if mode == 'classification':\n",
    "                        best_lead_metric = metrics[phase][\"acc\"][-1]\n",
    "                    elif mode == 'multilabel':\n",
    "                        best_lead_metric = metrics[phase][\"lrap\"][-1]\n",
    "                    else:\n",
    "                        best_lead_metric = metrics[phase][\"mse\"][-1]\n",
    "\n",
    "                # Check Early Stopping & adjust learning rate if needed\n",
    "                early_stopping(metrics[phase][\"loss\"][-1], model)\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(metrics[phase][\"loss\"][-1])\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    if mode == 'classification':\n",
    "        print('Best val loss: {:4f}, best Accuracy score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    elif mode == 'multilabel':\n",
    "        print('Best val loss: {:4f}, best LRAP score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    else:\n",
    "        print('Best val loss: {:4f}, best MSE score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    if return_metrics:\n",
    "        return model, metrics\n",
    "\n",
    "    # Plot loss & score over the course of training\n",
    "    _, _ = plt.subplots(nrows=2, ncols=1)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(epoch + 1), metrics[\"val\"][\"loss\"])\n",
    "    plt.title('Model Training')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.legend(['Validation Loss'], loc='best')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    if mode == 'classification':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"acc\"])\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "        plt.legend(['Validation Accuracy'], loc='best')\n",
    "    elif mode == 'multilabel':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"lrap\"])\n",
    "        plt.ylabel('Validation LRAP')\n",
    "        plt.legend(['Validation LRAP'], loc='best')\n",
    "    else:\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"mse\"])\n",
    "        plt.ylabel('Validation MSE')\n",
    "        plt.legend(['Validation MSE'], loc='best')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a375b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init_weights function\n",
    "\n",
    "def init_weights(model: torch.nn.Module, # neural network for analyzing glycans\n",
    "                mode: str = 'sparse', # initialization algorithm: 'sparse', 'kaiming', 'xavier'\n",
    "                sparsity: float = 0.1 # proportion of sparsity after initialization\n",
    "               ) -> None:\n",
    "    \"initializes linear layers of PyTorch model with a weight initialization\"\n",
    "    #print(\"Using init_weights from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if isinstance(model, torch.nn.Linear):\n",
    "        if mode == 'sparse':\n",
    "            torch.nn.init.sparse_(model.weight, sparsity = sparsity)\n",
    "        elif mode == 'kaiming':\n",
    "            torch.nn.init.kaiming_uniform_(model.weight)\n",
    "        elif mode == 'xavier':\n",
    "            torch.nn.init.xavier_uniform_(model.weight)\n",
    "        else:\n",
    "            print(\"This initialization option is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_model function\n",
    "\n",
    "def prep_model(model_type: Literal[\"SweetNet\", \"LectinOracle\", \"LectinOracle_flex\", \"NSequonPred\"], # type of model to create\n",
    "              num_classes: int, # number of unique classes for classification\n",
    "              libr: Optional[Dict[str, int]] = None, # dictionary of form glycoletter:index\n",
    "              trained: bool = False, # whether to use pretrained model\n",
    "              # set hidden_dim to 320 rather than 128 for the pretrained model\n",
    "              # but 128 is the default for the model in the paper\n",
    "              hidden_dim: int = 320, # hidden dimension for the model (SweetNet/LectinOracle only)\n",
    "              use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "             ) -> torch.nn.Module: # initialized PyTorch model\n",
    "    \"wrapper to instantiate model, initialize it, and put it on the GPU\"\n",
    "    #print(\"Using prep_model from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if libr is None:\n",
    "      libr = lib\n",
    "    if model_type == 'SweetNet':\n",
    "      model = SweetNet(len(libr), num_classes = num_classes, hidden_dim = hidden_dim, use_external_embeddings = use_external_embeddings)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'sparse'))\n",
    "      if trained:\n",
    "        if hidden_dim != 128:\n",
    "          raise ValueError(\"Hidden dimension must be 128 for pretrained model\")\n",
    "        model_path = download_model(\"glycowork_sweetnet_species.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle':\n",
    "      model = LectinOracle(len(libr), num_classes = num_classes, input_size_prot = int(10*hidden_dim))\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle_flex':\n",
    "      model = LectinOracle_flex(len(libr), num_classes = num_classes)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle_flex.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'NSequonPred':\n",
    "      model = NSequonPred()\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"NSequonPred_batch32.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    else:\n",
    "      print(\"Invalid Model Type\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbbca33",
   "metadata": {},
   "source": [
    "## Testing using same framework as iteration 0 (basic kingdom sweetnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# testing the modified SweetNet model on the GlycoWork dataset \n",
    "from glycowork.glycan_data.loader import df_species\n",
    "from glycowork.ml.train_test_split import hierarchy_filter\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "from glycowork.ml import model_training\n",
    "\n",
    "# silence the avalanche of \"undefined\" warnings\n",
    "#import warnings\n",
    "#from sklearn.exceptions import UndefinedMetricWarning\n",
    "#warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "train_x, val_x, train_y, val_y, id_val, class_list, class_converter = hierarchy_filter(df_species,\n",
    "                                                                                       rank = 'Kingdom')\n",
    "\n",
    "dataloaders = split_data_to_train(train_x, val_x, train_y, val_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets split out the training code so I don't have to load the data each time\n",
    "\n",
    "model = prep_model('SweetNet', len(class_list))\n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = len(class_list))\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'classification',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4382d5b4",
   "metadata": {},
   "source": [
    "### Extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdade799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the Kingdoms in the original data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "kingdom_counts = df_species['Kingdom'].value_counts()\n",
    "print(\"Kingdom distribution in the original data:\")\n",
    "print(kingdom_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f112562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Print the first few rows to get an overview\n",
    "print(df_species.head())\n",
    "\n",
    "# Print the data types of the columns\n",
    "print(df_species.info())\n",
    "\n",
    "# Check for duplicated glycans\n",
    "print(f\"Number of unique glycans: {df_species['glycan'].nunique()}\")\n",
    "print(f\"Total number of rows: {len(df_species)}\")\n",
    "\n",
    "# If you suspect a column contains multiple kingdoms:\n",
    "if 'Kingdom' in df_species.columns:  # Replace 'Kingdom' with the actual column name\n",
    "    # Check the first few values of that column\n",
    "    print(df_species['Kingdom'].head())\n",
    "    # If it's a string with delimiters, count the delimiters\n",
    "    if isinstance(df_species['Kingdom'][0], str) and ',' in df_species['Kingdom'][0]: #assuming ',' is the delimiter\n",
    "        print(f\"Example value: {df_species['Kingdom'][0]}\")\n",
    "        print(f\"Number of commas in the first value: {df_species['Kingdom'][0].count(',')}\")\n",
    "\n",
    "    # Check for multiple columns representing kingdoms\n",
    "    for col in df_species.columns:\n",
    "        if col in ['Animalia', 'Bacteria', 'Plantae']:  # Add all possible kingdom column names\n",
    "            print(f\"Column '{col}': Data type = {df_species[col].dtype}, Unique values = {df_species[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set()\n",
    "duplicates_count = 0\n",
    "\n",
    "for item in train_x:\n",
    "    if item in seen:\n",
    "        duplicates_count += 1\n",
    "    else:\n",
    "        seen.add(item)\n",
    "\n",
    "print(f\"Number of duplicates: {duplicates_count}\")\n",
    "print(f\"Number of unique items: {len(seen)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd3127",
   "metadata": {},
   "source": [
    "## New custom prep function for the GLM-Infused Sweetnet ||||run on kernel restart||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fdcb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the embeddings again here, so I don't have to jump up and down whenever I reload the kernel)\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            glm_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb565ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Literal \n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    from torch_geometric.nn import GraphConv\n",
    "    from torch_geometric.nn import global_mean_pool as gap\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "  raise ImportError(\"<torch or torch_geometric missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from glycowork.glycan_data.loader import lib\n",
    "\n",
    "def prep_infused_sweetnet(num_classes: int, # number of unique classes for classification\n",
    "                           embeddings_dict: Optional[Dict[str, np.ndarray]] = None, # embeddings for 'external' method\n",
    "                           initialization_method: Literal['external', 'random', 'one_hot'] = 'external', # specifies initialization method\n",
    "                           trainable_embeddings: bool = True, # whether the external embeddings should be trainable\n",
    "                           hidden_dim: int = 320, # hidden dimension for the model (be sure to match dimension of embeddings)  \n",
    "                           libr: Optional[Dict[str, int]] = None, # dictionary of form glycoletter:index\n",
    "                          ) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Instantiates and prepares a SweetNet model with specified embedding initialization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_classes : int\n",
    "        Number of unique classes for classification. (REQUIRED)\n",
    "    embeddings_dict : Optional[Dict[str, np.ndarray]], optional, default = None\n",
    "        The loaded external embeddings dictionary {glycan_word: embedding_vector}.\n",
    "        Required if initialization_method is 'external'.\n",
    "    initialization_method : {'external', 'random', 'one_hot'}, optional, default = 'external'\n",
    "        The method to initialize the embedding layer:\n",
    "        - 'external': Initialize with embeddings from embeddings_dict.\n",
    "        - 'random': Randomly initialized embeddings (train from scratch).\n",
    "        - 'one_hot': Initialize with one-hot encoded vectors.\n",
    "    trainable_embeddings : bool, optional, default = True\n",
    "        Whether the embedding layer should be trainable during training.\n",
    "    hidden_dim : int, optional, default = 320\n",
    "        Dimension of hidden layers. Must match the dimension of the embeddings\n",
    "        used if initialization_method is 'external'.\n",
    "    libr : Optional[Dict[str, int]], optional, default = None\n",
    "        Dictionary of form glycoletter:index.\n",
    "        If None, the standard glycowork library is used. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Module\n",
    "        An initialized PyTorch model (SweetNet).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        - If initialization_method is 'external' but embeddings_dict is None.\n",
    "        - If initialization_method is 'external' and embedding dimension does not match hidden_dim.\n",
    "        - If initialization_method is 'one_hot' and hidden_dim does not match library size.\n",
    "        - If initialization_method is 'random' or 'one_hot' and libr is None.\n",
    "        - If an unknown initialization_method is provided.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #  Check if libr is provided, if not, use the default library\n",
    "    if libr is None:\n",
    "        libr = lib\n",
    "\n",
    "    # Check if the required components are available in the current context\n",
    "    if 'SweetNet' not in globals() or not callable(globals()['SweetNet']) or \\\n",
    "       'init_weights' not in globals() or not callable(globals()['init_weights']):\n",
    "         raise ValueError(\"Required glycowork components (SweetNet, init_weights) not available or not callable. Please ensure they are imported correctly.\")\n",
    "\n",
    "    # Instantiate the SweetNet model\n",
    "    model = SweetNet(lib_size=len(libr), num_classes=num_classes, hidden_dim=hidden_dim)\n",
    "    print(f\"SweetNet model instantiated with lib_size={len(libr)}, num_classes={num_classes}, hidden_dim={hidden_dim}.\")\n",
    "\n",
    "\n",
    "    # Apply initial weights to all layers (embedding and non-embedding)\n",
    "    model = model.apply(lambda module: init_weights(module, mode = 'sparse')) # Experiment with 'sparse', 'xavier', or 'kaiming'\n",
    "    \n",
    "    # move model to the device (CPU or GPU)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if initialization_method == 'external':\n",
    "        print(\"Handling 'external' initialization method.\")\n",
    "        \n",
    "        # Check if embeddings_dict is provided\n",
    "        # If not, raise an error (this is a required parameter for 'external' method)\n",
    "        if embeddings_dict is None:\n",
    "            raise ValueError(\"embeddings_dict must be provided when initialization_method is 'external'.\")\n",
    "        \n",
    "        # Check that the dimension of the embeddings_dict matches hidden_dim\n",
    "        embedding_key = next(iter(embeddings_dict))\n",
    "        external_embedding_dim = embeddings_dict[embedding_key].shape[0]\n",
    "        if external_embedding_dim != hidden_dim:\n",
    "             raise ValueError(f\"External embedding dimension ({external_embedding_dim}) must match model's hidden_dim ({hidden_dim}).\")\n",
    "        \n",
    "        # Get the tensor of the embedding layer's weights. It already has initial random values.\n",
    "        embedding_tensor_to_populate = model.item_embedding.weight.data\n",
    "\n",
    "        #\n",
    "        with torch.no_grad():\n",
    "                 # Iterate through the library (which gives us the index for each glycan word)\n",
    "                 for glycan_word, index in libr.items():\n",
    "                    if glycan_word in embeddings_dict:\n",
    "\n",
    "                        # Get the embedding vector from the dictionary\n",
    "                        embedding_vector = embeddings_dict[glycan_word] \n",
    "\n",
    "                        # Convert to tensor, ensure correct dtype, and move to the same device\n",
    "                        embedding_vector_tensor = torch.tensor(embedding_vector, dtype=torch.float32).to(embedding_tensor_to_populate.device)\n",
    "\n",
    "                        # Copy the vector into the correct row of the model's embedding tensor\n",
    "                        # Relying on index from libr being valid for embedding_tensor_to_populate size\n",
    "                        embedding_tensor_to_populate[index].copy_(embedding_vector_tensor)\n",
    "                        \n",
    "                        #print(glycan_word)\n",
    "                        #print(embedding_vector)\n",
    "                    else:\n",
    "                        # If a glycan word in libr is NOT in embeddings_dict, its initial random value is preserved (for smaller dictionaries).\n",
    "                        print(f\"{glycan_word} is not in library, keeping its initial random value.\")\n",
    "                        pass # Explicitly do nothing, keeping the initial random value\n",
    "         \n",
    "\n",
    "    elif initialization_method == 'random':\n",
    "        print(\"Handling 'random' initialization method (training from scratch).\")\n",
    "        \n",
    "        # The item_embedding layer was already initialized randomly by the standard initialization loop above.\n",
    "\n",
    "        pass \n",
    "\n",
    "        \n",
    "    elif initialization_method == 'one_hot':\n",
    "        print(\" 'one_hot' initialization method not implemented yet\")\n",
    "        \n",
    "    # either I need the hidden_dim to be the same as the number of glycoletters in the library\n",
    "    # or I need to find a way to reduce the dimensionality of the one-hot encoding to match the hidden_dim\n",
    "    #or do what Roman suggested and reduce the dictionary to the 319 most common glycowords\n",
    "    # Let's tackle this later given time \n",
    "\n",
    "\n",
    "        # Determine the required embedding dimension for one-hot encoding\n",
    "        #required_hidden_dim = len(libr) + 1\n",
    "\n",
    "        # Create the one-hot embedding matrix (identity matrix)\n",
    "        #one_hot_matrix = torch.eye(required_hidden_dim, dtype=torch.float32)\n",
    "\n",
    "        # Copy one_hot_matrix to model.item_embedding.weight.data\n",
    "        #one_hot_matrix = one_hot_matrix.to(model.item_embedding.weight.device)\n",
    "\n",
    "        # Copy the one-hot matrix into the model's item_embedding.weight.data\n",
    "        #with torch.no_grad():\n",
    "           # model.item_embedding.weight.copy_(one_hot_matrix)\n",
    "            \n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        # This case should ideally be caught by the Literal type hint and docstring,\n",
    "        # but adding a runtime check is robust.\n",
    "        raise ValueError(f\"Unknown initialization_method: {initialization_method}\")\n",
    "\n",
    "\n",
    "    # Set trainability based on trainable_embeddings flag (outside the branches)\n",
    "    # This happens AFTER the initialization logic in the branches above.\n",
    "    # The logic for setting requires_grad is the same regardless of initialization method.\n",
    "    model.item_embedding.weight.requires_grad = trainable_embeddings\n",
    "    print(f\"SweetNet item_embedding layer set to trainable: {trainable_embeddings}.\")\n",
    "\n",
    "    \n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "#test = prep_infused_sweetnet(num_classes=10,embeddings_dict=glm_embeddings, initialization_method='external')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e20752",
   "metadata": {},
   "source": [
    "## Time to train a multi-class multi-label Sweetnet on our properly loaded data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fdeaa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SweetNet model instantiated with lib_size=2565, num_classes=18, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.7060 LRAP: 0.0029 NDCG: 0.2020\n",
      "val Loss: 3.8037 LRAP: 0.0000 NDCG: 0.1902\n",
      "Validation loss decreased (0.000000 --> 3.803666).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 3.6077 LRAP: 0.0176 NDCG: 0.2918\n",
      "val Loss: 3.7904 LRAP: 0.0000 NDCG: 0.2168\n",
      "Validation loss decreased (3.803666 --> 3.790358).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 3.4745 LRAP: 0.0098 NDCG: 0.3323\n",
      "val Loss: 3.7512 LRAP: 0.0000 NDCG: 0.2409\n",
      "Validation loss decreased (3.790358 --> 3.751169).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 3.3760 LRAP: 0.0078 NDCG: 0.3726\n",
      "val Loss: 3.6773 LRAP: 0.0000 NDCG: 0.3295\n",
      "Validation loss decreased (3.751169 --> 3.677317).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 3.2350 LRAP: 0.0049 NDCG: 0.3909\n",
      "val Loss: 3.5691 LRAP: 0.0137 NDCG: 0.4398\n",
      "Validation loss decreased (3.677317 --> 3.569063).  Saving model ...\n",
      "\n",
      "Training complete in 0m 2s\n",
      "Best val loss: 3.569063, best LRAP score: 0.8228\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using new prep_infused_sweetnet to generate glm-infused SweetNet model\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders\n",
    "model =  prep_infused_sweetnet(\n",
    "            initialization_method = 'random',\n",
    "            num_classes = classes,\n",
    "            embeddings_dict = glm_embeddings, \n",
    "            trainable_embeddings = True\n",
    "            ) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes)\n",
    "model_ft = train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 5, mode = 'multilabel',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd57110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using new prep_infused_sweetnet to generate glm-infused SweetNet model\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders\n",
    "model =  prep_infused_sweetnet(\n",
    "            initialization_method = 'external',\n",
    "            num_classes = classes,\n",
    "            embeddings_dict = glm_embeddings, \n",
    "            trainable_embeddings = True\n",
    "            ) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes)\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'multilabel',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.item_embedding.weight.data[3])\n",
    "print(model.item_embedding.weight.data[10])\n",
    "print(model.item_embedding.weight.data[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59cf29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using standard prep_model\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders\n",
    "model =  prep_model('SweetNet', classes) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes)\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'multilabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Try with my new glm-infused data\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders_emb\n",
    "model = prep_model('SweetNet', classes, use_external_embeddings = True) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes) #changed to 0.01 from 0.005 from 0.0005\n",
    "model_ft = train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'multilabel', use_external_embeddings = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5025834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweetnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
