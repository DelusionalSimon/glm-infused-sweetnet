{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2416a1b",
   "metadata": {},
   "source": [
    "# GLM-Infused SweetNet Experiments\n",
    "\n",
    "Experimenting with a modified version of SweetNet that allows it to take pre-trained embeddings as input. To get there I need a way to take the embeddings Iâ€™ve gotten from roman and transform them into nice inputs for the model, and a way to set the initial features using these inputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652c8f8",
   "metadata": {},
   "source": [
    "## Importing and exploring the GLM Embedding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0485fe3",
   "metadata": {},
   "source": [
    "### Exploring embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: glm_embeddings_1.pkl\n",
      "Embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# quick thing to load a pickle file\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            loaded_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of loaded object: <class 'dict'>\n",
      "Number of items (if dictionary): 2565\n",
      "Example keys (first 5): ['!GlcNAc', '-10', '-12', '-2', '-4']\n"
     ]
    }
   ],
   "source": [
    "# lets do some quick exploration\n",
    "\n",
    "# --- Explore the loaded data ---\n",
    "print(f\"Type of loaded object: {type(loaded_embeddings)}\")\n",
    "\n",
    "# Common formats for embeddings: dictionary or numpy array\n",
    "if isinstance(loaded_embeddings, dict):\n",
    "    print(f\"Number of items (if dictionary): {len(loaded_embeddings)}\")\n",
    "    # print some keys to see what they look like\n",
    "    print(f\"Example keys (first 5): {list(loaded_embeddings.keys())[:5]}\")\n",
    "elif hasattr(loaded_embeddings, 'shape'):\n",
    "    print(f\"Shape (if array/tensor): {loaded_embeddings.shape}\")\n",
    "    if hasattr(loaded_embeddings, 'dtype'):\n",
    "         print(f\"Data type (if array/tensor): {loaded_embeddings.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e6aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-6', '-8', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic', '1,5-Anhydro-Man-ol', '1,5-Anhydro-ManNAc-ol', '1,5-Anhydro-Xyl-ol', '1,5-Anhydro-Xyl2F-ol', '1-1', '1-2', '1-3', '1-4', '1-5', '1-6']\n"
     ]
    }
   ],
   "source": [
    "print(list(loaded_embeddings.keys())[5:30]) # Print more keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of value for '-8': <class 'numpy.ndarray'>\n",
      "Shape of value: (320,)\n",
      "Dtype of value: float32\n",
      "[ 9.33886290e-01 -7.57189512e-01 -5.22765040e-01  4.93726492e-01\n",
      "  3.03156078e-01 -1.72754931e+00  2.03015614e+00 -1.13539708e+00\n",
      " -8.32044244e-01 -6.09763384e-01 -5.63947335e-02 -2.68140852e-01\n",
      " -6.37493312e-01  1.45667583e-01 -7.75620103e-01 -1.39048725e-01\n",
      "  1.06042847e-01 -3.74972522e-01  7.91566074e-01 -1.03034627e+00\n",
      " -1.12639211e-01 -3.78986076e-03  5.92547238e-01  2.81559825e-01\n",
      " -5.21002829e-01  9.35327411e-01  2.56601274e-01 -3.91364455e-01\n",
      "  2.72188634e-02  5.00928342e-01 -5.55309415e-01  1.28289807e+00\n",
      " -6.45282388e-01  5.19899249e-01  6.10100806e-01  1.84122849e+00\n",
      "  3.11432898e-01 -7.64928609e-02 -1.05589128e+00  6.50653005e-01\n",
      "  9.70111132e-01  7.40227938e-01  8.39829683e-01 -3.04328918e-01\n",
      " -1.06630003e+00  4.53770608e-01  4.27673876e-01 -6.02427721e-01\n",
      "  4.39536482e-01 -1.16493046e+00 -2.04154789e-01  1.13036299e+00\n",
      "  2.51586974e-01  1.04393315e+00  2.60879964e-01  4.63881493e-02\n",
      "  8.49927664e-01 -1.21275747e+00 -5.25301337e-01 -7.54553556e-01\n",
      " -5.36846638e-01  1.71898973e+00  1.07118464e+00  1.25938666e+00\n",
      "  7.28268623e-01  2.50012755e-01 -8.84264708e-01  3.54878515e-01\n",
      " -9.51814711e-01  1.92197442e-01  6.22674108e-01 -7.19715357e-02\n",
      " -2.53418744e-01  6.10054433e-01 -1.37844992e+00  1.10613918e+00\n",
      " -7.89550483e-01  4.11728621e-01 -1.39660871e+00 -2.74130464e-01\n",
      " -4.85218346e-01 -1.64008796e+00 -2.54515797e-01 -4.76354361e-02\n",
      "  1.70321250e+00  1.37953115e+00  6.62403643e-01  1.23904690e-01\n",
      " -2.03382596e-02  2.49572158e-01 -1.19476050e-01  1.01610112e+00\n",
      "  1.54832602e-01 -3.18885893e-01  1.02479362e+00  2.19304472e-01\n",
      " -1.77515924e-01 -2.96848416e-01 -1.51161349e+00 -1.55658543e+00\n",
      "  6.01615787e-01 -1.18876457e+00  6.75462842e-01 -1.21065450e+00\n",
      "  1.00956786e+00  5.41580915e-01  4.89682317e-01 -4.31063682e-01\n",
      " -6.99561596e-01 -9.50598717e-01 -4.71236914e-01  8.96337509e-01\n",
      "  1.97975963e-01  6.51351273e-01 -1.65811467e+00 -2.37476051e-01\n",
      "  1.22424424e+00  3.85935336e-01  1.74970782e+00  1.08295810e+00\n",
      " -2.08416104e-01 -1.44780791e+00 -3.18115175e-01 -2.69204080e-02\n",
      " -7.30906725e-01  3.65380794e-01 -5.23220778e-01 -1.59638667e+00\n",
      "  9.76120412e-01  4.75375116e-01  1.10794783e+00 -9.16275680e-01\n",
      "  8.67535770e-01 -2.21260801e-01  3.58714461e-02 -1.62487292e+00\n",
      "  9.47338939e-01  2.52621353e-01 -2.44861484e-01  4.85217899e-01\n",
      " -1.72671735e-01  1.49431840e-01 -9.26872373e-01 -6.38668120e-01\n",
      " -1.37115136e-01  1.30791855e+00  1.25448748e-01  3.05962026e-01\n",
      " -2.51638025e-01  6.88706279e-01 -6.43941760e-01  6.10008895e-01\n",
      "  2.45932966e-01  1.53176570e+00 -2.05617994e-01  5.01646757e-01\n",
      " -4.11370814e-01 -5.36742508e-01 -1.23477876e-02  6.50121808e-01\n",
      " -3.78578186e-01  6.62264466e-01  1.53327346e-01 -9.97333288e-01\n",
      "  2.86916673e-01 -3.98133188e-01  1.19174033e-01 -1.07086766e+00\n",
      "  5.68605885e-02  8.55352730e-02 -2.43456244e-01 -5.13940752e-01\n",
      "  9.52608764e-01 -3.56329709e-01 -9.76832956e-02  1.55454218e-01\n",
      "  1.07665420e-01  7.78901517e-01  1.94103813e+00  5.98729789e-01\n",
      "  1.49250478e-01  6.60319090e-01 -9.16693985e-01 -1.80390513e+00\n",
      " -1.08837974e+00 -5.85823774e-01 -5.17625034e-01  1.13187218e+00\n",
      " -3.11186165e-01 -1.56313211e-01  4.89638031e-01  6.32191420e-01\n",
      " -9.01452422e-01  3.40963513e-01  3.77618819e-01  4.78747129e-01\n",
      " -1.26142776e+00  1.63014054e+00 -7.38181099e-02 -8.88819635e-01\n",
      " -9.81908560e-01 -3.11309278e-01 -2.87041283e+00 -6.68797910e-01\n",
      "  1.15292573e+00  1.82262063e+00  6.86679184e-01  3.54639411e-01\n",
      "  1.14279723e+00  1.23592412e+00 -4.26488072e-01  5.78116417e-01\n",
      "  2.67315298e-01  1.73516899e-01 -6.95198655e-01 -7.84443021e-01\n",
      "  1.87699527e-01  7.76465774e-01  1.17747712e+00  2.98208922e-01\n",
      "  1.80739570e+00 -6.55146241e-02  2.10267353e+00 -1.49224257e+00\n",
      "  1.67633876e-01 -5.96812427e-01  4.02143002e-01 -5.80711842e-01\n",
      " -6.86030865e-01  2.82077312e-01  4.62324202e-01 -8.51680398e-01\n",
      " -6.37305975e-01 -1.97909772e-01  8.27008903e-01 -2.47440666e-01\n",
      "  5.40550411e-01  2.20697820e-02 -3.67172241e-01  1.37753654e+00\n",
      "  2.57560164e-01  1.12044883e+00  1.47008979e+00 -3.09366286e-01\n",
      "  1.41206241e+00 -1.07911384e+00 -3.82883579e-01  1.15288660e-01\n",
      "  6.46931171e-01 -1.63524508e+00 -4.82143342e-01 -2.22676694e-02\n",
      " -2.94011176e-01  1.76649165e+00 -1.42879653e+00 -1.01673603e+00\n",
      "  6.92535341e-01  1.08943865e-01 -1.51619220e+00 -1.31418991e+00\n",
      " -5.36556542e-01 -9.08092409e-02 -3.43192220e-02 -5.01663029e-01\n",
      " -4.27816272e-01  5.04320741e-01 -8.19638968e-01  1.27975166e-01\n",
      "  6.98855758e-01  4.11748588e-01 -2.63869703e-01 -1.72789741e+00\n",
      "  2.40177006e-01 -3.30802739e-01  1.47785515e-01  4.70187128e-01\n",
      "  3.38367313e-01 -1.54152012e+00  3.17173868e-01 -1.70832485e-01\n",
      "  9.85031009e-01 -1.51257575e+00  7.86181986e-01  2.95546353e-01\n",
      "  4.57608998e-02 -6.43859148e-01  4.83155847e-01 -1.51108074e+00\n",
      " -1.82736918e-01 -3.47120881e-01 -5.70403397e-01 -1.21720120e-01\n",
      " -1.61197579e+00  1.02913380e-03 -4.93016541e-02 -1.70051694e+00\n",
      " -4.81017500e-01 -9.90746021e-01  3.51191968e-01 -6.38143182e-01\n",
      "  8.80924284e-01  1.06428635e+00 -1.31740403e+00 -1.46576715e+00\n",
      " -8.72395873e-01  1.48068953e+00 -2.76599586e-01 -1.15330029e+00\n",
      "  1.45732999e-01 -1.63671541e+00  2.22910285e-01 -3.31862628e-01\n",
      "  5.65533102e-01 -4.64938819e-01  1.83547580e+00 -7.03186333e-01\n",
      "  2.57217407e-01  1.83000445e+00  1.64521456e-01  1.26764941e+00]\n"
     ]
    }
   ],
   "source": [
    "example_key = '!GlcNAc' \n",
    "if example_key in loaded_embeddings:\n",
    "    embedding_vector = loaded_embeddings[example_key]\n",
    "    print(f\"Type of value for '{example_key}': {type(embedding_vector)}\")\n",
    "    if hasattr(embedding_vector, 'shape'):\n",
    "        print(f\"Shape of value: {embedding_vector.shape}\") # This gives dimensionality!\n",
    "        print(f\"Dtype of value: {embedding_vector.dtype}\")\n",
    "    print(embedding_vector) # Print the vector itself if it's not too long\n",
    "else:\n",
    "    print(f\"Key '{example_key}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7f66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'other': 122, 'linkage_or_modification': 36, 'monosaccharide': 2407})\n"
     ]
    }
   ],
   "source": [
    "# let's look at the keys a bit more closely\n",
    "\n",
    "import collections\n",
    "\n",
    "key_types = collections.defaultdict(int)\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        key_types['linkage_or_modification'] += 1\n",
    "    elif key[0].isalpha():\n",
    "        key_types['monosaccharide'] += 1\n",
    "    else:\n",
    "        key_types['other'] += 1\n",
    "\n",
    "print(key_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd6a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'other' keys: 122\n",
      "Examples of 'other' keys: ['!GlcNAc', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic', '1,5-Anhydro-Man-ol', '1,5-Anhydro-ManNAc-ol', '1,5-Anhydro-Xyl-ol', '1,5-Anhydro-Xyl2F-ol', '1b-4', '1dAlt-ol']\n"
     ]
    }
   ],
   "source": [
    "# Let's explore those Other keys \n",
    "\n",
    "other_keys = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        other_keys.append(key)\n",
    "\n",
    "print(f\"Number of 'other' keys: {len(other_keys)}\")\n",
    "print(f\"Examples of 'other' keys: {other_keys[:20]}\") # Print the first 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398bd9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More Examples of 'other' keys: ['1dEry-ol', '2,3-Anhydro-All', '2,3-Anhydro-Man', '2,3-Anhydro-Rib', '2,5-Anhydro-D-Alt-ol', '2,5-Anhydro-D-Alt3S-ol', '2,5-Anhydro-D-Tal', '2,5-Anhydro-Glc', '2,5-Anhydro-L-Man-ol', '2,5-Anhydro-Man', '2,5-Anhydro-Man-ol', '2,5-Anhydro-Man1S-ol', '2,5-Anhydro-Man3S-ol', '2,5-Anhydro-Man6S', '2,5-Anhydro-Tal-ol', '2,5-Anhydro-Tal6P', '2,6-Anhydro-Glc5NAc-ol', '2,6-Anhydro-L-Gul-ol', '2,6-Anhydro-L-Gul-onic', '2,6-Anhydro-Man-ol', '2,6-Anhydro-Tal5NAc-ol', '2,7-Anhydro-Kdo', '2,7-Anhydro-Kdof', '2dAraHexA', '3,6-Anhydro-Fruf', '3,6-Anhydro-Gal', '3,6-Anhydro-Gal2S', '3,6-Anhydro-Glc', '3,6-Anhydro-L-Gal', '3,6-Anhydro-L-Gal2Me', '3-Anhydro-Gal', '3-Anhydro-Gal2S', '3dFuc', '3dGal', '3dLyxHep-ulosaric', '4,7-Anhydro-Kdo', '4,7-Anhydro-KdoOPEtN', '4,8-Anhydro-Kdo', '4d8dNeu5Ac', '4dAraHex', '4dEry-ol', '4dFuc', '4dGal', '4dNeu5Ac', '4dThrHexNAcA4en', '4eLeg5Ac7Ac', '5dAraf', '5dAraf3Me', '5dLyxf3CFo', '5dLyxf3CMe']\n"
     ]
    }
   ],
   "source": [
    "# Let's look at 50 more keys\n",
    "\n",
    "print(f\"More Examples of 'other' keys: {other_keys[20:70]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3a766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'monosaccharide' keys: 2407\n",
      "Examples of 'monosaccharide' keys: ['Abe', 'Abe1PP', 'Abe2Ac', 'AbeOAc', 'Acarbose', 'AcefA', 'Aci5Ac7Ac', 'AcoNAc', 'All', 'All-ol', 'All1S2S3S4S', 'All2Ac3Ac', 'All2S3S4S', 'All3Ac', 'All6Ac', 'AllN', 'AllN1P', 'AllNAc', 'AllNAc6Me', 'AllOMe', 'Alt', 'AltA', 'AltA2N', 'AltA2S', 'AltAN', 'AltNAc', 'AltNAcA', 'AltNAcA1Prop', 'Altf', 'AltfOAc', 'Amikacin', 'Api', 'ApiOAc', 'ApiOMe-ol', 'Apif', 'Ara', 'Ara-ol', 'Ara1Cer2Ac', 'Ara1Me', 'Ara1N4P', 'Ara1P4N', 'Ara1PP', 'Ara1PP2NAc', 'Ara1PP4N', 'Ara1PP4NFo', 'Ara2Ac', 'Ara2Ac3Ac', 'Ara2Ac3Ac4Ac', 'Ara2Ac4Ac', 'Ara2Ac5P-ol']\n"
     ]
    }
   ],
   "source": [
    "# Let's explore those monosaccharide keys\n",
    "monosaccharide = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        pass # linkage_or_modification\n",
    "    elif key[0].isalpha():\n",
    "        monosaccharide.append(key)\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'monosaccharide' keys: {len(monosaccharide)}\")\n",
    "print(f\"Examples of 'monosaccharide' keys: {monosaccharide[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ddd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'linkage_or_modification' keys: 36\n",
      "Examples of 'linkage_or_modification' keys: ['-10', '-12', '-2', '-4', '-6', '-8', '1-1', '1-2', '1-3', '1-4', '1-5', '1-6', '1-?', '2-3', '2-4', '2-5', '2-6', '3-1', '3-5', '4-1', '4-5', '5-1', '5-2', '5-3', '5-4', '5-5', '5-6', '6-1', '6-3', '6-4', '?1-2', '?1-3', '?1-4', '?1-6', '?1-?', '?2-?']\n"
     ]
    }
   ],
   "source": [
    "# To be throughough, let's look at 50 Linkage or Modification keys as well\n",
    "linkage_or_modification = []\n",
    "for key in loaded_embeddings.keys():\n",
    "    if '-' in key and not any(char.isalpha() for char in key):\n",
    "        linkage_or_modification.append(key)\n",
    "    elif key[0].isalpha():\n",
    "        pass # monosaccharide\n",
    "    else:\n",
    "        pass # other\n",
    "\n",
    "print(f\"Number of 'linkage_or_modification' keys: {len(linkage_or_modification)}\")\n",
    "print(f\"Examples of 'linkage_or_modification' keys: {linkage_or_modification[:50]}\") # Print the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "df823d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All embeddings in the loaded dictionary appear to be the same.\n",
      "Number of embeddings in the dictionary: 2565\n",
      "First embedding:\n",
      "[ 9.33886290e-01 -7.57189512e-01 -5.22765040e-01  4.93726492e-01\n",
      "  3.03156078e-01 -1.72754931e+00  2.03015614e+00 -1.13539708e+00\n",
      " -8.32044244e-01 -6.09763384e-01 -5.63947335e-02 -2.68140852e-01\n",
      " -6.37493312e-01  1.45667583e-01 -7.75620103e-01 -1.39048725e-01\n",
      "  1.06042847e-01 -3.74972522e-01  7.91566074e-01 -1.03034627e+00\n",
      " -1.12639211e-01 -3.78986076e-03  5.92547238e-01  2.81559825e-01\n",
      " -5.21002829e-01  9.35327411e-01  2.56601274e-01 -3.91364455e-01\n",
      "  2.72188634e-02  5.00928342e-01 -5.55309415e-01  1.28289807e+00\n",
      " -6.45282388e-01  5.19899249e-01  6.10100806e-01  1.84122849e+00\n",
      "  3.11432898e-01 -7.64928609e-02 -1.05589128e+00  6.50653005e-01\n",
      "  9.70111132e-01  7.40227938e-01  8.39829683e-01 -3.04328918e-01\n",
      " -1.06630003e+00  4.53770608e-01  4.27673876e-01 -6.02427721e-01\n",
      "  4.39536482e-01 -1.16493046e+00 -2.04154789e-01  1.13036299e+00\n",
      "  2.51586974e-01  1.04393315e+00  2.60879964e-01  4.63881493e-02\n",
      "  8.49927664e-01 -1.21275747e+00 -5.25301337e-01 -7.54553556e-01\n",
      " -5.36846638e-01  1.71898973e+00  1.07118464e+00  1.25938666e+00\n",
      "  7.28268623e-01  2.50012755e-01 -8.84264708e-01  3.54878515e-01\n",
      " -9.51814711e-01  1.92197442e-01  6.22674108e-01 -7.19715357e-02\n",
      " -2.53418744e-01  6.10054433e-01 -1.37844992e+00  1.10613918e+00\n",
      " -7.89550483e-01  4.11728621e-01 -1.39660871e+00 -2.74130464e-01\n",
      " -4.85218346e-01 -1.64008796e+00 -2.54515797e-01 -4.76354361e-02\n",
      "  1.70321250e+00  1.37953115e+00  6.62403643e-01  1.23904690e-01\n",
      " -2.03382596e-02  2.49572158e-01 -1.19476050e-01  1.01610112e+00\n",
      "  1.54832602e-01 -3.18885893e-01  1.02479362e+00  2.19304472e-01\n",
      " -1.77515924e-01 -2.96848416e-01 -1.51161349e+00 -1.55658543e+00\n",
      "  6.01615787e-01 -1.18876457e+00  6.75462842e-01 -1.21065450e+00\n",
      "  1.00956786e+00  5.41580915e-01  4.89682317e-01 -4.31063682e-01\n",
      " -6.99561596e-01 -9.50598717e-01 -4.71236914e-01  8.96337509e-01\n",
      "  1.97975963e-01  6.51351273e-01 -1.65811467e+00 -2.37476051e-01\n",
      "  1.22424424e+00  3.85935336e-01  1.74970782e+00  1.08295810e+00\n",
      " -2.08416104e-01 -1.44780791e+00 -3.18115175e-01 -2.69204080e-02\n",
      " -7.30906725e-01  3.65380794e-01 -5.23220778e-01 -1.59638667e+00\n",
      "  9.76120412e-01  4.75375116e-01  1.10794783e+00 -9.16275680e-01\n",
      "  8.67535770e-01 -2.21260801e-01  3.58714461e-02 -1.62487292e+00\n",
      "  9.47338939e-01  2.52621353e-01 -2.44861484e-01  4.85217899e-01\n",
      " -1.72671735e-01  1.49431840e-01 -9.26872373e-01 -6.38668120e-01\n",
      " -1.37115136e-01  1.30791855e+00  1.25448748e-01  3.05962026e-01\n",
      " -2.51638025e-01  6.88706279e-01 -6.43941760e-01  6.10008895e-01\n",
      "  2.45932966e-01  1.53176570e+00 -2.05617994e-01  5.01646757e-01\n",
      " -4.11370814e-01 -5.36742508e-01 -1.23477876e-02  6.50121808e-01\n",
      " -3.78578186e-01  6.62264466e-01  1.53327346e-01 -9.97333288e-01\n",
      "  2.86916673e-01 -3.98133188e-01  1.19174033e-01 -1.07086766e+00\n",
      "  5.68605885e-02  8.55352730e-02 -2.43456244e-01 -5.13940752e-01\n",
      "  9.52608764e-01 -3.56329709e-01 -9.76832956e-02  1.55454218e-01\n",
      "  1.07665420e-01  7.78901517e-01  1.94103813e+00  5.98729789e-01\n",
      "  1.49250478e-01  6.60319090e-01 -9.16693985e-01 -1.80390513e+00\n",
      " -1.08837974e+00 -5.85823774e-01 -5.17625034e-01  1.13187218e+00\n",
      " -3.11186165e-01 -1.56313211e-01  4.89638031e-01  6.32191420e-01\n",
      " -9.01452422e-01  3.40963513e-01  3.77618819e-01  4.78747129e-01\n",
      " -1.26142776e+00  1.63014054e+00 -7.38181099e-02 -8.88819635e-01\n",
      " -9.81908560e-01 -3.11309278e-01 -2.87041283e+00 -6.68797910e-01\n",
      "  1.15292573e+00  1.82262063e+00  6.86679184e-01  3.54639411e-01\n",
      "  1.14279723e+00  1.23592412e+00 -4.26488072e-01  5.78116417e-01\n",
      "  2.67315298e-01  1.73516899e-01 -6.95198655e-01 -7.84443021e-01\n",
      "  1.87699527e-01  7.76465774e-01  1.17747712e+00  2.98208922e-01\n",
      "  1.80739570e+00 -6.55146241e-02  2.10267353e+00 -1.49224257e+00\n",
      "  1.67633876e-01 -5.96812427e-01  4.02143002e-01 -5.80711842e-01\n",
      " -6.86030865e-01  2.82077312e-01  4.62324202e-01 -8.51680398e-01\n",
      " -6.37305975e-01 -1.97909772e-01  8.27008903e-01 -2.47440666e-01\n",
      "  5.40550411e-01  2.20697820e-02 -3.67172241e-01  1.37753654e+00\n",
      "  2.57560164e-01  1.12044883e+00  1.47008979e+00 -3.09366286e-01\n",
      "  1.41206241e+00 -1.07911384e+00 -3.82883579e-01  1.15288660e-01\n",
      "  6.46931171e-01 -1.63524508e+00 -4.82143342e-01 -2.22676694e-02\n",
      " -2.94011176e-01  1.76649165e+00 -1.42879653e+00 -1.01673603e+00\n",
      "  6.92535341e-01  1.08943865e-01 -1.51619220e+00 -1.31418991e+00\n",
      " -5.36556542e-01 -9.08092409e-02 -3.43192220e-02 -5.01663029e-01\n",
      " -4.27816272e-01  5.04320741e-01 -8.19638968e-01  1.27975166e-01\n",
      "  6.98855758e-01  4.11748588e-01 -2.63869703e-01 -1.72789741e+00\n",
      "  2.40177006e-01 -3.30802739e-01  1.47785515e-01  4.70187128e-01\n",
      "  3.38367313e-01 -1.54152012e+00  3.17173868e-01 -1.70832485e-01\n",
      "  9.85031009e-01 -1.51257575e+00  7.86181986e-01  2.95546353e-01\n",
      "  4.57608998e-02 -6.43859148e-01  4.83155847e-01 -1.51108074e+00\n",
      " -1.82736918e-01 -3.47120881e-01 -5.70403397e-01 -1.21720120e-01\n",
      " -1.61197579e+00  1.02913380e-03 -4.93016541e-02 -1.70051694e+00\n",
      " -4.81017500e-01 -9.90746021e-01  3.51191968e-01 -6.38143182e-01\n",
      "  8.80924284e-01  1.06428635e+00 -1.31740403e+00 -1.46576715e+00\n",
      " -8.72395873e-01  1.48068953e+00 -2.76599586e-01 -1.15330029e+00\n",
      "  1.45732999e-01 -1.63671541e+00  2.22910285e-01 -3.31862628e-01\n",
      "  5.65533102e-01 -4.64938819e-01  1.83547580e+00 -7.03186333e-01\n",
      "  2.57217407e-01  1.83000445e+00  1.64521456e-01  1.26764941e+00]\n"
     ]
    }
   ],
   "source": [
    "# wait, a couple of weeks later and I couldn't figure out why my glm-infused model wasn't converging, \n",
    "# and I just realised that the embeddings were all the same, no wonder it didn't work. \n",
    "# I should have looked at the embeddings themselves, not just the keys.\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    loaded_embeddings = pickle.load(f)\n",
    "\n",
    "first_embedding = None\n",
    "all_same = True\n",
    "if loaded_embeddings:\n",
    "    first_key = next(iter(loaded_embeddings))\n",
    "    first_embedding = loaded_embeddings[first_key]\n",
    "    for key, embedding in loaded_embeddings.items():\n",
    "        if not (embedding == first_embedding).all():\n",
    "            all_same = False\n",
    "            print(f\"Found a different embedding for key: {key}\")\n",
    "            break\n",
    "\n",
    "if all_same and first_embedding is not None:\n",
    "    print(\"All embeddings in the loaded dictionary appear to be the same.\")\n",
    "elif first_embedding is None:\n",
    "    print(\"The embedding dictionary is empty.\")\n",
    "else:\n",
    "    print(\"Embeddings in the dictionary are not all the same.\")\n",
    "\n",
    "print(f\"Number of embeddings in the dictionary: {len(loaded_embeddings)}\")\n",
    "if first_embedding is not None:\n",
    "    print(f\"First embedding:\")\n",
    "    print(first_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9ad5a",
   "metadata": {},
   "source": [
    "### Load the glycowork libr\n",
    "\n",
    "I'll load the glycowork library and compare it to the keys in the embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b14506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in glycowork vocabulary: 2565\n",
      "Example keys from glycowork vocabulary (first 20): ['!GlcNAc', '-10', '-12', '-2', '-4', '-6', '-8', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic']\n"
     ]
    }
   ],
   "source": [
    "from glycowork.glycan_data import loader\n",
    "\n",
    "glycowork_vocabulary = loader.lib\n",
    "\n",
    "print(f\"Number of items in glycowork vocabulary: {len(glycowork_vocabulary)}\")\n",
    "print(f\"Example keys from glycowork vocabulary (first 20): {list(glycowork_vocabulary.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e1f3d",
   "metadata": {},
   "source": [
    "Nice, they seem to correspond one to one!\n",
    "\n",
    "That saves me a lot of work down the line (Thanks Roman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of value for '-10': <class 'int'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# let's look at one of the keys in the glycowork vocabulary to see what they return\n",
    "example_glycowork_key = '-10'\n",
    "if example_glycowork_key in glycowork_vocabulary:\n",
    "    glycowork_value = glycowork_vocabulary[example_glycowork_key]\n",
    "    print(f\"Type of value for '{example_glycowork_key}': {type(glycowork_value)}\")\n",
    "    print(glycowork_value)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f827a0",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54413ae2",
   "metadata": {},
   "source": [
    "### Load, filter, and transform glycowork data into glycan_loaders ||||run on kernel restart||||\n",
    "This is used to load and filter glycowork data for a specific prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79b4d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the full sugarbase dataset and make a custom dataframe for the problem you want to tackle\n",
    "\n",
    "from glycowork.glycan_data.loader import build_custom_df, df_glycan\n",
    "from glycowork.ml.train_test_split import prepare_multilabel\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "def build_multilabel_dataset(glycan_dataset: str = 'df_species',\n",
    "                          glycan_class: str = 'Kingdom',\n",
    "                          min_class_size: int = 6) -> Tuple[List[str], List[List[float]], List[str]]:\n",
    "    \"\"\"\n",
    "    Loads glycan data, prepares it for multi-label classification,\n",
    "    and removes classes with fewer than min_class_size samples.\n",
    "\n",
    "    Args:\n",
    "        glycan_dataset: The glycowork dataset to use. Options include: \n",
    "            'df_species', 'df_tissue', and 'df_disease'.            \n",
    "        glycan_class: The class to predict. Options include:\n",
    "            df_species: 'Species', 'Genus', 'Family', 'Order', 'Class', 'Phylum', 'Kingdom', 'Domain', 'ref'\n",
    "            df_tissue:  'tissue_sample', 'tissue_species', 'tissue_id', 'tissue_ref'\n",
    "            df_disease: 'disease_association', 'disease_sample', 'disease_direction', 'disease_species', 'disease_id', 'disease_ref'\n",
    "        min_class_size: Minimum number of samples required for a class to be included. Set to 1 to include all classes.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - glycan_sequences: List of glycan strings after filtering rare classes.\n",
    "        - binary_labels: List of corresponding multi-label binary vectors.\n",
    "        - label_names: The ordered list of names for each position in the binary vectors.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    all_glycan_data = df_glycan\n",
    "\n",
    "    # Build custom dataframe\n",
    "    custom_glycan_df = build_custom_df(all_glycan_data, glycan_dataset)\n",
    "\n",
    "    # Extract the list of unique individual labels from the chosen class from the custom_glycan_df\n",
    "    # These are used to dechipher the labels when the model is used for prediction\n",
    "    label_names = sorted(list(custom_glycan_df[glycan_class].unique()))\n",
    "    print(f\"Found {len(label_names)} unique individual classes/labels.\")\n",
    "\n",
    "\n",
    "    # Prepare for multi-label prediction\n",
    "    glycans, labels = prepare_multilabel(custom_glycan_df, glycan_class)\n",
    "\n",
    "    # if needed, removes classes with fewer than min_class_size samples.\n",
    "    if(min_class_size > 1):\n",
    "\n",
    "        # Convert labels to string representation for counting\n",
    "        label_strings = [''.join(map(str, label)) for label in labels]\n",
    "\n",
    "        # Count occurrences of each label combination\n",
    "        label_counts = Counter(label_strings)\n",
    "\n",
    "        # Filter glycans and labels based on class size\n",
    "        glycan_sequences = [glycans[i] for i, label_str in enumerate(label_strings) if label_counts[label_str] >= min_class_size]\n",
    "        binary_labels = [labels[i] for i, label_str in enumerate(label_strings) if label_counts[label_str] >= min_class_size]\n",
    "        print(f\"Number of unique glycans left after filtering rare classes (size >= {min_class_size}): {len(glycan_sequences)}/{len(glycans)}\")\n",
    "        print(f\"Number of unique labels left: {len(labels[0])}\")\n",
    "\n",
    "    else:\n",
    "        glycan_sequences = glycans\n",
    "        binary_labels = labels\n",
    "        print(f\"Number of unique glycans: {len(glycan_sequences)}\")\n",
    "\n",
    "    return glycan_sequences, binary_labels, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd78302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 unique individual classes/labels.\n",
      "Number of unique glycans left after filtering rare classes (size >= 1000): 1034/1648\n",
      "Number of unique labels left: 60\n"
     ]
    }
   ],
   "source": [
    "glycans, labels, label_names = build_multilabel_dataset(glycan_dataset='df_disease', glycan_class='disease_association', min_class_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b77c4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Crohn_disease', 'Fabry_disease', 'Krabbe_disease', 'LPS_induced_inflammation', 'Lyme_disease', 'Parkinson_disease', 'REM_sleep_behavior_disorder', 'Sjogrens_syndrome', 'Toxoplasma_gondii_infection', 'X-linked_lymphoproliferative_syndrome_1', 'acute_atopic_conjunctivitis', 'alzheimers_disease', 'amyotrophic_lateral_sclerosis', 'atrial_fibrillation', 'autoimmune_pancreatitis', 'benign_breast_tumor_tissues_vs_para_carcinoma_tissues', 'cataract', 'cholangiocarcinoma', 'chronic_low_back_pain', 'colon_adenocarcinoma', 'colon_cancer', 'colorectal_adenocarcinoma', 'colorectal_cancer', 'cystic_fibrosis', 'diabetic_kidney_disease', 'differentiated_thyroid_gland_carcinoma', 'esophageal_cancer', 'esophagus_adenocarcinoma', 'female_breast_cancer', 'filarial_elephantiasis', 'gastric_cancer', 'gastric_polyps', 'gastric_ulcer', 'gastritis', 'glioblastoma', 'influenza', 'liver_cancer', 'liver_cirrhosis', 'lung_cancer', 'lung_non_small_cell_carcinoma', 'lung_small_cell_carcinoma', 'lung_squamous_cell_carcinoma', 'melanoma', 'metachromatic_leukodystrophy', 'multiple_sclerosis', 'pancreatic_cancer', 'prostate_cancer', 'rectum_adenocarcinoma', 'renal_cell_carcinoma', 'restless_legs_syndrome', 'staphyloenterotoxemia', 'stomach_cancer', 'stomach_carcinoma', 'thymic_carcinoma', 'thyroid_gland_papillary_carcinoma', 'type_1_diabetes_mellitus', 'type_2_diabetes_mellitus', 'urinary_bladder_cancer', 'vernal_conjunctivitis']\n"
     ]
    }
   ],
   "source": [
    "print (class_members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c440705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique glycans: 1458\n",
      "Number of label vectors: 1458\n",
      "Shape of first label vector (number of members in class): 60\n",
      "\n",
      "First 5 glycans:\n",
      "['Neu5Ac(a2-?)Gal(b1-4)GlcNAc(b1-2)Man(a1-3)[Gal(b1-4)GlcNAc(b1-2)Man(a1-6)][GlcNAc(b1-4)]Man(b1-4)GlcNAc(b1-4)GlcNAc', 'Neu5Ac(a2-?)Gal(b1-4)GlcNAc(b1-2)[Neu5Ac(a2-?)Gal(b1-4)GlcNAc(b1-4)]Man(a1-3)[Neu5Ac(a2-?)Gal(b1-4)[Fuc(a1-3)]GlcNAc(b1-2)Man(a1-6)]Man(b1-4)GlcNAc(b1-4)[Fuc(a1-6)]GlcNAc', 'Fuc(a1-2)[GalNAc(a1-3)]Gal(b1-4)GlcNAc(b1-3)Gal(b1-4)GlcNAc(b1-6)[Gal(b1-3)]GalNAc', 'Fuc(a1-2)Gal(b1-?)GlcNAc(b1-6)[Fuc(a1-?)[Gal(b1-?)]GlcNAc(b1-3)]Gal(b1-3)[GlcNAc(b1-6)]GalNAc', 'Fuc(a1-2)Gal(b1-4)[Fuc(a1-3)]GlcNAc(b1-3)Gal(b1-3)GalNAc']\n",
      "\n",
      "First 5 label vectors:\n",
      "[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "# quick inspection of the data\n",
    "\n",
    "print(f\"Number of unique glycans: {len(glycans)}\")\n",
    "print(f\"Number of label vectors: {len(labels)}\")\n",
    "print(f\"Shape of first label vector (number of members in class): {len(labels[0])}\")\n",
    "print(f\"\\nFirst 5 glycans:\\n{glycans[:5]}\")\n",
    "print(f\"\\nFirst 5 label vectors:\\n{labels[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ff9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and testing sets using StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import List, Union\n",
    "\n",
    "def multilabel_split(glycans: List[str], # list of IUPAC-condensed glycans\n",
    "                 labels: List[Union[float, int, str]], # list of prediction labels\n",
    "                 train_size: float = 0.7, # size of train set, the rest is split into validation and test sets\n",
    "                 random_state: int = 42 # random state for reproducibility\n",
    "                )-> Tuple[List[str], List[str], List[str], List[List[float]], List[List[float]], List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets using StratifiedShuffleSplit.\n",
    "    \n",
    "    Args:\n",
    "        glycans: List of glycans.\n",
    "        labels: List of label vectors.\n",
    "        train_size: Proportion of the dataset to include in the validation and test split.\n",
    "        random_state: Controls the randomness of the split.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - train_glycans: Training set of glycans.\n",
    "            - val_glycans: Validation set of glycans.\n",
    "            - test_glycans: Testing set of glycans.\n",
    "            - train_labels: Training set of labels.\n",
    "            - val_labels: Validation set of labels.\n",
    "            - test_labels: Testing set of labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert labels to a suitable format for stratification (string representation)\n",
    "    label_strings = [''.join(map(str, label)) for label in labels]\n",
    "\n",
    "    # Initial split for train vs. (val + test)\n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = 1 - train_size, random_state = random_state)\n",
    "    train_index, temp_index = next(sss.split(glycans, label_strings))\n",
    "    train_glycans = [glycans[i] for i in train_index]\n",
    "    train_labels = [labels[i] for i in train_index]\n",
    "    temp_glycans = [glycans[i] for i in temp_index]\n",
    "    temp_labels = [labels[i] for i in temp_index]\n",
    "\n",
    "    # Split the remaining (val + test) into validation and test sets\n",
    "    sss_val_test = StratifiedShuffleSplit(n_splits = 1, test_size = 0.5, random_state = random_state)\n",
    "    val_index, test_index = next(sss_val_test.split(temp_glycans, [''.join(map(str, label)) for label in temp_labels]))\n",
    "    val_glycans = [temp_glycans[i] for i in val_index]\n",
    "    val_labels = [temp_labels[i] for i in val_index]\n",
    "    test_glycans = [temp_glycans[i] for i in test_index]\n",
    "    test_labels = [temp_labels[i] for i in test_index]\n",
    "\n",
    "    print(\"Split complete!\")\n",
    "    print(f\"Train set size: {len(train_glycans)}\")\n",
    "    print(f\"Validation set size: {len(val_glycans)}\")\n",
    "    print(f\"Test set size: {len(test_glycans)}\")\n",
    "        \n",
    "    return train_glycans, val_glycans, test_glycans, train_labels, val_labels, test_labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4723022a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete!\n",
      "Train set size: 1020\n",
      "Validation set size: 219\n",
      "Test set size: 219\n"
     ]
    }
   ],
   "source": [
    "# Lets use the split function\n",
    "train_glycans, val_glycans, test_glycans, train_labels, val_labels, test_labels = multilabel_split(glycans, labels, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "068f4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms IUPAC into graphs and makes the data loaders for the training and validation sets\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "\n",
    "glycan_loaders = split_data_to_train(\n",
    "    glycan_list_train = train_glycans,\n",
    "    glycan_list_val = val_glycans,\n",
    "    labels_train = train_labels,\n",
    "    labels_val = val_labels,\n",
    "    batch_size = 32,  # 32 or 128 seem to work well on this system\n",
    "    drop_last = False,\n",
    "    augment_prob = 0.0,  # Adjust if you want augmentation for training\n",
    "    generalization_prob = 0.2  # Adjust if you want generalization for training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae2f77",
   "metadata": {},
   "source": [
    "### GIFFLAR Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a5ce2",
   "metadata": {},
   "source": [
    "#### Load GIFFLAR Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the GIFFLAR dataset for the Taxonomy Kingdom (takes a long time to run(~40m), just use the file it generated)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../GIFFLAR') \n",
    "\n",
    "from gifflar.benchmarks import get_dataset\n",
    "import pathlib\n",
    "\n",
    "data_config_kingdom = {\"name\": \"Taxonomy_Kingdom\"}\n",
    "root_dir = pathlib.Path(\"./data_gifflar\")  # Choose a directory to save the data\n",
    "root_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "kingdom_dataset_config = get_dataset(data_config_kingdom, root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c4b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Taxonomy_Kingdom', 'filepath': WindowsPath('data_gifflar/taxonomy_Kingdom.tsv')}\n"
     ]
    }
   ],
   "source": [
    "# Print the dataset configuration\n",
    "print(kingdom_dataset_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a488a",
   "metadata": {},
   "source": [
    "#### Load and transform GIFFLAR Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "339c831f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               IUPAC  Amoebozoa  Animalia  \\\n",
      "0  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "1  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "2  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "3  3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-...          0         0   \n",
      "4  3,6-Anhydro-L-Gal(a1-3)GalOMe(b1-4)3,6-Anhydro...          0         0   \n",
      "\n",
      "   Bacteria  Bamfordvirae  Chromista  Euryarchaeota  Excavata  Fungi  \\\n",
      "0         1             0          0              0         0      0   \n",
      "1         1             0          0              0         0      0   \n",
      "2         1             0          0              0         0      0   \n",
      "3         1             0          0              0         0      0   \n",
      "4         1             0          0              0         0      0   \n",
      "\n",
      "   Heunggongvirae  Metazoa  Orthornavirae  Pararnavirae  Plantae  Protista  \\\n",
      "0               0        0              0             0        0         0   \n",
      "1               0        0              0             0        0         0   \n",
      "2               0        0              0             0        0         0   \n",
      "3               0        0              0             0        0         0   \n",
      "4               0        0              0             0        0         0   \n",
      "\n",
      "   Riboviria  split  \n",
      "0          0  train  \n",
      "1          0  train  \n",
      "2          0  train  \n",
      "3          0    val  \n",
      "4          0   test  \n",
      "Shape of the DataFrame: (16452, 17)\n"
     ]
    }
   ],
   "source": [
    "# Lets load the dataset into a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "filepath = 'data_gifflar/taxonomy_Kingdom.tsv'\n",
    "#filepath = kingdom_dataset_config['filepath'] #If you've loaded it recently, which you shouldn't\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "multilabel_kingdom_df = pd.read_csv(filepath, sep=\"\\t\")\n",
    "# Display the first few rows of the DataFrame\n",
    "print(multilabel_kingdom_df.head())\n",
    "# Display the shape of the DataFrame\n",
    "print(f\"Shape of the DataFrame: {multilabel_kingdom_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b5a99",
   "metadata": {},
   "source": [
    "That looks fine\n",
    "\n",
    "Let's convert them into graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f975f366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['IUPAC', 'Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae',\n",
      "       'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae',\n",
      "       'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista',\n",
      "       'Riboviria', 'split'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(multilabel_kingdom_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f37ce3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's try the prepare multilabel function from the train_test_split module\n",
    "\n",
    "from glycowork.ml.train_test_split import prepare_multilabel\n",
    "\n",
    "# Prepare the multilabel dataset using the melt function\n",
    "# The melt function is used to transform the DataFrame from wide format to long format\n",
    "kingdom_df_melted = multilabel_kingdom_df.melt(\n",
    "    id_vars=['IUPAC', 'split'],\n",
    "    value_vars=['Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae', 'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae', 'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista', 'Riboviria'],\n",
    "    var_name='Kingdom',\n",
    "    value_name='Association'\n",
    ")\n",
    "\n",
    "# Filter for associations where the glycan belongs to the kingdom (Association == 1)\n",
    "kingdom_df_melted = kingdom_df_melted[kingdom_df_melted['Association'] == 1]\n",
    "\n",
    "# Splitting the dataset using the 'split' column\n",
    "train_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'train'].drop(columns=['split'])\n",
    "val_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'val'].drop(columns=['split'])\n",
    "test_melted_df = kingdom_df_melted[kingdom_df_melted['split'] == 'test'].drop(columns=['split'])\n",
    "\n",
    "# Finally using the prepare_multilabel function to prepare the data for training\n",
    "glycan_train, label_train = prepare_multilabel(train_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "glycan_val, label_val = prepare_multilabel(val_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "glycan_test, label_test = prepare_multilabel(test_melted_df, rank='Kingdom', glycan_col='IUPAC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b68ad291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make them into graphs again, but hyper efficiently this time\n",
    "\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "\n",
    "multilabel_kingdom_loaders = split_data_to_train(\n",
    "    glycan_list_train=glycan_train,\n",
    "    glycan_list_val=glycan_val,\n",
    "    labels_train=label_train,\n",
    "    labels_val=label_val,\n",
    "    batch_size=32,  # Adjust as needed\n",
    "    drop_last=False,\n",
    "    augment_prob=0.0,  # Adjust if you want augmentation for training\n",
    "    generalization_prob=0.0  # Adjust if you want generalization for training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26e9af",
   "metadata": {},
   "source": [
    "#### Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2104e79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training glycans: 11593\n",
      "Number of validation glycans: 3213\n",
      "Number of test glycans: 1646\n",
      "Shape of training labels: 11593 x 15\n",
      "Shape of validation labels: 3213 x 15\n",
      "Shape of test labels: 1646 x 15\n",
      "--- Checking example from train set (Direct) ---\n",
      "Glycan: 3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-Gal(a1-3)Gal\n",
      "Split in original data: train\n",
      "Labels in split data: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Labels in original data: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Labels match!\n",
      "--- Checking example from val set (Direct) ---\n",
      "Glycan: 3,6-Anhydro-L-Gal(a1-3)Gal(b1-4)3,6-Anhydro-L-Gal(a1-3)GalOMe(b1-4)3,6-Anhydro-L-Gal(a1-3)Gal\n",
      "Split in original data: val\n",
      "Labels in split data: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Labels in original data: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Labels match!\n",
      "--- Checking example from test set (Direct) ---\n",
      "Glycan: 3,6-Anhydro-L-Gal(a1-3)GalOMe(b1-4)3,6-Anhydro-L-Gal(a1-3)Gal\n",
      "Split in original data: test\n",
      "Labels in split data: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Labels in original data: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Labels match!\n"
     ]
    }
   ],
   "source": [
    "# Let's validate the split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "filepath = 'data_gifflar/taxonomy_Kingdom.tsv'\n",
    "\n",
    "# 1. Load the original DataFrame\n",
    "multilabel_kingdom_df_original = pd.read_csv(filepath, sep=\"\\t\")\n",
    "kingdom_cols = ['Amoebozoa', 'Animalia', 'Bacteria', 'Bamfordvirae', 'Chromista', 'Euryarchaeota', 'Excavata', 'Fungi', 'Heunggongvirae', 'Metazoa', 'Orthornavirae', 'Pararnavirae', 'Plantae', 'Protista', 'Riboviria']\n",
    "\n",
    "# 2. Split the original DataFrame by 'split'\n",
    "train_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'train']\n",
    "val_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'val']\n",
    "test_df = multilabel_kingdom_df_original[multilabel_kingdom_df_original['split'] == 'test']\n",
    "\n",
    "# 3. Extract glycans and labels directly\n",
    "glycan_train_list = train_df['IUPAC'].tolist()\n",
    "label_train_list = [train_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_train_list = list(zip(*label_train_list)) # Transpose\n",
    "\n",
    "glycan_val_list = val_df['IUPAC'].tolist()\n",
    "label_val_list = [val_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_val_list = list(zip(*label_val_list)) # Transpose\n",
    "\n",
    "glycan_test_list = test_df['IUPAC'].tolist()\n",
    "label_test_list = [test_df[kingdom].values.tolist() for kingdom in kingdom_cols]\n",
    "label_test_list = list(zip(*label_test_list)) # Transpose\n",
    "\n",
    "print(f\"Number of training glycans: {len(glycan_train_list)}\")\n",
    "print(f\"Number of validation glycans: {len(glycan_val_list)}\")\n",
    "print(f\"Number of test glycans: {len(glycan_test_list)}\")\n",
    "print(f\"Shape of training labels: {len(label_train_list)} x {len(label_train_list[0]) if label_train_list else 0}\")\n",
    "print(f\"Shape of validation labels: {len(label_val_list)} x {len(label_val_list[0]) if label_val_list else 0}\")\n",
    "print(f\"Shape of test labels: {len(label_test_list)} x {len(label_test_list[0]) if label_test_list else 0}\")\n",
    "\n",
    "# Now re-run the checking function (modified for this new label extraction)\n",
    "def check_example_direct(glycan_list, label_list, split_name, original_df, kingdom_cols):\n",
    "    if glycan_list:\n",
    "        example_index = 0\n",
    "        example_glycan = glycan_list[example_index]\n",
    "        example_labels_split = list(label_list[example_index])\n",
    "\n",
    "        original_row = original_df[original_df['IUPAC'] == example_glycan].iloc[0]\n",
    "        labels_original = np.array([original_row[col] for col in kingdom_cols], dtype=np.float32).tolist()\n",
    "\n",
    "        print(f\"--- Checking example from {split_name} set (Direct) ---\")\n",
    "        print(f\"Glycan: {example_glycan}\")\n",
    "        print(f\"Split in original data: {original_row['split']}\")\n",
    "        print(f\"Labels in split data: {example_labels_split}\")\n",
    "        print(f\"Labels in original data: {labels_original}\")\n",
    "        if labels_original == example_labels_split:\n",
    "            print(\"Labels match!\")\n",
    "        else:\n",
    "            print(\"Labels DO NOT match!\")\n",
    "    else:\n",
    "        print(f\"{split_name} set is empty.\")\n",
    "\n",
    "check_example_direct(glycan_train_list, label_train_list, 'train', multilabel_kingdom_df_original, kingdom_cols)\n",
    "check_example_direct(glycan_val_list, label_val_list, 'val', multilabel_kingdom_df_original, kingdom_cols)\n",
    "check_example_direct(glycan_test_list, label_test_list, 'test', multilabel_kingdom_df_original, kingdom_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0f833",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multilabel_kingdom_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmultilabel_kingdom_loader\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'multilabel_kingdom_loader' is not defined"
     ]
    }
   ],
   "source": [
    "print(multilabel_kingdom_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e0b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: DataBatch(edge_index=[2, 430], labels=[462], string_labels=[32], y=[480], num_nodes=462, x=[462, 320], batch=[462], ptr=[33])\n",
      "Number of graphs in batch: 32\n",
      "\n",
      "First graph data: Data(edge_index=[2, 8], labels=[9], string_labels=[9], y=[15], x=[9, 320], num_nodes=9)\n",
      "Node features (x): tensor([[ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        ...,\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676]])\n",
      "Edge indices (edge_index): tensor([[1, 2, 3, 4, 5, 6, 7, 8],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7]])\n",
      "Labels (y): tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "String labels: ['Rha', 'a1-3', 'Rha', 'a1-4', 'GalNAcA3Ac', 'a1-3', 'QuiNAc', 'b1-2', 'Rha']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check one of the graphs in the dataloader\n",
    "\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataLoader is called 'multilabel_kingdom_loader'\n",
    "inspected_loader = multilabel_kingdom_loaders_emb['train'] # Access the train DataLoader\n",
    "\n",
    "# Get the first batch of data\n",
    "try:\n",
    "    batch = next(iter(inspected_loader))\n",
    "    print(\"Batch:\", batch)\n",
    "    print(\"Number of graphs in batch:\", batch.num_graphs)\n",
    "\n",
    "    # Extract the first graph from the batch\n",
    "    first_graph_data = batch[2] #change to check other graphs\n",
    "    print(\"\\nFirst graph data:\", first_graph_data)\n",
    "    print(\"Node features (x):\", first_graph_data.x)\n",
    "    print(\"Edge indices (edge_index):\", first_graph_data.edge_index)\n",
    "    print(\"Labels (y):\", first_graph_data.y)\n",
    "    print(\"String labels:\", first_graph_data.string_labels)\n",
    "\n",
    "    # Convert the PyG Data object to a NetworkX graph for visualization\n",
    "    nx_graph = to_networkx(first_graph_data) \n",
    "\n",
    "    # Visualize the NetworkX graph\n",
    "    nx_graph = to_networkx(first_graph_data)\n",
    "    pos = nx.spring_layout(nx_graph)\n",
    "    nx.draw(nx_graph, pos, with_labels=True, node_size=500, node_color=\"red\", font_size=10, font_weight=\"bold\", arrows=False)\n",
    "    plt.title(\"First Graph from DataLoader (PyG)\")\n",
    "    plt.show()\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"The DataLoader is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87dc8a6",
   "metadata": {},
   "source": [
    "## Let's make a function to add embeddings to a dataloader object ||||run on kernel restart||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da9d11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: glm_embeddings_1.pkl\n",
      "Embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Let's load the embeddings again here, so I don't have to jump up and down whenever I reload the kernel)\n",
    "\n",
    "import pickle\n",
    "import os # To check if file exists\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            glm_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72ad776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add GLM embeddings to a dictionary of dataloaders before loading them into the model\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def add_glm_embeddings_to_dataloaders(dataloaders, glm_embeddings):\n",
    "    embedded_loaders = {}\n",
    "    embedding_dim = 320\n",
    "    for split, loader in dataloaders.items():\n",
    "        embedded_data_list = []\n",
    "        for batch in loader:\n",
    "            for graph in batch.to_data_list():\n",
    "                node_embeddings = []\n",
    "                if hasattr(graph, 'string_labels'):\n",
    "                    for label in graph.string_labels:\n",
    "                        if label in glm_embeddings:\n",
    "                            embedding = glm_embeddings[label]\n",
    "                            node_embeddings.append(torch.tensor(embedding))\n",
    "                        else:\n",
    "                            node_embeddings.append(torch.zeros(embedding_dim))\n",
    "                    graph.x = torch.stack(node_embeddings).float()\n",
    "                    #print(f\"Shape of graph.x after adding embeddings: {graph.x.shape}\")\n",
    "                embedded_data_list.append(graph)\n",
    "\n",
    "        embedded_loaders[split] = DataLoader(embedded_data_list, batch_size=32)\n",
    "        print(f\"GLM embeddings added to {split} DataLoader.\")\n",
    "    return embedded_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b83ae5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multilabel_kingdom_loaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# testing the function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m multilabel_kingdom_loaders_emb = add_glm_embeddings_to_dataloaders(\u001b[43mmultilabel_kingdom_loaders\u001b[49m, glm_embeddings)\n",
      "\u001b[31mNameError\u001b[39m: name 'multilabel_kingdom_loaders' is not defined"
     ]
    }
   ],
   "source": [
    "# testing the function\n",
    "\n",
    "multilabel_kingdom_loaders_emb = add_glm_embeddings_to_dataloaders(multilabel_kingdom_loaders, glm_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef713ec2",
   "metadata": {},
   "source": [
    "## Lets look at the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "83e13ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample string_labels from the first graph:\n",
      "['6dTal', 'a1-2', 'Rhaf', 'b1-5', 'Sug']\n"
     ]
    }
   ],
   "source": [
    "# Looking at string labels\n",
    "\n",
    "train_loader = multilabel_kingdom_loaders_emb['train'] \n",
    "batch = next(iter(train_loader))\n",
    "first_graph_labels = batch[2].string_labels\n",
    "print(\"Sample string_labels from the first graph:\")\n",
    "print(first_graph_labels[:20])  # Print the first 20 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e6368a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embeddings from the first graph:\n",
      "tensor([[ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676],\n",
      "        [ 0.9339, -0.7572, -0.5228,  ...,  1.8300,  0.1645,  1.2676]])\n"
     ]
    }
   ],
   "source": [
    "# looking at the embeddings themselves\n",
    "\n",
    "train_loader = multilabel_kingdom_loaders_emb['train'] \n",
    "batch = next(iter(train_loader))\n",
    "first_graph_embeddings = batch[2].x\n",
    "print(\"Sample embeddings from the first graph:\")\n",
    "print(first_graph_embeddings[:20])  # Print the first 20 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef4130ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample keys from glm_embeddings:\n",
      "['!GlcNAc', '-10', '-12', '-2', '-4', '-6', '-8', '0dHex', '1,4-Anhydro-Gal-ol', '1,5-Anhydro-D-AltNAc-ol', '1,5-Anhydro-D-FucN-ol', '1,5-Anhydro-D-Rha4NAc-ol', '1,5-Anhydro-Gal-ol', '1,5-Anhydro-GalNAc-ol', '1,5-Anhydro-Glc-ol', '1,5-Anhydro-Glc-onic', '1,5-Anhydro-GlcN2S-ol', '1,5-Anhydro-GlcN2S6S-ol', '1,5-Anhydro-GlcNAc-ol', '1,5-Anhydro-GlcNAc-onic', '1,5-Anhydro-Man-ol', '1,5-Anhydro-ManNAc-ol', '1,5-Anhydro-Xyl-ol', '1,5-Anhydro-Xyl2F-ol', '1-1', '1-2', '1-3', '1-4', '1-5', '1-6', '1-?', '1b-4', '1dAlt-ol', '1dEry-ol', '2,3-Anhydro-All', '2,3-Anhydro-Man', '2,3-Anhydro-Rib', '2,5-Anhydro-D-Alt-ol', '2,5-Anhydro-D-Alt3S-ol', '2,5-Anhydro-D-Tal', '2,5-Anhydro-Glc', '2,5-Anhydro-L-Man-ol', '2,5-Anhydro-Man', '2,5-Anhydro-Man-ol', '2,5-Anhydro-Man1S-ol', '2,5-Anhydro-Man3S-ol', '2,5-Anhydro-Man6S', '2,5-Anhydro-Tal-ol', '2,5-Anhydro-Tal6P', '2,6-Anhydro-Glc5NAc-ol', '2,6-Anhydro-L-Gul-ol', '2,6-Anhydro-L-Gul-onic', '2,6-Anhydro-Man-ol', '2,6-Anhydro-Tal5NAc-ol', '2,7-Anhydro-Kdo', '2,7-Anhydro-Kdof', '2-3', '2-4', '2-5', '2-6', '2dAraHexA', '3,6-Anhydro-Fruf', '3,6-Anhydro-Gal', '3,6-Anhydro-Gal2S', '3,6-Anhydro-Glc', '3,6-Anhydro-L-Gal', '3,6-Anhydro-L-Gal2Me', '3-1', '3-5', '3-Anhydro-Gal', '3-Anhydro-Gal2S', '3dFuc', '3dGal', '3dLyxHep-ulosaric', '4,7-Anhydro-Kdo', '4,7-Anhydro-KdoOPEtN', '4,8-Anhydro-Kdo', '4-1', '4-5', '4d8dNeu5Ac', '4dAraHex', '4dEry-ol', '4dFuc', '4dGal', '4dNeu5Ac', '4dThrHexNAcA4en', '4eLeg5Ac7Ac', '5-1', '5-2', '5-3', '5-4', '5-5', '5-6', '5dAraf', '5dAraf3Me', '5dLyxf3CFo', '5dLyxf3CMe', '5dPenf3CFo', '6-1', '6-3', '6-4', '6dAll', '6dAll3Me', '6dAlt', '6dAltNAc', '6dAltNAc1PP4N', '6dAltNAc1PP4NAc', '6dAltNAc3PCho', '6dAltOAc', '6dAltf', '6dAltfOAc', '6dFruf', '6dGal', '6dGalNAc', '6dGul', '6dHex', '6dHexN', '6dHexNAc4NAc', '6dManHep', '6dTal', '6dTal1PP', '6dTal2Ac', '6dTal2Ac3Ac', '6dTal2Ac3Ac4Ac', '6dTal2Ac3Me', '6dTal2Ac3Me4Ac', '6dTal2Ac4Ac', '6dTal2Me', '6dTal2Me4Ac', '6dTal3Me', '6dTal4Ac', '6dTalNAc', '6dTalNAc1PP', '6dTalNAc4Ac', '6dTalNAcOAc', '6dTalOAc', '6dTalOAcOAc', '6dTalOAcOMe', '6dTalOMe', '6dTalOMe-ol', '6dTalf', '7dNeu5Ac', '8dNeu5Ac', '8eAci5Ac7Ac', '8eLeg', '8eLeg5Ac7Ac', '8eLeg5Ac7Ac8Ac', '8eLeg5Ac7AcGro', '8eLeg5But7Ac', '8eLegNAcNBut', '8ePse5Ac7Ac', '9dNeu5Ac', '?1-2', '?1-3', '?1-4', '?1-6', '?1-?', '?2-?', 'Abe', 'Abe1PP', 'Abe2Ac', 'AbeOAc', 'Acarbose', 'AcefA', 'Aci5Ac7Ac', 'AcoNAc', 'All', 'All-ol', 'All1S2S3S4S', 'All2Ac3Ac', 'All2S3S4S', 'All3Ac', 'All6Ac', 'AllN', 'AllN1P', 'AllNAc', 'AllNAc6Me', 'AllOMe', 'Alt', 'AltA', 'AltA2N', 'AltA2S', 'AltAN', 'AltNAc', 'AltNAcA', 'AltNAcA1Prop', 'Altf', 'AltfOAc', 'Amikacin', 'Api', 'ApiOAc', 'ApiOMe-ol', 'Apif', 'Ara', 'Ara-ol', 'Ara1Cer2Ac', 'Ara1Me', 'Ara1N4P', 'Ara1P4N', 'Ara1PP']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample keys from glm_embeddings:\")\n",
    "print(list(glm_embeddings.keys())[:200])  # Print the first 20 keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b56aa7",
   "metadata": {},
   "source": [
    "## Copies of glycowork functions for experimentation ||||run on kernel restart||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5658e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SweetNet class\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Union, Literal\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.nn import GraphConv\n",
    "    from torch_geometric.nn import global_mean_pool as gap\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "  raise ImportError(\"<torch or torch_geometric missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from glycowork.glycan_data.loader import lib, download_model \n",
    "\n",
    "class SweetNet(torch.nn.Module):\n",
    "    def __init__(self, lib_size: int, # number of unique tokens for graph nodes\n",
    "                 num_classes: int = 1, # number of output classes (>1 for multilabel)\n",
    "                 hidden_dim: int = 320, # dimension of hidden layers (changed from 128 to 320)\n",
    "                 use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "                ) -> None:\n",
    "        \"given glycan graphs as input, predicts properties via a graph neural network\"\n",
    "        #print(\"Using SweetNet from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "        super(SweetNet, self).__init__()\n",
    "        self.use_external_embeddings = use_external_embeddings\n",
    "        # Convolution operations on the graph\n",
    "        self.conv1 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GraphConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Node embedding\n",
    "        if use_external_embeddings:\n",
    "            self.embedding_dim = hidden_dim\n",
    "        else:\n",
    "            self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    " \n",
    "\n",
    "        #self.item_embedding = torch.nn.Embedding(num_embeddings=lib_size+1, embedding_dim=hidden_dim)\n",
    "\n",
    "        # Fully connected part\n",
    "        self.lin1 = torch.nn.Linear(hidden_dim, 1024)\n",
    "        self.lin2 = torch.nn.Linear(1024, 128)\n",
    "        self.lin3 = torch.nn.Linear(128, num_classes)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(1024)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.act1 = torch.nn.LeakyReLU()\n",
    "        self.act2 = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor,\n",
    "                inference: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \n",
    "        # Getting node features\n",
    "        if self.use_external_embeddings:\n",
    "           # Use external embeddings (already in x)\n",
    "            pass # x is already the embeddings\n",
    "        else:\n",
    "             # Use internal embedding\n",
    "            x = self.item_embedding(x).squeeze(1)\n",
    "            \n",
    "        \n",
    "        # Graph convolution operations (now using GLM embeddings as x)\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = gap(x, batch)\n",
    "\n",
    "        # Fully connected part\n",
    "        x = self.act1(self.bn1(self.lin1(x)))\n",
    "        x_out = self.bn2(self.lin2(x))\n",
    "        x = F.dropout(self.act2(x_out), p = 0.5, training = self.training)\n",
    "\n",
    "        x = self.lin3(x).squeeze(1)\n",
    "\n",
    "        if inference:\n",
    "          return x, x_out\n",
    "        else:\n",
    "          return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03900750",
   "metadata": {},
   "source": [
    "### Other functions I might modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eccb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model function\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import torch\n",
    "    # Choose the correct computing architecture\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "except ImportError:\n",
    "    raise ImportError(\"<torch missing; did you do 'pip install glycowork[ml]'?>\")\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, mean_squared_error, \\\n",
    "    label_ranking_average_precision_score, ndcg_score, roc_auc_score, mean_absolute_error, r2_score\n",
    "from glycowork.motif.annotate import annotate_dataset\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 7, # epochs to wait after last improvement\n",
    "                 verbose: bool = False # whether to print messages\n",
    "                ) -> None:\n",
    "        \"Early stops the training if validation loss doesn't improve after a given patience\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = 0\n",
    "\n",
    "    def __call__(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss: float, model: torch.nn.Module) -> None:\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), 'drive/My Drive/checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def sigmoid(x: float # input value\n",
    "          ) -> float: # sigmoid transformed value\n",
    "    \"Apply sigmoid transformation to input\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def disable_running_stats(model: torch.nn.Module # model to disable batch norm\n",
    "                       ) -> None:\n",
    "    \"Disable batch normalization running statistics\"\n",
    "\n",
    "    def _disable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "\n",
    "def enable_running_stats(model: torch.nn.Module # model to enable batch norm\n",
    "                      ) -> None:\n",
    "    \"Enable batch normalization running statistics\"\n",
    "\n",
    "    def _enable(module):\n",
    "        if isinstance(module, torch.nn.BatchNorm1d) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def train_model(model: torch.nn.Module, # graph neural network for analyzing glycans\n",
    "               dataloaders: Dict[str, torch.utils.data.DataLoader], # dict with 'train' and 'val' loaders\n",
    "               criterion: torch.nn.Module, # PyTorch loss function\n",
    "               optimizer: torch.optim.Optimizer, # PyTorch optimizer, has to be SAM if mode != \"regression\"\n",
    "               scheduler: torch.optim.lr_scheduler._LRScheduler, # PyTorch learning rate decay\n",
    "               num_epochs: int = 25, # number of epochs for training\n",
    "               patience: int = 50, # epochs without improvement until early stop\n",
    "               mode: str = 'classification', # 'classification', 'multilabel', or 'regression'\n",
    "               mode2: str = 'multi', # 'multi' or 'binary' classification\n",
    "               return_metrics: bool = False, # whether to return metrics\n",
    "               use_external_embeddings: bool = False # whether to use external embeddings\n",
    "              ) -> Union[torch.nn.Module, tuple[torch.nn.Module, dict[str, dict[str, list[float]]]]]: # best model from training and the training and validation metrics\n",
    "    \"trains a deep learning model on predicting glycan properties\"\n",
    "\n",
    "    since = time.time()\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "    best_lead_metric = float(\"inf\")\n",
    "\n",
    "    if mode == 'classification':\n",
    "         # Removed auroc from metrics to avoid calculation issues for now\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": []}\n",
    "    elif mode == 'multilabel':\n",
    "        blank_metrics = {\"loss\": [], \"acc\": [], \"mcc\": [], \"lrap\": [], \"ndcg\": []}\n",
    "    else:\n",
    "        blank_metrics = {\"loss\": [], \"mse\": [], \"mae\": [], \"r2\": []}\n",
    "\n",
    "    metrics = {\"train\": copy.deepcopy(blank_metrics), \"val\": copy.deepcopy(blank_metrics)}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_metrics = copy.deepcopy(blank_metrics)\n",
    "            running_metrics[\"weights\"] = []\n",
    "\n",
    "            for data in dataloaders[phase]:\n",
    "                # Get all relevant node attributes\n",
    "                print(f\"Phase: {phase}, Data: {data}\")\n",
    "                print(f\"Phase: {phase}, Data.x: {getattr(data, 'x', None)}\") # Check if x exists and its value\n",
    "                if use_external_embeddings:\n",
    "                    x, y, edge_index, batch = data.x, data.y, data.edge_index, data.batch\n",
    "                else:\n",
    "                    x, y, edge_index, batch = data.labels, data.y, data.edge_index, data.batch\n",
    "                prot = getattr(data, 'train_idx', None)\n",
    "                if prot is not None:\n",
    "                    prot = prot.view(max(batch) + 1, -1).to(device)\n",
    "                x = x.to(device)\n",
    "                if mode == 'multilabel':\n",
    "                    y = y.view(max(batch) + 1, -1).to(device)\n",
    "                else:\n",
    "                    y = y.to(device)\n",
    "                y = y.view(-1, 1) if mode == 'regression' else y\n",
    "                edge_index = edge_index.to(device)\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # First forward pass\n",
    "                    if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                        enable_running_stats(model)\n",
    "                    pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                    loss = criterion(pred, y)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        if mode + mode2 == 'classificationmulti' or mode + mode2 == 'multilabelmulti':\n",
    "                            optimizer.first_step(zero_grad = True)\n",
    "                            # Second forward pass\n",
    "                            disable_running_stats(model)\n",
    "                            second_pred = model(prot, x, edge_index, batch) if prot is not None else model(x, edge_index, batch)\n",
    "                            criterion(second_pred, y).backward()\n",
    "                            optimizer.second_step(zero_grad = True)\n",
    "                        else:\n",
    "                            optimizer.step()\n",
    "                \n",
    "                # Check for single-class batches\n",
    "                unique_classes = torch.unique(y).cpu().numpy()\n",
    "                if len(unique_classes) == 1:\n",
    "                    print(f\"WARNING: Single-class batch detected in {phase} phase at epoch {epoch}!\")\n",
    "                    print(f\"Batch labels: {unique_classes}\")\n",
    "\n",
    "                # Collecting relevant metrics\n",
    "                running_metrics[\"loss\"].append(loss.item())\n",
    "                running_metrics[\"weights\"].append(batch.max().cpu() + 1)\n",
    "\n",
    "                y_det = y.detach().cpu().numpy()\n",
    "                pred_det = pred.cpu().detach().numpy()\n",
    "                if mode == 'classification':\n",
    "                    if mode2 == 'multi':\n",
    "                        pred_proba = np.exp(pred_det) / np.sum(np.exp(pred_det), axis = 1, keepdims = True)  # numpy softmax\n",
    "                        pred2 = np.argmax(pred_det, axis = 1)\n",
    "                    else:\n",
    "                        pred_proba = sigmoid(pred_det)\n",
    "                        pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det, pred2))\n",
    "                    # commented out auroc because it was throwing errors\n",
    "                    #running_metrics[\"auroc\"].append(roc_auc_score(y_det.astype(int), pred_proba if mode2 == 'binary' else pred_proba[:, 1]))\n",
    "                elif mode == 'multilabel':\n",
    "                    pred_proba = sigmoid(pred_det)\n",
    "                    pred2 = (pred_proba >= 0.5).astype(int)\n",
    "                    running_metrics[\"acc\"].append(accuracy_score(y_det.astype(int), pred2))\n",
    "                    running_metrics[\"mcc\"].append(matthews_corrcoef(y_det.flatten(), pred2.flatten()))\n",
    "                    running_metrics[\"lrap\"].append(label_ranking_average_precision_score(y_det.astype(int), pred_proba))\n",
    "                    running_metrics[\"ndcg\"].append(ndcg_score(y_det.astype(int), pred_proba))\n",
    "                else:\n",
    "                    running_metrics[\"mse\"].append(mean_squared_error(y_det, pred_det))\n",
    "                    running_metrics[\"mae\"].append(mean_absolute_error(y_det, pred_det))\n",
    "                    running_metrics[\"r2\"].append(r2_score(y_det, pred_det))\n",
    "\n",
    "            # Averaging metrics at end of epoch\n",
    "            for key in running_metrics:\n",
    "                if key == \"weights\":\n",
    "                    continue\n",
    "                metrics[phase][key].append(np.average(running_metrics[key], weights = running_metrics[\"weights\"]))\n",
    "\n",
    "            if mode == 'classification':\n",
    "                print('{} Loss: {:.4f} Accuracy: {:.4f} MCC: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            elif mode == 'multilabel':\n",
    "                print('{} Loss: {:.4f} LRAP: {:.4f} NDCG: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"acc\"][-1], metrics[phase][\"mcc\"][-1]))\n",
    "            else:\n",
    "                print('{} Loss: {:.4f} MSE: {:.4f} MAE: {:.4f}'.format(phase, metrics[phase][\"loss\"][-1], metrics[phase][\"mse\"][-1], metrics[phase][\"mae\"][-1]))\n",
    "\n",
    "            # Keep best model state_dict\n",
    "            if phase == \"val\":\n",
    "                if metrics[phase][\"loss\"][-1] <= best_loss:\n",
    "                    best_loss = metrics[phase][\"loss\"][-1]\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                    # Extract the lead metric (ACC, LRAP, or MSE) of the new best model\n",
    "                    if mode == 'classification':\n",
    "                        best_lead_metric = metrics[phase][\"acc\"][-1]\n",
    "                    elif mode == 'multilabel':\n",
    "                        best_lead_metric = metrics[phase][\"lrap\"][-1]\n",
    "                    else:\n",
    "                        best_lead_metric = metrics[phase][\"mse\"][-1]\n",
    "\n",
    "                # Check Early Stopping & adjust learning rate if needed\n",
    "                early_stopping(metrics[phase][\"loss\"][-1], model)\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(metrics[phase][\"loss\"][-1])\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    if mode == 'classification':\n",
    "        print('Best val loss: {:4f}, best Accuracy score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    elif mode == 'multilabel':\n",
    "        print('Best val loss: {:4f}, best LRAP score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    else:\n",
    "        print('Best val loss: {:4f}, best MSE score: {:.4f}'.format(best_loss, best_lead_metric))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    if return_metrics:\n",
    "        return model, metrics\n",
    "\n",
    "    # Plot loss & score over the course of training\n",
    "    _, _ = plt.subplots(nrows=2, ncols=1)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(epoch + 1), metrics[\"val\"][\"loss\"])\n",
    "    plt.title('Model Training')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.legend(['Validation Loss'], loc='best')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    if mode == 'classification':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"acc\"])\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "        plt.legend(['Validation Accuracy'], loc='best')\n",
    "    elif mode == 'multilabel':\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"lrap\"])\n",
    "        plt.ylabel('Validation LRAP')\n",
    "        plt.legend(['Validation LRAP'], loc='best')\n",
    "    else:\n",
    "        plt.plot(range(epoch + 1), metrics[\"val\"][\"mse\"])\n",
    "        plt.ylabel('Validation MSE')\n",
    "        plt.legend(['Validation MSE'], loc='best')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57a375b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init_weights function\n",
    "\n",
    "def init_weights(model: torch.nn.Module, # neural network for analyzing glycans\n",
    "                mode: str = 'sparse', # initialization algorithm: 'sparse', 'kaiming', 'xavier'\n",
    "                sparsity: float = 0.1 # proportion of sparsity after initialization\n",
    "               ) -> None:\n",
    "    \"initializes linear layers of PyTorch model with a weight initialization\"\n",
    "    #print(\"Using init_weights from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if isinstance(model, torch.nn.Linear):\n",
    "        if mode == 'sparse':\n",
    "            torch.nn.init.sparse_(model.weight, sparsity = sparsity)\n",
    "        elif mode == 'kaiming':\n",
    "            torch.nn.init.kaiming_uniform_(model.weight)\n",
    "        elif mode == 'xavier':\n",
    "            torch.nn.init.xavier_uniform_(model.weight)\n",
    "        else:\n",
    "            print(\"This initialization option is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a059da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_model function\n",
    "\n",
    "def prep_model(model_type: Literal[\"SweetNet\", \"LectinOracle\", \"LectinOracle_flex\", \"NSequonPred\"], # type of model to create\n",
    "              num_classes: int, # number of unique classes for classification\n",
    "              libr: Optional[Dict[str, int]] = None, # dictionary of form glycoletter:index\n",
    "              trained: bool = False, # whether to use pretrained model\n",
    "              # set hidden_dim to 320 rather than 128 for the pretrained model\n",
    "              # but 128 is the default for the model in the paper\n",
    "              hidden_dim: int = 320, # hidden dimension for the model (SweetNet/LectinOracle only)\n",
    "              use_external_embeddings: bool = False # whether to use external embeddings (GLM or other)\n",
    "             ) -> torch.nn.Module: # initialized PyTorch model\n",
    "    \"wrapper to instantiate model, initialize it, and put it on the GPU\"\n",
    "    #print(\"Using prep_model from notebook cell!\") # Check to see if I am running this in the notebook\n",
    "    if libr is None:\n",
    "      libr = lib\n",
    "    if model_type == 'SweetNet':\n",
    "      model = SweetNet(len(libr), num_classes = num_classes, hidden_dim = hidden_dim, use_external_embeddings = use_external_embeddings)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'sparse'))\n",
    "      if trained:\n",
    "        if hidden_dim != 128:\n",
    "          raise ValueError(\"Hidden dimension must be 128 for pretrained model\")\n",
    "        model_path = download_model(\"glycowork_sweetnet_species.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle':\n",
    "      model = LectinOracle(len(libr), num_classes = num_classes, input_size_prot = int(10*hidden_dim))\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'LectinOracle_flex':\n",
    "      model = LectinOracle_flex(len(libr), num_classes = num_classes)\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"glycowork_lectinoracle_flex.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    elif model_type == 'NSequonPred':\n",
    "      model = NSequonPred()\n",
    "      model = model.apply(lambda module: init_weights(module, mode = 'xavier'))\n",
    "      if trained:\n",
    "        model_path = download_model(\"NSequonPred_batch32.pt\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location = device, weights_only = True))\n",
    "      model = model.to(device)\n",
    "    else:\n",
    "      print(\"Invalid Model Type\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbbca33",
   "metadata": {},
   "source": [
    "## Testing using same framework as iteration 0 (basic kingdom sweetnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# testing the modified SweetNet model on the GlycoWork dataset \n",
    "from glycowork.glycan_data.loader import df_species\n",
    "from glycowork.ml.train_test_split import hierarchy_filter\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "from glycowork.ml import model_training\n",
    "\n",
    "# silence the avalanche of \"undefined\" warnings\n",
    "#import warnings\n",
    "#from sklearn.exceptions import UndefinedMetricWarning\n",
    "#warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "train_x, val_x, train_y, val_y, id_val, class_list, class_converter = hierarchy_filter(df_species,\n",
    "                                                                                       rank = 'Kingdom')\n",
    "\n",
    "dataloaders = split_data_to_train(train_x, val_x, train_y, val_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets split out the training code so I don't have to load the data each time\n",
    "\n",
    "model = prep_model('SweetNet', len(class_list))\n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = len(class_list))\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'classification',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4382d5b4",
   "metadata": {},
   "source": [
    "### Extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdade799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the Kingdoms in the original data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "kingdom_counts = df_species['Kingdom'].value_counts()\n",
    "print(\"Kingdom distribution in the original data:\")\n",
    "print(kingdom_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f112562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Print the first few rows to get an overview\n",
    "print(df_species.head())\n",
    "\n",
    "# Print the data types of the columns\n",
    "print(df_species.info())\n",
    "\n",
    "# Check for duplicated glycans\n",
    "print(f\"Number of unique glycans: {df_species['glycan'].nunique()}\")\n",
    "print(f\"Total number of rows: {len(df_species)}\")\n",
    "\n",
    "# If you suspect a column contains multiple kingdoms:\n",
    "if 'Kingdom' in df_species.columns:  # Replace 'Kingdom' with the actual column name\n",
    "    # Check the first few values of that column\n",
    "    print(df_species['Kingdom'].head())\n",
    "    # If it's a string with delimiters, count the delimiters\n",
    "    if isinstance(df_species['Kingdom'][0], str) and ',' in df_species['Kingdom'][0]: #assuming ',' is the delimiter\n",
    "        print(f\"Example value: {df_species['Kingdom'][0]}\")\n",
    "        print(f\"Number of commas in the first value: {df_species['Kingdom'][0].count(',')}\")\n",
    "\n",
    "    # Check for multiple columns representing kingdoms\n",
    "    for col in df_species.columns:\n",
    "        if col in ['Animalia', 'Bacteria', 'Plantae']:  # Add all possible kingdom column names\n",
    "            print(f\"Column '{col}': Data type = {df_species[col].dtype}, Unique values = {df_species[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set()\n",
    "duplicates_count = 0\n",
    "\n",
    "for item in train_x:\n",
    "    if item in seen:\n",
    "        duplicates_count += 1\n",
    "    else:\n",
    "        seen.add(item)\n",
    "\n",
    "print(f\"Number of duplicates: {duplicates_count}\")\n",
    "print(f\"Number of unique items: {len(seen)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e20752",
   "metadata": {},
   "source": [
    "## Time to train a multi-class multi-label Sweetnet on our properly loaded data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b59cf29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 4.5875 LRAP: 0.0020 NDCG: 0.2996\n",
      "val Loss: 4.7695 LRAP: 0.0000 NDCG: 0.3847\n",
      "Validation loss decreased (0.000000 --> 4.769493).  Saving model ...\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 3.7621 LRAP: 0.0059 NDCG: 0.4535\n",
      "val Loss: 3.4575 LRAP: 0.0183 NDCG: 0.5116\n",
      "Validation loss decreased (4.769493 --> 3.457523).  Saving model ...\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 2.7265 LRAP: 0.0157 NDCG: 0.5061\n",
      "val Loss: 2.1835 LRAP: 0.0228 NDCG: 0.5155\n",
      "Validation loss decreased (3.457523 --> 2.183513).  Saving model ...\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 2.0921 LRAP: 0.0176 NDCG: 0.5523\n",
      "val Loss: 1.9376 LRAP: 0.0228 NDCG: 0.5786\n",
      "Validation loss decreased (2.183513 --> 1.937568).  Saving model ...\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 1.9863 LRAP: 0.0373 NDCG: 0.5784\n",
      "val Loss: 1.8661 LRAP: 0.0228 NDCG: 0.5946\n",
      "Validation loss decreased (1.937568 --> 1.866086).  Saving model ...\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 1.9540 LRAP: 0.0333 NDCG: 0.5770\n",
      "val Loss: 1.8977 LRAP: 0.0274 NDCG: 0.5849\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 1.9140 LRAP: 0.0451 NDCG: 0.5750\n",
      "val Loss: 1.8028 LRAP: 0.0228 NDCG: 0.5796\n",
      "Validation loss decreased (1.866086 --> 1.802828).  Saving model ...\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 1.8871 LRAP: 0.0441 NDCG: 0.5825\n",
      "val Loss: 1.7652 LRAP: 0.0320 NDCG: 0.5967\n",
      "Validation loss decreased (1.802828 --> 1.765217).  Saving model ...\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 1.8117 LRAP: 0.0696 NDCG: 0.5853\n",
      "val Loss: 1.8528 LRAP: 0.0822 NDCG: 0.5437\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 1.7734 LRAP: 0.1471 NDCG: 0.5893\n",
      "val Loss: 1.6825 LRAP: 0.0959 NDCG: 0.5878\n",
      "Validation loss decreased (1.765217 --> 1.682534).  Saving model ...\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 1.7457 LRAP: 0.1167 NDCG: 0.5915\n",
      "val Loss: 1.6395 LRAP: 0.1461 NDCG: 0.6003\n",
      "Validation loss decreased (1.682534 --> 1.639541).  Saving model ...\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 1.7138 LRAP: 0.1794 NDCG: 0.5976\n",
      "val Loss: 1.6184 LRAP: 0.1598 NDCG: 0.6043\n",
      "Validation loss decreased (1.639541 --> 1.618392).  Saving model ...\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 1.6869 LRAP: 0.2176 NDCG: 0.6048\n",
      "val Loss: 1.6134 LRAP: 0.1324 NDCG: 0.5987\n",
      "Validation loss decreased (1.618392 --> 1.613369).  Saving model ...\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 1.6620 LRAP: 0.2775 NDCG: 0.6185\n",
      "val Loss: 1.6103 LRAP: 0.3288 NDCG: 0.6339\n",
      "Validation loss decreased (1.613369 --> 1.610257).  Saving model ...\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 1.6557 LRAP: 0.3137 NDCG: 0.6194\n",
      "val Loss: 1.5976 LRAP: 0.3151 NDCG: 0.6216\n",
      "Validation loss decreased (1.610257 --> 1.597590).  Saving model ...\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 1.6466 LRAP: 0.3294 NDCG: 0.6216\n",
      "val Loss: 1.6276 LRAP: 0.3333 NDCG: 0.6192\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 1.6386 LRAP: 0.3539 NDCG: 0.6224\n",
      "val Loss: 1.5905 LRAP: 0.3379 NDCG: 0.6339\n",
      "Validation loss decreased (1.597590 --> 1.590536).  Saving model ...\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 1.6266 LRAP: 0.4020 NDCG: 0.6373\n",
      "val Loss: 1.5543 LRAP: 0.4338 NDCG: 0.6533\n",
      "Validation loss decreased (1.590536 --> 1.554282).  Saving model ...\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 1.6153 LRAP: 0.3863 NDCG: 0.6365\n",
      "val Loss: 1.5561 LRAP: 0.3653 NDCG: 0.6298\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 1.6055 LRAP: 0.3941 NDCG: 0.6337\n",
      "val Loss: 1.5646 LRAP: 0.4338 NDCG: 0.6551\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 1.5913 LRAP: 0.4314 NDCG: 0.6507\n",
      "val Loss: 1.5414 LRAP: 0.4521 NDCG: 0.6602\n",
      "Validation loss decreased (1.554282 --> 1.541361).  Saving model ...\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 1.5834 LRAP: 0.4461 NDCG: 0.6479\n",
      "val Loss: 1.5643 LRAP: 0.4749 NDCG: 0.6627\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 1.5813 LRAP: 0.4529 NDCG: 0.6511\n",
      "val Loss: 1.5690 LRAP: 0.4429 NDCG: 0.6487\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 1.5798 LRAP: 0.4647 NDCG: 0.6583\n",
      "val Loss: 1.5364 LRAP: 0.4429 NDCG: 0.6578\n",
      "Validation loss decreased (1.541361 --> 1.536418).  Saving model ...\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 1.5413 LRAP: 0.4961 NDCG: 0.6682\n",
      "val Loss: 1.5469 LRAP: 0.4886 NDCG: 0.6739\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 1.5417 LRAP: 0.4892 NDCG: 0.6633\n",
      "val Loss: 1.5325 LRAP: 0.4566 NDCG: 0.6624\n",
      "Validation loss decreased (1.536418 --> 1.532516).  Saving model ...\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 1.5413 LRAP: 0.4873 NDCG: 0.6604\n",
      "val Loss: 1.5393 LRAP: 0.5023 NDCG: 0.6800\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 1.5307 LRAP: 0.4775 NDCG: 0.6590\n",
      "val Loss: 1.5396 LRAP: 0.4977 NDCG: 0.6816\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 1.5333 LRAP: 0.4931 NDCG: 0.6594\n",
      "val Loss: 1.5219 LRAP: 0.4247 NDCG: 0.6471\n",
      "Validation loss decreased (1.532516 --> 1.521912).  Saving model ...\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 1.5256 LRAP: 0.4961 NDCG: 0.6588\n",
      "val Loss: 1.5226 LRAP: 0.5297 NDCG: 0.6765\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 1.5194 LRAP: 0.5088 NDCG: 0.6674\n",
      "val Loss: 1.5136 LRAP: 0.5297 NDCG: 0.6872\n",
      "Validation loss decreased (1.521912 --> 1.513589).  Saving model ...\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 1.5174 LRAP: 0.4912 NDCG: 0.6634\n",
      "val Loss: 1.5060 LRAP: 0.5205 NDCG: 0.6786\n",
      "Validation loss decreased (1.513589 --> 1.506016).  Saving model ...\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 1.4951 LRAP: 0.5225 NDCG: 0.6706\n",
      "val Loss: 1.5205 LRAP: 0.4932 NDCG: 0.6672\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 1.4968 LRAP: 0.5382 NDCG: 0.6787\n",
      "val Loss: 1.4880 LRAP: 0.5205 NDCG: 0.6786\n",
      "Validation loss decreased (1.506016 --> 1.488009).  Saving model ...\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 1.4846 LRAP: 0.5235 NDCG: 0.6710\n",
      "val Loss: 1.4901 LRAP: 0.5023 NDCG: 0.6684\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 1.4867 LRAP: 0.5225 NDCG: 0.6737\n",
      "val Loss: 1.4955 LRAP: 0.5342 NDCG: 0.6831\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 1.4758 LRAP: 0.5235 NDCG: 0.6736\n",
      "val Loss: 1.4895 LRAP: 0.5571 NDCG: 0.6928\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 1.4731 LRAP: 0.5333 NDCG: 0.6754\n",
      "val Loss: 1.4777 LRAP: 0.4886 NDCG: 0.6647\n",
      "Validation loss decreased (1.488009 --> 1.477740).  Saving model ...\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 1.5110 LRAP: 0.5196 NDCG: 0.6726\n",
      "val Loss: 1.5062 LRAP: 0.5479 NDCG: 0.6893\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 1.4768 LRAP: 0.5216 NDCG: 0.6767\n",
      "val Loss: 1.4914 LRAP: 0.5068 NDCG: 0.6637\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 1.4576 LRAP: 0.5314 NDCG: 0.6755\n",
      "val Loss: 1.4678 LRAP: 0.5479 NDCG: 0.6848\n",
      "Validation loss decreased (1.477740 --> 1.467754).  Saving model ...\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 1.4499 LRAP: 0.5402 NDCG: 0.6740\n",
      "val Loss: 1.4840 LRAP: 0.4932 NDCG: 0.6625\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 1.4494 LRAP: 0.5539 NDCG: 0.6739\n",
      "val Loss: 1.4793 LRAP: 0.5479 NDCG: 0.6819\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 1.4639 LRAP: 0.5412 NDCG: 0.6732\n",
      "val Loss: 1.4676 LRAP: 0.5616 NDCG: 0.6768\n",
      "Validation loss decreased (1.467754 --> 1.467578).  Saving model ...\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 1.4532 LRAP: 0.5578 NDCG: 0.6776\n",
      "val Loss: 1.4653 LRAP: 0.5342 NDCG: 0.6671\n",
      "Validation loss decreased (1.467578 --> 1.465347).  Saving model ...\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 1.4452 LRAP: 0.5500 NDCG: 0.6764\n",
      "val Loss: 1.4562 LRAP: 0.5479 NDCG: 0.6687\n",
      "Validation loss decreased (1.465347 --> 1.456183).  Saving model ...\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 1.4246 LRAP: 0.5686 NDCG: 0.6791\n",
      "val Loss: 1.4689 LRAP: 0.5114 NDCG: 0.6491\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 1.4246 LRAP: 0.5618 NDCG: 0.6724\n",
      "val Loss: 1.4442 LRAP: 0.5479 NDCG: 0.6737\n",
      "Validation loss decreased (1.456183 --> 1.444197).  Saving model ...\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 1.4207 LRAP: 0.5471 NDCG: 0.6693\n",
      "val Loss: 1.4531 LRAP: 0.5388 NDCG: 0.6639\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 1.4310 LRAP: 0.5627 NDCG: 0.6755\n",
      "val Loss: 1.4612 LRAP: 0.5297 NDCG: 0.6608\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 1.4155 LRAP: 0.5578 NDCG: 0.6752\n",
      "val Loss: 1.4692 LRAP: 0.5297 NDCG: 0.6563\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 1.4250 LRAP: 0.5461 NDCG: 0.6658\n",
      "val Loss: 1.4607 LRAP: 0.5662 NDCG: 0.6715\n",
      "EarlyStopping counter: 4 out of 50\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 1.4190 LRAP: 0.5510 NDCG: 0.6720\n",
      "val Loss: 1.4831 LRAP: 0.5068 NDCG: 0.6448\n",
      "EarlyStopping counter: 5 out of 50\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 1.4085 LRAP: 0.5735 NDCG: 0.6788\n",
      "val Loss: 1.4551 LRAP: 0.5571 NDCG: 0.6625\n",
      "EarlyStopping counter: 6 out of 50\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 1.4077 LRAP: 0.5510 NDCG: 0.6714\n",
      "val Loss: 1.4510 LRAP: 0.5708 NDCG: 0.6679\n",
      "EarlyStopping counter: 7 out of 50\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 1.3924 LRAP: 0.5706 NDCG: 0.6832\n",
      "val Loss: 1.4455 LRAP: 0.5753 NDCG: 0.6655\n",
      "EarlyStopping counter: 8 out of 50\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 1.4021 LRAP: 0.5686 NDCG: 0.6758\n",
      "val Loss: 1.4570 LRAP: 0.5479 NDCG: 0.6629\n",
      "EarlyStopping counter: 9 out of 50\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 1.3979 LRAP: 0.5882 NDCG: 0.6804\n",
      "val Loss: 1.4558 LRAP: 0.5434 NDCG: 0.6613\n",
      "EarlyStopping counter: 10 out of 50\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 1.3837 LRAP: 0.5784 NDCG: 0.6793\n",
      "val Loss: 1.4516 LRAP: 0.5525 NDCG: 0.6690\n",
      "EarlyStopping counter: 11 out of 50\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 1.3965 LRAP: 0.5725 NDCG: 0.6792\n",
      "val Loss: 1.4488 LRAP: 0.5753 NDCG: 0.6718\n",
      "EarlyStopping counter: 12 out of 50\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 1.4040 LRAP: 0.5922 NDCG: 0.6817\n",
      "val Loss: 1.4556 LRAP: 0.5342 NDCG: 0.6586\n",
      "EarlyStopping counter: 13 out of 50\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 1.3875 LRAP: 0.5735 NDCG: 0.6818\n",
      "val Loss: 1.4564 LRAP: 0.5662 NDCG: 0.6639\n",
      "EarlyStopping counter: 14 out of 50\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 1.3923 LRAP: 0.5686 NDCG: 0.6713\n",
      "val Loss: 1.4460 LRAP: 0.5571 NDCG: 0.6656\n",
      "EarlyStopping counter: 15 out of 50\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 1.3873 LRAP: 0.5706 NDCG: 0.6787\n",
      "val Loss: 1.4519 LRAP: 0.5571 NDCG: 0.6668\n",
      "EarlyStopping counter: 16 out of 50\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 1.3917 LRAP: 0.5735 NDCG: 0.6772\n",
      "val Loss: 1.4486 LRAP: 0.5616 NDCG: 0.6651\n",
      "EarlyStopping counter: 17 out of 50\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 1.3932 LRAP: 0.5676 NDCG: 0.6749\n",
      "val Loss: 1.4534 LRAP: 0.5297 NDCG: 0.6625\n",
      "EarlyStopping counter: 18 out of 50\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 1.3999 LRAP: 0.5755 NDCG: 0.6790\n",
      "val Loss: 1.4457 LRAP: 0.5662 NDCG: 0.6681\n",
      "EarlyStopping counter: 19 out of 50\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 1.3789 LRAP: 0.5873 NDCG: 0.6842\n",
      "val Loss: 1.4504 LRAP: 0.5434 NDCG: 0.6599\n",
      "EarlyStopping counter: 20 out of 50\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 1.3895 LRAP: 0.5873 NDCG: 0.6839\n",
      "val Loss: 1.4503 LRAP: 0.5616 NDCG: 0.6623\n",
      "EarlyStopping counter: 21 out of 50\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 1.3988 LRAP: 0.5716 NDCG: 0.6778\n",
      "val Loss: 1.4472 LRAP: 0.5662 NDCG: 0.6650\n",
      "EarlyStopping counter: 22 out of 50\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 1.3959 LRAP: 0.5706 NDCG: 0.6790\n",
      "val Loss: 1.4471 LRAP: 0.5662 NDCG: 0.6690\n",
      "EarlyStopping counter: 23 out of 50\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 1.3795 LRAP: 0.5676 NDCG: 0.6775\n",
      "val Loss: 1.4500 LRAP: 0.5571 NDCG: 0.6600\n",
      "EarlyStopping counter: 24 out of 50\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 1.3854 LRAP: 0.5882 NDCG: 0.6801\n",
      "val Loss: 1.4471 LRAP: 0.5662 NDCG: 0.6650\n",
      "EarlyStopping counter: 25 out of 50\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 1.3889 LRAP: 0.5784 NDCG: 0.6762\n",
      "val Loss: 1.4497 LRAP: 0.5388 NDCG: 0.6579\n",
      "EarlyStopping counter: 26 out of 50\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 1.3917 LRAP: 0.5863 NDCG: 0.6770\n",
      "val Loss: 1.4504 LRAP: 0.5388 NDCG: 0.6585\n",
      "EarlyStopping counter: 27 out of 50\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 1.3769 LRAP: 0.5775 NDCG: 0.6756\n",
      "val Loss: 1.4471 LRAP: 0.5434 NDCG: 0.6617\n",
      "EarlyStopping counter: 28 out of 50\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 1.3903 LRAP: 0.5676 NDCG: 0.6736\n",
      "val Loss: 1.4492 LRAP: 0.5525 NDCG: 0.6629\n",
      "EarlyStopping counter: 29 out of 50\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 1.3933 LRAP: 0.5775 NDCG: 0.6760\n",
      "val Loss: 1.4480 LRAP: 0.5616 NDCG: 0.6651\n",
      "EarlyStopping counter: 30 out of 50\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 1.3830 LRAP: 0.5725 NDCG: 0.6734\n",
      "val Loss: 1.4479 LRAP: 0.5571 NDCG: 0.6670\n",
      "EarlyStopping counter: 31 out of 50\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 1.3769 LRAP: 0.5745 NDCG: 0.6746\n",
      "val Loss: 1.4518 LRAP: 0.5525 NDCG: 0.6597\n",
      "EarlyStopping counter: 32 out of 50\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 1.3902 LRAP: 0.5706 NDCG: 0.6734\n",
      "val Loss: 1.4519 LRAP: 0.5388 NDCG: 0.6582\n",
      "EarlyStopping counter: 33 out of 50\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 1.3965 LRAP: 0.5667 NDCG: 0.6751\n",
      "val Loss: 1.4553 LRAP: 0.5388 NDCG: 0.6588\n",
      "EarlyStopping counter: 34 out of 50\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 1.3852 LRAP: 0.5647 NDCG: 0.6746\n",
      "val Loss: 1.4488 LRAP: 0.5708 NDCG: 0.6659\n",
      "EarlyStopping counter: 35 out of 50\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 1.3886 LRAP: 0.5794 NDCG: 0.6765\n",
      "val Loss: 1.4477 LRAP: 0.5525 NDCG: 0.6628\n",
      "EarlyStopping counter: 36 out of 50\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 1.3984 LRAP: 0.5559 NDCG: 0.6765\n",
      "val Loss: 1.4607 LRAP: 0.5297 NDCG: 0.6529\n",
      "EarlyStopping counter: 37 out of 50\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 1.4007 LRAP: 0.5775 NDCG: 0.6775\n",
      "val Loss: 1.4467 LRAP: 0.5662 NDCG: 0.6652\n",
      "EarlyStopping counter: 38 out of 50\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 1.3869 LRAP: 0.5696 NDCG: 0.6792\n",
      "val Loss: 1.4606 LRAP: 0.5342 NDCG: 0.6529\n",
      "EarlyStopping counter: 39 out of 50\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 1.3752 LRAP: 0.5706 NDCG: 0.6842\n",
      "val Loss: 1.4556 LRAP: 0.5342 NDCG: 0.6545\n",
      "EarlyStopping counter: 40 out of 50\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 1.3884 LRAP: 0.5784 NDCG: 0.6775\n",
      "val Loss: 1.4477 LRAP: 0.5479 NDCG: 0.6655\n",
      "EarlyStopping counter: 41 out of 50\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 1.3938 LRAP: 0.5873 NDCG: 0.6808\n",
      "val Loss: 1.4467 LRAP: 0.5753 NDCG: 0.6689\n",
      "EarlyStopping counter: 42 out of 50\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 1.3927 LRAP: 0.5618 NDCG: 0.6723\n",
      "val Loss: 1.4501 LRAP: 0.5571 NDCG: 0.6655\n",
      "EarlyStopping counter: 43 out of 50\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 1.3884 LRAP: 0.5725 NDCG: 0.6739\n",
      "val Loss: 1.4487 LRAP: 0.5616 NDCG: 0.6664\n",
      "EarlyStopping counter: 44 out of 50\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 1.3793 LRAP: 0.5814 NDCG: 0.6806\n",
      "val Loss: 1.4486 LRAP: 0.5571 NDCG: 0.6634\n",
      "EarlyStopping counter: 45 out of 50\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 1.3880 LRAP: 0.5696 NDCG: 0.6791\n",
      "val Loss: 1.4513 LRAP: 0.5525 NDCG: 0.6617\n",
      "EarlyStopping counter: 46 out of 50\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 1.3773 LRAP: 0.5725 NDCG: 0.6730\n",
      "val Loss: 1.4491 LRAP: 0.5616 NDCG: 0.6659\n",
      "EarlyStopping counter: 47 out of 50\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 1.3940 LRAP: 0.5775 NDCG: 0.6816\n",
      "val Loss: 1.4484 LRAP: 0.5662 NDCG: 0.6628\n",
      "EarlyStopping counter: 48 out of 50\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 1.3852 LRAP: 0.5647 NDCG: 0.6778\n",
      "val Loss: 1.4486 LRAP: 0.5845 NDCG: 0.6691\n",
      "EarlyStopping counter: 49 out of 50\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 1.4055 LRAP: 0.5814 NDCG: 0.6805\n",
      "val Loss: 1.4458 LRAP: 0.5753 NDCG: 0.6727\n",
      "EarlyStopping counter: 50 out of 50\n",
      "Early stopping\n",
      "Training complete in 1m 17s\n",
      "Best val loss: 1.444197, best LRAP score: 0.8895\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHHCAYAAABeLEexAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAgjhJREFUeJzt3Qd4U2UXB/BD96aDTdl7b5AhQ/gYIsgQBRFZisoeDhBxs0RRGaKogANkKMhQQGSKsjey96YUKHTQne/5n5KYLmghbdLk//OJSW5ubm5uQu/Jec/7vrkMBoNBiIiIiByYk7V3gIiIiMjaGBARERGRw2NARERERA6PARERERE5PAZERERE5PAYEBEREZHDY0BEREREDo8BERERETk8BkRERETk8BgQEZHV5cqVS959991MP+/MmTP63Dlz5oi1Yf+xLw8C+4/n4v0QkXUwICKiZCdlXDZv3pzqcczyU6RIEX38iSeekJyiePHipvd1r4stBFVEZD0uVnxtIrJBHh4eMm/ePGnUqFGy5Rs3bpQLFy6Iu7u75CSfffaZREREmO7//vvv8tNPP8mnn34qefLkMS1v0KDBQ73OW2+9JSNHjnyg5/bo0UO6du2a444tkT1hQEREyTz++OOyaNEimTJliri4/PcnAkFSrVq1JDQ0VHKSDh06JLt/5coVDYiwHNmj9ERGRoq3t3eGXwfHyvx4ZYazs7NeiMh62GRGRMl069ZNrl+/LmvWrDEti42NlZ9//lmeffbZdIOHESNGaJMashzlypWTjz/+WJvZzMXExMiwYcMkb9684uvrK+3bt9esU1ouXrwoffr0kfz58+s2K1WqJLNmzZKs0KtXL/Hx8ZGTJ09qQIh96969uz72119/SZcuXaRo0aK6H3iPeA937ty5bw0R7g8cOFB+/fVXqVy5sul9rFq16r41RAjW0DSJ5su6detq5q5kyZLy/fffp9r//fv3S5MmTcTT01OCg4Plww8/lNmzZ7MuiSgTmCEiomRwIq5fv75mUdq0aaPLVq5cKbdu3dJmHWSOzCHoQWCzfv166du3r1SvXl1Wr14tr732mgY1aJoyeuGFF+THH3/UwApNVOvWrZO2bdum2oerV6/KI488YgooEEBhH7D927dvy9ChQy3+vuPj46VVq1baVIhgzsvLS5cjWxYVFSWvvPKKBAUFyfbt22Xq1KkayOGx+0FAs3jxYunfv78GWjh+nTt3lnPnzun27uXEiRPy1FNP6fvu2bOnBoQI3pCpQ2AFOMbNmjXTYzVq1CjNan3zzTdsfiPKLAMRkcFgmD17NtI5hh07dhimTZtm8PX1NURFReljXbp0MTRr1kxvFytWzNC2bVvT83799Vd93ocffphse0899ZQhV65chhMnTuj9vXv36nr9+/dPtt6zzz6ry9955x3Tsr59+xoKFixoCA0NTbZu165dDblz5zbt1+nTp/W52PeMmjRpkj4HzzXq2bOnLhs5cmSq9Y2vZW78+PH63s6ePWtahv1P+ScV993c3EzHAPbt26fLp06dmurYm+8TjjOWbdq0ybQsJCTE4O7ubhgxYoRp2aBBg3Rf9uzZY1p2/fp1Q2BgYKptElH62GRGRKk8/fTT2iS0YsUKCQ8P1+v0mstQpIz6l8GDBydbjiY0xATI7BjXg5Trpcz24Dm//PKLtGvXTm+jZsl4QQYHmardu3dLVkAWKCU0Q5k3DWI/kN3Cvu3Zs+e+22zRooWUKlXKdL9q1ari5+cnp06duu9zK1asKI8++qjpPjJlaI40fy6a35DRQ2bOKDAw0NTkR0QZwyYzIkoFJ16cyFFIjeaihIQEbbpJy9mzZ6VQoULaHGSuQoUKpseN105OTsmCA8AJ3ty1a9ckLCxMZs6cqZe0hISEiKWhIBr1Nymhaevtt9+WZcuWyc2bN5M9huDsflB7lFJAQECqbT3oc3FcERClVLp06ftun4j+w4CIiNKEjNCLL76ovbJQS+Tv758tr5uYmKjXzz33nNbNpAVZFktDzQ0CNnMIBP/3v//JjRs35I033pDy5ctrjQ7qdlDLY9zXe0mv91jKgnNLP5eIMocBERGlqWPHjvLSSy/J1q1bZcGCBemuV6xYMfnzzz+1ac08S3TkyBHT48ZrBBDoyWWeFTp69Giy7Rl7oCEYQZbKmg4cOCDHjh2T7777Tp5//nnTcvMeeNaG44ri65TSWkZE6WMNERGlCd3QZ8yYod3JUc+THnRTR/Aybdq0ZMvRuww9n4w91YzXKXupYeDElFkR9MJCHdHBgwdTvR6a1LKLMUNjnpHB7c8//1xsBeqqtmzZInv37jUtQ0Zr7ty5Vt0vopyGGSIiSld6TVbmECyh2/fo0aN1zJtq1arJH3/8IUuXLtWCaWPNEIp+McbRF198obU3KExeu3ZtmpmMCRMmaDf+evXqabMdiotxkkcxNbJRuJ0d0ESG/X/11Ve1mQzF0AjUMlL/k11ef/11HcoATXuDBg0ydbtH/RGO04POr0bkaJghIqKHgrobFBwj+EFvNFwfOnRIJk2aJJMnT062LsbRQS8z9IzCiTwuLk5+++23VNvEYIwY76d37946hg/GIkJWBif4iRMnZtt7c3V1leXLl2swN378eHnvvfekTJkyaQ6OaC0YKBLBI4rYx40bpxk3BLIY1BIwoCMR3V8u9L3PwHpERJSDIDD96quvdB43TgtCdH/MEBER5XAppxHB1Cs//PCDjrrNYIgoY1hDRESUw2EcoqZNm2qzGaY9+fbbb3WKkzFjxlh714hyDAZEREQ5HHr6YfJdDGSJIuqaNWtqUNS4cWNr7xpRjsEaIiIiInJ4rCEiIiIih8eAiIiIiBwea4jSgOkFLl26pNMHcFAzIiKinAFVQJhGCBNOp5yb8H4YEKUBwRAGOyMiIqKc5/z58xIcHJyp5zAgSoNxgkocUAzVT0RERLYPw00goWE+0XRGMSBKg7GZDMEQAyIiIqKc5UHKXVhUTURERA6PARERERE5PAZERERE5PBYQ5SNQsKj5cjlcPF2d5ZaxQKtvTtERFk6fElsbKy1d4PsjKura5ZNWMyAKBttOhYqry7aJ4+WySM/9K1n7d0hIsoSCIROnz6tQRGRpfn7+0uBAgUsPk4gA6JsFOjtqtdhUXHW3hUioiwbGO/y5cv6Kx7dnzM7OB7Rvb5bUVFREhISovcLFiwolsSAKBsFeLnp9Y1IppGJyD7Fx8frSQsjBXt5eVl7d8jOeHp66jWConz58lm0+YyhuxUCoptRDIiIyD4lJCTotZtb0t87IkszBtpxcZZtbWFAlI0CvJP+QETFJkh0XNIfDSIie8R5ICmnfbcYEGUjPw8XcXZK+iBZR0RERGQ7GBBlc1TLOiIiIvvUtGlTGTp0qOl+8eLF5bPPPrvveeHXX3996Ne21HYcGQOibBbgldTTjHVERES2oV27dtK6des0H/vrr7802Ni/f3+mt7tjxw7p16+fWNK7774r1atXT7UcPfvatGkjWWnOnDna5d1eMSCyUh0RM0RERLahb9++smbNGrlw4UKqx2bPni21a9eWqlWrZnq7efPmzbaedhiXx93dPVtey14xIMpmgXebzMKYISIisglPPPGEBi/IgJiLiIiQRYsWacB0/fp16datmxQuXFiDnCpVqshPP/10z+2mbDI7fvy4NG7cWDw8PKRixYoahKX0xhtvSNmyZfU1SpYsKWPGjDH1psL+vffee7Jv3z7NWuFi3OeUTWYHDhyQxx57TLupBwUFaaYK78eoV69e0qFDB/n44491PJ+goCAZMGDAQ/XcOnfunDz55JPi4+Mjfn5+8vTTT8vVq1dNj2O/mzVrJr6+vvp4rVq1ZOfOnfrY2bNnNVMXEBAg3t7eUqlSJfn9998lO3EcIqtliFhUTUSOMZjeHSv1qvV0dc5QjyQXFxd5/vnnNbgYPXq06TkIhjCMAAIhBBM4gSNgwcn8t99+kx49ekipUqWkbt26930NjNrdqVMnyZ8/v2zbtk1u3bqVrN7ICMEC9gPjOCGoefHFF3XZ66+/Ls8884wcPHhQVq1aJX/++aeunzt37lTbiIyMlFatWkn9+vW12Q5j9rzwwgsycODAZEHf+vXrNRjC9YkTJ3T7aI7Da2YW3p8xGNq4caOOR4UAC9vcsGGDrtO9e3epUaOGzJgxQ8cP2rt3r07FAVgXI5xv2rRJA6JDhw7ptrITAyIrjVbNGiIicgQIhiq+vdoqr33o/Vbi5Zax01yfPn1k0qRJejJHcbSxuaxz584adODy6quvmtYfNGiQrF69WhYuXJihgAgBzJEjR/Q5CHZg3Lhxqep+3nrrrWQZJrzm/PnzNSBCtgdBAgI4NJGlZ968eRIdHS3ff/+9Bhcwbdo0zcBMnDhRgzJANgbLEZyUL19e2rZtK2vXrn2ggAjPQwCHKVswQjng9ZHpQVBWp04dzSC99tpr+lpQpkwZ0/PxGI41Mm+A7Fh2Y5NZNmMvMyIi24OTdIMGDWTWrFl6HxkTFFSjuQyQKfrggw/0hB0YGKiBCYIbnMgz4vDhwxooGIMhQAYnpQULFkjDhg014MFrIEDK6GuYv1a1atVMwRBgm8jiHD161LQMwYr5SM8FCxY0TYuRWcb3ZwyGAM2CKMLGYzB8+HDNVLVo0UImTJggJ0+eNK07ePBg+fDDD3U/33nnnQcqYn9YzBBlM45WTUSOBM1WyNRY67UzA8EPMj/Tp0/X7BCaw5o0aaKPIXv0+eefa00QgiIEG2jyQjOPpWzZskWblVAnhCYvZKWQHfrkk08kKxibq4xy5cqVpRPyoofcs88+q82NK1eu1MAH769jx44aKOE947E//vhDxo8fr+8bn0d2YYYomwXerSFiQEREjgAnWTRbWeOS2RGNUQSMyWjR5ITmHjSjGbfx999/a43Mc889p9kXNOkcO3Ysw9uuUKGCnD9/XrvHG23dujXZOv/8848UK1ZM65jQsw1NSig2NocpUYzTo9zrtVDAjFoiI+w/3lu5cuUkK1S4+/5wMUIdUFhYmGaKjFAwPmzYMA16UFOFwNMI2aWXX35ZFi9eLCNGjJCvv/5ashMDIisVVd9kUTURkU1BExWKgEeNGqWBC3piGSE4Qa8wBC1oAnrppZeS9aC6HzQTIRjo2bOnBitojkPgYw6vgeYxZE3QnDRlyhRZsmRJsnVQV4Q6HRQkh4aGSkxMTKrXQpYJPdnwWijCRtE0Mi0oAjfWDz2ohIQEfW3zC44H3h8yZ3jt3bt3y/bt27VQHRk2BHd37tzRom4UWCPIQ4CG2iIEUoBsG5og8d7wfOyz8bHswoDISgMzsoaIiMj2oNns5s2b2nxjXu+DWp6aNWvqchRdo8YH3dYzCtkZBDcIDFCEjSaisWPHJlunffv2mj1B4IDeXgi+0O3eHAqPMYgkuq9jqIC0uv6jyz6Cixs3bmgx81NPPSXNmzfXAuqHFRERoT3FzC8o1kYmbenSpVqojaEFECAhi4aaKECtEoYuQJCEwBDZOBSUo3nQGGihpxmCILw/rPPFF19IdsplQJ9ISub27dvadotukeheadFtR8dJ1Xf/0NtHPmgtHpls4yYismXo3YRf+SVKlNAsBVF2fsce5vzNDFE283V3EZe7E7yyjoiIiMg2MCCyxgSvnL6DiIjIpjAgsuYEryysJiIisgkMiKw5OCObzIiIiGwCAyIrjkXECV6JyF6xvw7ltO8WAyIrYA0REdkr41QQlhzBmchcVFRUmiNtPyxO3WHVGiL+wSAi+4KJRzEOzrVr1/SEhfF3iCyVGUIwhPnWMEea+TxslsCAyKo1RCyqJiL760mLSUIxTkzKaSeILAHBEAbGtDQGRFbAGiIismeYbwvTULDZjCwNWUdLZ4ZsLiBatWqVziPTqFEjvY/ZhjGxGyaFw20MB24vWENERPYOTWUcqZpyEptp3H3ttdd0yG04cOCAznT7+OOPa9p1+PDhYo9NZqwhIiIisg02kyFC4INsEPzyyy/yxBNPyLhx43TWWwRG9iSQ4xARERHZFCdbanM2dqX7888/pWXLlno7MDDQlDmyFwHeSb3MouMS5U5sgrV3h4iIyOHZTIYItUNoGmvYsKFs375dFixYoMuPHTsmwcHBYk983F3E1TmXxCUYdIJXTzdPa+8SERGRQ7OZDNG0adN0/Iqff/5ZZsyYIYULF9blK1eulNatWz/wdidMmKDdQIcOHSq2Avvjb2w2Yx0RERGR1dlMhqho0aKyYsWKVMs//fTTB97mjh075KuvvpKqVauKLdYRXQuP0QwRERERWZfNZIhQPI3eZUZLly6VDh06yJtvvvlAY1lERERI9+7dteu+LXbZN9YR3eTgjERERFZnMwHRSy+9pPVCcOrUKenatasO/75o0SJ5/fXXM729AQMGSNu2baVFixb3XTcmJkYLt80v2TU4I7veExERWZ/NBEQIhqpXr663EQQ1btxY5s2bJ3PmzNFu+Jkxf/58zTiNHz8+Q+tjvdy5c5suRYoUkazGGiIiIiLb4WRLk7YlJiaaut0bxx5CcBIaGprh7Zw/f16GDBkic+fOzfAoqaNGjZJbt26ZLthGdo1FxBoiIiIi67OZouratWvLhx9+qE1cGzdu1J5mxgEb8+fPn+Ht7Nq1S2fCrVmzpmlZQkKCbNq0SXuyoXks5Two7u7uerHG9B2sISIiIrI+mwmIPvvsMy2C/vXXX2X06NFSunRpXY5u+A0aNMjwdpo3b56sOBt69+4t5cuXlzfeeCPLJoXLrEBjUTWbzIiIiKzOZgIidI1PGcjApEmTMhXE+Pr6SuXKlZMt8/b2lqCgoFTLrYk1RERERLbDZgIi8yavw4cP623MbWbe9GVPWENERERkO2wmIELdzzPPPKP1Q/7+/rosLCxMmjVrpr3G8ubN+8Db3rBhg9gaU7d7BkRERERWZzO9zAYNGqSDKf77779y48YNvRw8eFDHBBo8eLDYG2NRNSd4JSIisj6byRCtWrVKu9tXqFDBtAxNZtOnTzfNfG9PvN2cTRO83oiKlcKc4JWIiMhqbCZDhDGIXF2Tel6ZwzLj+ET2BBO8BhjriFhYTUREZFU2ExA99thjOqDipUuXTMsuXrwow4YN06709oh1RERERLbBZgIiDJqIeqHixYtLqVKl9FKiRAldNmXKFLFHxgwRu94TERFZl83UEGGKDsw/hjqiI0eO6DLUE2VkctacyjTjPQMiIiIiq7KZgMhYV/O///1PL0YIjtq3b6+Tv9pthojTdxAREVmVzTSZpQdzj508eVLsuYYojDVEREREVmXzAZE9Yw0RERGRbWBAZEXsZUZERGQbGBBZkb9XUlH1jUjWEBERETl0UXVAQIAWU6cnPj5e7D5DxCYzIiIixw6IPvvsM3FUppGqo2LFYDDcMzAkIiIiOw6IevbsKY7KmCGKiU+UO3EJ4uVm9Y+DiIjIIbGGyIq83JzFzTnpI2BPMyIiIuthQGTtCV5No1WzsJqIiMhaGBDZUB0RERERWQcDIhupI2KTGRERkfUwILKyAA7OSEREZHU2060pISFB5syZI2vXrpWQkBBJTExM9vi6devEHgUam8yYISIiIrIamwmIhgwZogFR27ZtpXLlyg4zJo8xQ3SdAREREZHV2ExANH/+fFm4cKE8/vjj4kiC2GRGRERkdTZTQ+Tm5ialS5cWR2PMELGomoiIyHpsJiAaMWKEfP755zqFhSMx1hAxICIiIrIem2ky27x5s6xfv15WrlwplSpVElfXpAELjRYvXiz23e2eAzMSERGJowdE/v7+0rFjR3E0phnvOcErERGR1dhMQDR79mxxRMapOxISDXL7Trzk9kqeGSMiIiIHCoiMrl27JkePHtXb5cqVk7x584o9c3dxFh93F4mIiZcbUbEMiIiIiBy5qDoyMlL69OkjBQsWlMaNG+ulUKFC0rdvX4mKihJHyBKxsJqIiMjBA6Lhw4fLxo0bZfny5RIWFqaXpUuX6jL0QLNn7GlGRERkXTbTZPbLL7/Izz//LE2bNjUtwyCNnp6e8vTTT8uMGTPE7gurGRARERE5doYIzWL58+dPtTxfvnwO0GR2N0PE0aqJiIgcOyCqX7++vPPOOxIdHW1adufOHXnvvff0MXvGJjMiIiLrspkmM4xS3apVKwkODpZq1arpsn379omHh4esXr1a7FmgDwMiIiIia7KZgAgz3B8/flzmzp0rR44c0WXdunWT7t27ax2RI2SIWENERETk4AEReHl5yYsvvvjQ20EBNi5nzpzR+5gK5O2335Y2bdqILdcQXWdARERE5HgB0bJlyzRIwbxluH0v7du3z/B20ew2YcIEKVOmjE6H8d1338mTTz4pe/bs0eDI1gSZTd9BRERE2S+XwYrTyzs5OcmVK1e0JxlupwfzeyUkJDzUawUGBsqkSZN0oMf7uX37tuTOnVtu3bolfn5+ktVOXouQ5p9sFF8PFznwbqssfz0iIiJ7dPshzt9WzRAlJiameduSEEgtWrRIR8JOr7daTEyMXswPqDVqiMKj4yU2PlHcXGym8x8REZFDsJkz7/fff58sKDGKjY3VxzLrwIED4uPjI+7u7vLyyy/LkiVLpGLFimmuO378eI0ojZciRYpIdsrt6SpOdye5D2OzGRERkWM1mZlzdnaWy5cva/OZuevXr+uyzDaZIZA6d+6cps0wAvY333yj04CkFRSllSFCUJRdTWZQ64M1WlS9auijUr5A9rwmERGRPbmdU5vMzCEuQ61QShcuXNA3l1lubm5SunRpvV2rVi3ZsWOHjnX01VdfpVoXWSRcrN3TDAHRjQhmiIiIiLKb1QOiGjVqaCCES/PmzcXF5b9dQlbo9OnT0rp164d+HdQopdUkZ2vzmXH6DiIiIgcMiDp06KDXe/fu1ZGqUfdjnuUpXry4dO7cOVPbHDVqlHbnL1q0qISHh8u8efNkw4YNNj3iNQdnJCIicuCACPOXAQKfZ555RqfqeFghISHy/PPPa00SmtuqVq2qwdD//vc/sVUcnJGIiMh6rB4QGfXs2dNi2/r2228lpzENzsiAiIiIyHEDItQLffrpp7Jw4ULtHYZeYuZu3Lgh9syYIboRFWftXSEiInI4NjMO0XvvvSeTJ0/WZjN0lxs+fLh06tRJR7B+9913xd4Fervq9Y1I2y38JiIislc2ExBhlvuvv/5aRowYoT3NMNM9xg7CpKxbt24VexfondTt/0YkM0REREQOGxBhTrMqVarobfQ0Q5YInnjiCfntt9/E3rGXGRERkfXYTECEGerRKwxKlSolf/zxh97GgIrWHjQxOwSYmsxidZBKIiIicsCAqGPHjrJ27Vq9PWjQIBkzZoyUKVNGu8/36dNH7F3Q3Saz2IREiYzN3DQlREREZCe9zCZMmGC6jcJqDKq4ZcsWDYratWsn9s7TzVk8XJ0kOi5Rm8183G3moyEiIrJ7NnvWrV+/vl4cCeqILt2K1sEZiwR6WXt3iIiIHIZVA6Jly5ZleN327duLvQv0SQqIWFhNRETkQAGRcR4zI0zwmrKgGMuMAzfau4C7Pc1QWE1EREQOUlSNGeiNF/Qqq169uqxcuVLCwsL0gts1a9aUVatWiSMwzXjPgIiIiMgxa4iGDh0qX375pTRq1Mi0rFWrVuLl5SX9+vWTw4cPi8MERFEMiIiIiByy2/3JkyfF398/1XLMVn/mzBlxBByckYiIyMEDojp16uj8ZVevXjUtw+3XXntN6tatK47AOMErepkRERGRAwZEs2bN0pGqMf5Q6dKl9YLbFy9elG+//VYcQdDdgIgZIiIiIgetIUIAtH//flmzZo0cOXJEl1WoUEFatGhh6mnmKBki1hARERE5aEAECHxatmypF0dkLKpmhoiIiMiBAqIpU6ZoDzIPDw+9fS+DBw8WRwmIwu7ESUKiQZydHCMzRkREZG25DFacWr1EiRKyc+dOCQoK0tv3yhydOnUq2/br9u3b2rvt1q1b4ufnl22vG5+QKKVHr9Tbu95qIUE+SRO+EhERUdaev62aITp9+nSatx2Vi7OT5PZ0lVt34uRmVCwDIiIiIkfrZUbJe5pdj2AdERERUXaxaoYI4w5l1OTJk8VhepqFRmqGiIiIiBwgINqzZ0+G1nOUbvfJJ3iNs/auEBEROQyrBkTr16+35svbdJPZjcgYa+8KERGRw2ANka0OzsgMERERkWMOzIgu+AsXLpRz585JbGzyGprFixeLIwj0dtVr1hARERE5YIZo/vz50qBBAzl8+LAsWbJE4uLi5N9//5V169bpmAKOItA7qas9J3glIiJywIBo3Lhx8umnn8ry5cvFzc1NPv/8c53T7Omnn9ZJXh2FKUPEgIiIiMjxAqKTJ09K27Zt9TYCosjISO1dNmzYMJk5c6Y4Xi8zBkREREQOFxAFBARIeHi43i5cuLAcPHhQb4eFhUlUVJQ4iqC7TWYMiIiIiBwwIGrcuLGsWbNGb3fp0kWGDBkiL774onTr1k2aN28ujiLIJylDdCcuQS7cdJxAkIiIyKEDImMmaNq0adK1a1e9PXr0aB3F+urVq9K5c2f59ttvxVF4u7tIw9JBevun7eesvTtEREQOwaqz3YOTk5PUqVNHXnjhBQ2IfH19xdqsNdu90coDl+WVubslj4+b/DOyubi5WD1uJSIisnkPc/62+pl248aNUqlSJRkxYoQULFhQevbsKX/99Zc4shYV80s+X3cJjYiVVf9esfbuEBER2T2rB0SPPvqozJo1Sy5fvixTp06VM2fOSJMmTaRs2bIyceJEuXLF8QICV2cn6VY3aaiBH7eetfbuEBER2T2rB0RG3t7e0rt3b80YHTt2TAurp0+frmMQtW/fPlPbGj9+vDbDofktX7580qFDBzl69KjkJAiInJ1yyfbTN+TY1aTed0RERGTnAZG50qVLy5tvvilvvfWWBjW//fZbpp6PoGrAgAGydetW7bmGUa9btmypYxvlFAVye0iLCvn0NrNEREREDjSXGWzatEmb0H755RctuMZI1X379s3UNlatWpXs/pw5czRTtGvXLu3en1P0eKS4rP73qizefVHeaF1ee6ARERGR5dnEGfbSpUsatOBy4sQJndNsypQpGgyhKe1hodocAgMD03w8JiZGL+ZV6ragQakgKZHHW06HRsqvey9K93rFrL1LREREdsnqTWZt2rSRYsWKaUF1x44ddXLXzZs3az2RJYKhxMREGTp0qDRs2FAqV66cbs0RuukZL0WKFBFb4OSUS7rXMxZXnxMrj5BARERkt6weELm6usrPP/8sFy5c0F5l5cqVs+j2UUuEwR/nz5+f7jqjRo3SLJLxcv78ebEVT9UKFncXJzl8+bbsPhdm7d0hIiKyS1YfmDErDRw4UJYuXap1SSVKlMgxAzOm9OqiffLzrgtSJp+PTHu2ppQrYP3BK4mIiGxNjh6YMSsgxkMwtGTJElm3bl2mgiFb1L9pKQnydpPjIRHSbtpmmbX5tCQmJo9jo+MSZP3REM0kERERUebYZYaof//+Mm/ePM0OmTfBIWr09PTMcRkiuBYeI6//vE/WH72m9x8tk0fefqKiHLh4S/7496psPHZNJ4R1dc4lU7rWkDZVClp7l4mIiLLVw5y/7TIgypUrV5rLZ8+eLb169cqRARHgo8KYRB/+dlhi4hNTPe7r7iLhMfHilEtkQueq8nTtjBWHn7oWIX6erpLHxz0L9pqIiCh7PMz52ya63VuaHcZ4pkCvR/3iUr9UkAxdsFcOXrwt5Qv4SsuK+aVlpQJ6e/SSg7Jg53l5/ef9Eh4dL30blbjncfpq0ymZsPKI5PdzlzXDm4ifh2u2viciIiJbYJcZoodlqxkic/jYImMTxCfFYI1YPu73w/L1X6f1/pDmZWRoizKpsmbxCYny9rJ/Zd62c6ZlfRqWkLfbVcymd0BERGRZLKp2QAhwUgZDxuVvPl5BXm1ZVu9/vva4PPXlFpm//Zzcjo7TZREx8dL3u50aDCFOeuZu09p3W87IkSssyiYiIsfDDFEOzRBlxHf/nJH3VxyShLs90jCeUatKBbS3Gnqjebg6aQE2mtte+XGXrDx4ReqVCJT5/R5Jtw6LiIjIVrGo2sLsJSCCK7eiZcmei/LL7gtyIiTCtBwF1N/2rC3Vivjr/Qs3o6TF5I0SHZcoU7rVkPbVCllxr4mIiDKPAZGF2VNAZISPef+FWxoYXb0dLW+1rShFAr2SrTN17XH5ZM0xLbBeN6IpJ5MlIqIchb3M6L7QBIZskDEjlJYXG5eUn3dfkLPXo2TKuuMyqk2FbN1HIiIia2FRNZl4uDrLO3d7mWE07F1nb1h7l4iIiLIFAyJK5rHy+aV5+XwSl2CQzjO2yNNfbpEV+y9JXELqgSCJiIjsBWuIHKSGKDNCI2LkveWHZOWByxJ/t4daAT8P6VI7WGoXD5RqwbnF38vN2rtJRESUDIuqLczRAyIjFF/P3XZOxytCkGSuWJCXVA32l+AATx0PycvNWYuwvd1ctDu/u4uzuLng2kl8PVykkL+nNskZ4Wt38lqErDsSImsPh2gPOIzA3blmsM7T5uLM5CUREWUOAyILY0CUXEx8gqw8cEXWHw2RfefD5Mz1qAfaTj5fdw2gCuT20GlHzt1IezsYEgDd/jvVLCyVCvlxTCQiIsoQBkQWxoDo3m5Fxcn+i2HajR+Zo8iYeJ1GBNdRMQkaQGHyWVxi4xPlRmSs3IlLSLUdN2cneaRUkNYslcnvI3/8e1WW77sk1yNjTeuUyecjHWoUlierF5LggOTDBKQ3nYmrcy7NUN1rvZtRcXf3O14iYxIkKjZemwVL5/NhAEZElEMxILIwBkSWZQxAMPjjhZt35FLYHQ1u0DSWcqwjFG9vOnZNFu++KGsOX9WAyqhO8QAdOwkjb8cnGHTdmLsB1/WIGA2kcB8BUeXCuaV2sQCpVSxQqgbnlothd2TX2Zt62X32ZrKgy1zJPN7SqnIBaV2pgD4PwRFeD9u/ejtGA7sigZ6S39dDnJwYOBER2RIGRBbGgMg2YO61VQevyK97LsqWU9fF0t9UT1fUPTmLl5uL3j4dGimxZr3p0MTnlCuXXIuIMU1/YoQ6qWKB3lpLhVqpW3fiTJfw6HhdH/+0jM9CjVWZfL5SroCvlMufdB3g5SaJBsPdC8aKEl3m7+ma4WALr4EegdgHIiJHd5sBkWUxILI9l2/dkT8PXZWo2ARxdsolrs5O4uKcS5vdAr3dJMjHXYK83fQ2mvGQCdqJjNCZm3IsJFzrkmoVRcYoQGoWC9DaJPMibwiPjpMNR69pEIZ6KbyWEeKTvL7u2hSHbFPKAMmSXJxy6fvAPgf5uGmQFODlqj37/L1c5WZkrJy+HiVnr0fKmdBIuR0dLzWL+kvrygV0rrpiQd5Zsl+JiQY5cPGWFsAXDvCUEnm8NWhkEyMR2QoGRBbGgMi+oKYJgVNmTtzRcQmy93yYZnby+3locIJADNBUd/HmHTlzPVJH9UZwlNsTAYurXvt5uprWxf/xumFRsXLsargcvRIhR6/e1mvULSEDhd3CNQKO8Jj4h36/FQr6aYCUeDd7FJ+QqNd4vYiYeM1goXYK89ahhyB6AerF3VUCfdyksL+nFPL3kIK5PSWPj5vsO39LNh67JptPhGrzpDkcn+JB3prxql08QOoUD5TSeX1MGS78ebl8K1qDqJDwGM3Eebk7a29EPBeBH4KqnNKrEJ/RiWsRcu56lFQtklvy+XpYe5eIyAwDIgtjQETWYixCR5YLTXU3ImLlZlSshEXFma79PF00CCmex1uv0ey3/ug1WX3wijYtZmX2ytfdRSoU8tNJg1ETltZLIShE/RWaD0+GRGih+70geMzv665DM6AHIgIlZACTLrk0iEIGEEEpArQ8vu5yJzbBVJN2/kaUhEbGalNkvRKBOj1NyuyfeXB8MzJOrkfGmF3Hyg0cX72OlYQEgwaGxowjAt3ToVGy59xN2XsuLFnQiuCzcdk80qRMXqlaxF+8XJ0z3NyJ4Co0Ep0SEsQ5Vy5xcko6Frhg//G+cQwyAkEvgl0EvviTjs8FATGyqEHe/wXzlHXw7w4/pPBPwtvNOUM/wPCckPBoOX/jjj4XNZLoiZvyc8dni4wwhkJBljppiJOka/yYwQ8bZmqTMCCyMAZElFPhpL72SIicux55t1kxKahAM5wX/njiD6m7i/h4uOgYUTgZo6kQ9VrIHIWGx8jFsGhtokTxOwrJS+XzliZl80qTsvmkRlF/0x9rBG/nb0bJqWuRcuBCmDZR7jkXlqpHIV4btVYIeFD0jkwVeiMiS3U9ItY0+KeloJ6qehF/KZXX+27BfVKAiWtLZOAQqCCDdvJaZJqPo75MM2FuONbOerLy8XDVY4/A5MrtpOOLoBIBzL3gszMGR8ZxvdzujvEVF59oqlvDCTM9xuZe9KLM5+eh9/FZJ13iknpmOuUSd1dn3T6usQ6CTmwXTcfokYnHAjRAdJNAL1fx9XDV7wA+R6yDS2x86uAX3xfdrouzuLs63c3WGh9NuoHgDdtCwJp0najHyvie8Vx8j+ISsV7SOpr9vFurZ6zDw219PVdn8bj7Xtycc91dN1Hi4g0Sl5io9YgafJoFovgvVYATn6DHAcEKMqrYhjlsB/uKx82/xzh+yBT7eSRljfEeEChjOV4Lm8F3AM3vKb8DeBzfL9Qo4tgiG5leJxDz7wma1hHA4xrHGVnnpEtSfaL2/I1L1PeEaxwzPI6gGdfOd/cv5ftDXSWOt14Sko4dPhd8HsYfLUnvLekI4tp0KM3eGo6zfnfvfh+wDZQv9HikmFgSAyILY0BE9GDQnHjo0m05eOmWZlgwjAFqmtLLdOCkg2AFwdelu4EY/nBjO8aehDgphIYnBTVJl1gNOtBT0fiLGjVWGAZi2+kbci08+SCiKeEPf9LJw1WvUaeFE0kgarW83fRxYyCl15Ex2nyIZsgaRQOkfAFfDTSxL3+fCNXmxL+Oh973ddOCc4e3m4sehwSc1O9eP8xfZZzUjCdD48mfbBcCi6SBa510bDYEXmnBdxzr6b+Ju0OF4Nq8I0hO82T1QvJ51xoW3SYDIgtjQESUM+HPGQYO3XbqumZiEJQZm9uSCtXd9Fe7pYdMwOsiM3bnbqYEGQNjZgVZqYjopPot/MrO55fUPIgL6qdSBovYFk5y0bGJEhWXlH3Bds1/qRuHl0D2wXhBRiLltowBJ5pakO3DMUGEhOwOml5x7e3mopkPUwYhLkGzB95mI9AjQ6XNjXebFtF0i6wifukbM2Gebi4psj9JGQbddtzdcckSkrITKSF4+y8DlnTBvhvHMsMF2zE2peo6zk6mJsakpETStQ7Hcff18F5w35gp1ayprp80nAbep/E6LXjfnm5JGTpk6pBNSQnZK+Po/LjGfiDzhszd7bvX2H/ELcZepYDaRB2o1s/DVEOHzx6BNb7DCI7wusiuFg3y0u9tWvAeEbjjgmZ1XCPrhODamDnDNY6th1kWENkxYxCO7FaCZriS9s38cBg/DxxvVxe8P3ymST9WjDWKxtcwb641ZtyM3wc8x5RpQpYvIVFK5vGRFhXziyUxILIwBkRERESOdf7OGV07iIiIiLIQAyIiIiJyeAyIiIiIyOExICIiIiKHl3xmTVLGOnMUZxEREVHOYDxvP0h/MQZEaQgPD9frIkWKWHtXiIiI6AHO4+htlhnsdp+GxMREuXTpkvj6+lp8OHRErwi0zp8/zy792YTHPPvxmGc/HvPsx2Nue8ccIQ2CoUKFCokThsfOBGaI0oCDGBwcnKWvgQ+S/4CyF4959uMxz3485tmPx9y2jnlmM0NGLKomIiIih8eAiIiIiBweA6Js5u7uLu+8845eU/bgMc9+PObZj8c8+/GY29cxZ1E1EREROTxmiIiIiMjhMSAiIiIih8eAiIiIiBweAyIiIiJyeAyIstH06dOlePHi4uHhIfXq1ZPt27dbe5fsxvjx46VOnTo6uni+fPmkQ4cOcvTo0WTrREdHy4ABAyQoKEh8fHykc+fOcvXqVavts72ZMGGCjuw+dOhQ0zIec8u7ePGiPPfcc3pMPT09pUqVKrJz507T4+gn8/bbb0vBggX18RYtWsjx48etus85WUJCgowZM0ZKlCihx7NUqVLywQcfJJsri8f84WzatEnatWuno0vjb8ivv/6a7PGMHN8bN25I9+7ddbBGf39/6du3r0RERGRqPxgQZZMFCxbI8OHDtbvg7t27pVq1atKqVSsJCQmx9q7ZhY0bN+qJd+vWrbJmzRqJi4uTli1bSmRkpGmdYcOGyfLly2XRokW6PqZn6dSpk1X3217s2LFDvvrqK6latWqy5TzmlnXz5k1p2LChuLq6ysqVK+XQoUPyySefSEBAgGmdjz76SKZMmSJffvmlbNu2Tby9vfVvDYJTyryJEyfKjBkzZNq0aXL48GG9j2M8depU0zo85g8Hf6dxTkTSIC0ZOb4Ihv7991/9+79ixQoNsvr165e5HUG3e8p6devWNQwYMMB0PyEhwVCoUCHD+PHjrbpf9iokJAQ/3wwbN27U+2FhYQZXV1fDokWLTOscPnxY19myZYsV9zTnCw8PN5QpU8awZs0aQ5MmTQxDhgzR5TzmlvfGG28YGjVqlO7jiYmJhgIFChgmTZpkWobPwd3d3fDTTz9l017al7Zt2xr69OmTbFmnTp0M3bt319s85paFvw9Lliwx3c/I8T106JA+b8eOHaZ1Vq5caciVK5fh4sWLGX5tZoiyQWxsrOzatUvTfObzpeH+li1brLpv9urWrVt6HRgYqNc4/sgamX8G5cuXl6JFi/IzeEjIzLVt2zbZsQUec8tbtmyZ1K5dW7p06aJNwzVq1JCvv/7a9Pjp06flypUryY455nVCEz2P+YNp0KCBrF27Vo4dO6b39+3bJ5s3b5Y2bdrofR7zrJWR44trNJPh34YR1sd5FhmljOLkrtkgNDRU26Hz58+fbDnuHzlyxGr7Za8SExO1jgVNC5UrV9Zl+Afl5uam/2hSfgZ4jB7M/PnztQkYTWYp8Zhb3qlTp7T5Bs3vb775ph73wYMH63Hu2bOn6bim9beGx/zBjBw5UmdYRzDv7Oysf8vHjh2rTTTAY561MnJ8cY0fCOZcXFz0B3FmPgMGRGSXGYuDBw/qrzjKOufPn5chQ4Zomz06ClD2BPv4FTxu3Di9jwwRvuuorUBARJa3cOFCmTt3rsybN08qVaoke/fu1R9cKADmMbcvbDLLBnny5NFfFil71+B+gQIFrLZf9mjgwIFaULd+/XoJDg42LcdxRtNlWFhYsvX5GTw4NImhU0DNmjX11xguKJxG8SNu4xccj7lloZdNxYoVky2rUKGCnDt3Tm8bjyv/1ljOa6+9plmirl27ao++Hj16aGcB9GwFHvOslZHji+uUHZTi4+O151lmPgMGRNkA6exatWppO7T5Lz3cr1+/vlX3zV6gFg/B0JIlS2TdunXaRdYcjj965ph/BuiWjxMJP4MH07x5czlw4ID+YjZekL1AU4LxNo+5ZaEZOOVwEqhtKVasmN7G9x4nAPNjjuYe1FHwmD+YqKgorUUxhx+4+BsOPOZZKyPHF9f44YUfaUY4D+AzQq1RhlmsNJzuaf78+VoVP2fOHK2I79evn8Hf399w5coVa++aXXjllVcMuXPnNmzYsMFw+fJl0yUqKsq0zssvv2woWrSoYd26dYadO3ca6tevrxeyHPNeZsBjblnbt283uLi4GMaOHWs4fvy4Ye7cuQYvLy/Djz/+aFpnwoQJ+rdl6dKlhv379xuefPJJQ4kSJQx37tyx6r7nVD179jQULlzYsGLFCsPp06cNixcvNuTJk8fw+uuvm9bhMX/4nqp79uzRC8KSyZMn6+2zZ89m+Pi2bt3aUKNGDcO2bdsMmzdv1p6v3bp1y9R+MCDKRlOnTtWTg5ubm3bD37p1q7V3yW7gH1Fal9mzZ5vWwT+e/v37GwICAvQk0rFjRw2aKOsCIh5zy1u+fLmhcuXK+gOrfPnyhpkzZyZ7HN2Ux4wZY8ifP7+u07x5c8PRo0ettr853e3bt/U7jb/dHh4ehpIlSxpGjx5tiImJMa3DY/5w1q9fn+bfbwSjGT2+169f1wDIx8fH4OfnZ+jdu7cGWpmRC/+zbIKLiIiIKGdhDRERERE5PAZERERE5PAYEBEREZHDY0BEREREDo8BERERETk8BkRERETk8BgQERERkcNjQEREREQOjwEREREROTwGREREROTwGBARERGRw3Ox9g7YosTERLl06ZL4+vpKrly5rL07RERElAGYnjU8PFwKFSokTk6Zy/kwIEoDgqEiRYpYezeIiIjoAZw/f16Cg4Mz9RwGRGlAZsh4QP38/Ky9O0RERJQBt2/f1oSG8TyeGQyI0mBsJkMwxICIiIgoZ3mQchcWVRMREZHDY0BEREREDo8BERERETk8BkRERBYWHh0nE1Yekb9PhFp7V4gog1hUTUQOKSHRIKdDI6RUXh+Ljzf23vJD8vOuCzJr82n5qd8jUqtYQJrrJSYa5EZUrNyJTZDI2HiJik1Iuh0TL3fiEvQ+Lj7uzvJk9cLi4eps0f3cez5MRi0+IAOblZa2VQtadNuOBp9l2J04cXdxEm93y59ab0TGaoAdHOApFQr6Wfy7QAyIiMgOvfHzfvnnVKj8/HIDye/nkeY609adkE//PCaPVykgnz5TXdxdLHOC2XA0RIMhiE1IlJd+2ClLBzaSwv6eydY7ERIu/X7YJaeuRWZou99vOSszuteSokFeaZ6Md527KRdv3pHQiBi5Fh4j1yJi9KT55uMVxCeNEzQGsHt32b9y+PJtGbZgrxTI7S61igWmWi8uIVF+2HJWSub1lqbl8qW5b3j9j1YflZUHL2ugifuJBhGDGKR1pQIyum1FcXPJWIPE+RtR8vEfR6V8AT/p3bB4mid+ZOBmbT4j+y6Eya07cXIbl+g4iY5LlK51i8gbrcqLk1PWDqp79Eq4TFl7XM7diNLjjeMen2jQgGh8pyrSqWbGx8D5ZdcFfS9PVC0kdYoHJAvQESB/u/mUfLnxlETExOsyV+dcGhRVDc4tTcvmkxYV82f4tfD5/LL7gsTEJUj3esUyfJzOXo+UIfP3SnxionzcpZp+PvYmlwH/KijVOAa5c+eWW7dusds9UQ5z+dYdaTBhneAvW/+mpeT11uVTrRMdl6Dr4Fc31C8ZJDOfryW+Hq4P9do4Ubf6dJNcuhUt3eoWkb3nb2nAgZPXzy/XN2UO8Ev/5R93SXh00gnO09VZvN2dxdPNWbxcXcTL3Vm83JzFE7fdnGXziVDdVz8PF5n8dHXTCRCBxx+Hrsina47L0avhae7TS41LyqjHK6RavvHYNek5a7vpfh4ft1SBG47TgLm7Ze2REME5ekKnKvJMnaLJtoNTyFu/HpS5286le1walAqSGc/Vktye9z6+fx2/JoN+2iNhUXF6H/sysk15eaJqQQ0SEJz9tP2cfP7ncbl+97NLy7P1isqHT1bOsqBo1cErMnzhXs3epeeN1uXl5SYl75l9xLGbtPqofLHhpGlZyTze0qV2EelQo5BsPHpNJq85JiHhMfpYiTzeGgAav7fmnzGO0/0ynXvO3ZQxSw/KwYu39X6XWsEyoXNVcb7Pcdp8PFQGzNutrw0I+t5tX0m61ilic7M5PMz5mwFRGhgQEeVcMzedlHG/H9Hbgd5u8s/Ix1JlGRbvviDDF+6TIG83PelHxiZIpUJ+Mqd3Xcnr6/7Ar/3Wrwfkx63npEigp6we2lhPXB2m/y2hEbHSqlJ+zfAs2nVeRi85qNmE2sUC5KsetSTI596veSnsjp6Q9pwL0/uvNC0lNYr4y2d/HpdDl5NObr7uLlIlOLfk8XHX94C/7LP+Pq3ZhD+HN5FiQd6m7eHPfucZ/8juc2HSrW5RbTpLGbgh4/LCdztl++kbGgwZzxTjOlbRgMO4nXG/H5av/zqt67z/ZGWpUji34PzqlCuXnLkeqdk6HN+y+X1kdu+6qTJlxu18temUfLTqiGaWsB9hUbFy+Va0Po7j1LFmYfn2r9NyKjTSFDj0alhc8vq4a6Dl5+kq+y/cktG/HtB9xcka+5rZoOjCzSgNUnA8WlUqIM89UsyUZUQAOmXdcT3uxkCvb6MSerxxwfftkz+OycxNp/TxXg2Ky5gnKqYZcCC4Q3OlMZvYtFxePdZpBVn4Pr3aspy0q1pIj/OFm3c0o/T3iesaIBqDG2SmXJxTZ+JuRsbKR6uPyPwd5/XY+Hq4aLMsjjWaSj99unqaGTx8LrP/PiNjfz+smaVqRfzF39NVg2loV62QjOtY+b4/JPBecUEW9n7B18NiQGRhDIiIcq62U/6Sfy8lBQmA9P5TtZI3X3T84m8NLl5tWVabgXrN3q5BS7EgL/mhT700m6XuZ8vJ69Lt6616e94L9aRB6Tx6e9fZG9Jt5jZtPqtR1N8U1DxZvZBM7Fw1w7UgsfGJMn7lYT1BmUNzWJ9GJfTEbJ6BwZ/252dtl7+Oh2qz1Zc9aiX7xf/ct9v0l/5frzfTfTMGblj3/Q6VpPfsHXocEWh907O2rPr3ium1P+xQWQOFT9cck8/XJgUHH3WuKk/XST3l0b+Xbum2kOXI5+sus3rVkcqFc5v2MTwmXkb9ckB+O3DZdGL/oENlPXEjsPhy40mtpzJCEDu0RRnpWreouKZx8v91z0XN3uBk/3TtYJnQqaoGRQgq//j3ima7cFLG+0TAE+Dtps9DgIDXwmvGxCeatoeAsm2VgvJsvWJaE4bjAGjOG/14hTQDkG/+OiUf/nZYb6NJFlk98885KjZeM2/rj17TfRnfsYoeO+zDb/svy4Kd52XX2ZsS4OUqgx4rI90fKZpuk+7Cnedl5C/79f22qJBfpj1bw/Rax6+Gy/J9l+SHrWfl5t2sW+eawTLq8fKy88wNzcbFJRikefl8Mr17zWT7iGzn+8sPyaK7AVunGoVlXKcq4ubsJF//dUqbSBEkFQ/y0qDK2clJXJxy6ftB8IOgDc2fuEbWFvsHWAffOwRgrSsX1CDOkhgQWRgDIqKcCXU5LSZv0j+6OGEhc4GMxbKBDU2p/YMXb8kTUzfrie6fkc31l/2Z0EjpMWubnL9xRzxcnfQ5OGlXuXspne/ehdeo82j9+SY5ez1KMy4p/8gjC/Dqon2m+4Obl5FhLco8UHMDTphv6AnQoBmIFx8taTqpp1Xn0ubzTXoymt/vEXmkZJAGIU9/tUV2nLmpz0fTR8rADUEQAhU0oyFrhmOB5+Ek/+3m07r+/yrmlzWHrurtd9tVlF4NS6S7zxfD7kjv2dvl2NUIPRn6e7maCsZxUgV8Zu+0ryTP1Sua7LhcuRWt2Q00H+HYvtSk5H0zEkv3XtS6KGy6cdm8cisqVvZduJVqPZy8G5bOoxmoudvOytXbSU1Tj5QMlA7VC2utDY6TOQQEH3asLE/Xvvd8l8v2XZIRC/dqwIFAtZC/pxTwc5cCuT20yerAxVv6XZv+bE1pXiF1DRDqkpDJyUjAjEBv4E97NGiuWyJQmpTNq4HQkSv/NaOWy++rgSYeN693e+mHXRoANigVpHVPu8/dlN1nb8qxq+F6/JDQQR0aAm7zzwUB2+Cf9uhn+6A61iis9XuWxIDIwhgQEeVMk/84KlPWndBfvB89VVXqT1inJ4nF/RtIzaJJPb1e/3mfLNx5QdpXKyRTutUwPTfkdrT0/W6nnqhSQjbn867/rZvShysOyTebT0vB3B6yelhj8UvjhD1t3XFtTnu9dblMFdymBbUcOFFlpObJ2IyHJsFlAxvJtlPX5dlvtukvdGSHzIvOzQM3NG39+EI9rVsxwuli/MojpiYhwPvp37T0ffcDTXCv/LhLm3lSwmt91rW61Cmeuqj7QSEgGLpgryngwrm8VtEAzQrFJSZqYGmeSYSigV568kfzpvHkv/9CmGbGVuy/JP5ebvLlc7XS7TWY0j8nQuWVuf/V3phDUPhtzzoZ3tb94HNFEycCWSME/QiO0LT1eJWCaWbUtp66Ln3n7NBmzZSQ/UEzKILKtKBZEwX3qOfCcUYzcEJiojaX4jMtEuilzX1FAry0GRb/FhF8xcQn6DXq44IDMp+NvRcGRBbGgIjIslCXgaYinIwy2tsos/CnrMmkDdrr5/Ou1bWb+oiF+/RXvjGgwR/weuPW6h9j1MrUTnECRo3IiWsRcuDCLQ2MkE3aefamnkz/fuMx/ZWfUkh4tDQYv05PBrN71ZFm5dPuiWUt1yNipOnHG7SAG0Eigh7UqvSsX0zee7JyqvW/++eMPo7aF2Qz0jrOxjqZ/s1KydAWZTO8Lzi+xponnAy93JIKyH3cXLKkAHrt4auyZM9FaVAqj7SomE/y+SZ/P6dDI+X3A5e1+aheySDNKqbXNIWgBgEG9jkzkD1ELdWV29Fy9Va0XqM33DN1iiQLNi0BzZOvLdovQT5uWm+Ef2+5ve4fNKOG7LVF+7QOq2ZRfw3S8AMiXzo9NG0ZAyILY0BEZDn4hY3eTKhhQLHsJ12qScVClv93hR40Hb/4R0+0O99qoScuBDbtpiU1j/098jFZtveSNvtgP34f3ChDTVbPfLVFtp2+ISP+V1YGNS+T6vHp609oES5OJIv7NxRb9PWmU1oY6+3mrJkANPtsfL2pFMydOsDLKPzaz6rglsga529+m4koy2jzzNfbTAWd6LnTftpmHb8FhZeWtHTvJb1uWTG/6Vc8el0hUEEdB5qNUFwKz9cvluH6HWOtyMJd5zXDYQ73jb18UN9iq55vUEwLxo3NIshOPEwwBAyGyN7wG01EWWL9kRDt5YTB5FCkuvG1plqbgaYljK3S6Yt/tHDzXtDrBs0AuL6X+IRErfEANJWZ69mguF5/ueGkFj2jUBVNaBmF2gsUGaPgeuvp5PUvm45f01402CYG1bNVaAZCbQwgW4Zu+0SUHEeqJiKLQ3AyVEe1Td6lFwWp6H3z9tJ/tUan8xf/yPyXHpFKhZK6YaccO+WZmVu0ZxKgSBNj2ZTN76td5euXCjKt+8/J69plHN2UG5VJ6u5u1KZyQfnQ97D22gF0wc9MHQgGS2xXvZDM23ZOFu44r/UoRlhm7MqM9WwZMmcYWDGfn3uatVBEjo4ZIiKymJPXIrSQGd1xEQyhdwvGvzF2HUYzFTI4a4Y11ikK0COm56wdcu56VLLtICPUe84ODYbQHRvQvRfjtmAAP4z3g27cGGfFvLkM46Gk7EmDpp3udwcShB6PFMv0+zI2m608eMXUY+jq7Wgd0waMAxXaMhx7jN3zWPmMT/NA5EiYISKih4baIBQXY3A9YzeN5x4pKu+1r5zmyLTovfJtrzryzFdb9bkYAwjzjmFMIBTrYloL9HxB12T0BsPoywiO0MSG5Rh8D8HRpuOhOkXG6ruD5WHsmLRgEMGVB65I7eIBUjKvT6bfX7Xg3DqOC6bHQIYLQdWCHee1qzHGsEHWiohyNvYySwN7mRFlDKa9wCCBxgyNccA+zJ6OYf7vB2P/dP7yH63PqVzYT+a+8IiMXnJAVuy/rL3F5r5QT2rcHT/I3KlrETJh5RH54+7AgMYmNYyrk1XzV2FAwg9WHNKBGn8d0FAenbhO5yyb/HS1hx5XiIgsg73MiCjZmD+HUgw4lxWQyRk4b7cGQ+iwhQk4Vw55VL5+vnaGgiFjpuj7PvV0OgaM3vvYxxs0GELhL+b4SisYAmR5Zj5fW0dfRiAFPepnfObuBx1VF/uF2idM8YBgCCMQo+iaiHI+qwdE06dPl+LFi4uHh4fUq1dPtm//b/bltHz22WdSrlw58fT0lCJFisiwYcMkOjppAkBISEiQMWPGSIkSJXSdUqVKyQcffKCDiRHZO3zPu87cKm2n/qXTF9xrPdT7oFs8Zu5esOOcnuTxHPTYuh+sM3TBHvnzcIhOxTC3bz2Z9mxNHd8nszA4HaaHwBg5GPEWwRWG83+0TNqj45rDVBTLBjTSzBBm/M5KmLgT2S/4+I+jpmLqjM5FRkS2zao1RAsWLJDhw4fLl19+qcEQgp1WrVrJ0aNHJV++1KO9zps3T0aOHCmzZs2SBg0ayLFjx6RXr15aLDh58mRdZ+LEiTJjxgz57rvvpFKlSrJz507p3bu3ptAGDx5shXdJ9GAwvxZGuEWvpoyO+YK5mNANHFDcjAwGemSZQ1HwkPl7ZMPRpBmrU8Jkna+3Kqej3KY1Vg/qZl77eb/8fuCKDvCHTI5xItMHhfGCvu5ZWyauOirPP1IsU13YkRXCFAHZAcXVeN/G31fP1rv3fFZElHNYtYYIQVCdOnVk2rRpej8xMVGzPoMGDdLAJ6WBAwfK4cOHZe3ataZlI0aMkG3btsnmzZv1/hNPPCH58+eXb7/91rRO586dNVv0448/Zmi/WENE1obZyPv9sFMnv0RmAsXCXWoH3zcDg7F/0DvLCJNHog6nVrFA01QFfb/bIaeuRWowExzgqYXLmKMJY+lsOnbNNIhi9SL+8kbr8sm6t2MgwjeXHJD5O85r768vuteUlpUKiKNAMNho4jq5fCtaJ8lc+FJ9a+8SEVno/G21DFFsbKzs2rVLRo0aZVrm5OQkLVq0kC1btqT5HGSFENSgWa1u3bpy6tQp+f3336VHjx7J1pk5c6Zmj8qWLSv79u3TYMmYQUpLTEyMXswPKJG1YNJJNEdhdGVkhm5Exsqsv0/rBfUyr7YslyrrY3T4StJ3t03lAnInLkGzQL1n75CFL9eX0PBYGTAvaaJJTEKKWh/MYp5yAk5M8/DNX6e1Nxe6t5tDwgg/oVCqg8k4HSkYAvSYG/RYGS2uHvTY/Sc0JaKcw2oBUWhoqNb7IJtjDvePHDmS5nOeffZZfV6jRo20BiI+Pl5efvllefPNN03rILOEgKZ8+fLi7OysrzF27Fjp3r17uvsyfvx4ee+99yz47ogezI9bz8qYpQc16Hi8SgH5pEt12XIqVBbtvCB/Hr6qhcejlxzUebnScuRyuKkJqneDEtLj2206OWm3mVvldnS8ZjhqFPXXZq6UE10CZmkf0bKcFihPXXtCp6XAeEJG2C9kncZ2qGLTIzNnJYw5lBPGHSIiOx6HaMOGDTJu3Dj54osvtLntxIkTMmTIEC2aRiE1LFy4UObOnav1Rqgh2rt3rwwdOlQKFSokPXv2THO7yFKhlskIARWa7oiyCwL8qetO6JQWgIEE338yaQwfDKSHCwYCfGT8Wh2gEN3V05qJ+sjdDFGFAn46cvK3PevoaM9HriQFSp1qFJZxnarctxAYwdIHHSrrdA+RsfGmYMggBvF2cxFv9xz1p4OI6L6s9lctT548msG5evW/cUQA9wsUSDsNj6AHzWMvvPCC3q9SpYpERkZKv379ZPTo0drk9tprr2mWqGvXrqZ1zp49q1mg9AIid3d3vRBZCzJAxmBo8GOlZdj/yqYqaM7v5yFl8vnoAIX7LtyS/1VMHhDFxCfIyWuRert8waSBAnN7ucr3ferqTOc1iwZkalJTQFBl61NSEBHl6G73bm5uUqtWrWQF0iiqxv369dMuVIyKitKgxxyCKjDWhqe3DrZNZA1XbkXLrrM37rnO4j0X9Lp/01IyvGW5dIOWasFJ4/vsOx+W6rHjVyO0SQw9ywqYZY+QSfq8aw2d5DQzwRARkSPJVIbozJkzsmbNGi2IbtKkiVSuXPmhXhzNVMja1K5dW4uk0e0eGR90k4fnn39eChcurNkdaNeunRZH16hRw9RkhqwRlhsDI9xGzVDRokW1yWzPnj36nD59+jzUvhI9qBe+36G1P7+8Ut/U28tceHSc7DxzM9mcWenBgIeLdl2QfRdSB0TGZrHyBXwZ+BARZVVAtH79eu3SfudO0hgnLi4uOh7Qc889Jw/qmWeekWvXrsnbb78tV65ckerVq8uqVatMhdbnzp1Llu1566239A89ri9evCh58+Y1BUBGU6dO1SCpf//+EhISorVDL730kr4GUXZDN3cEQ7B83+U0AyLM1I7C5eJBXlI8j/c9t4eu8MYMEbrAm4/MfOTy3fqhBxgckYjI0WV4HCL07ELdDwY9xKjSCEqWLFkily79N4eRveA4RGQp3/x1Sj787bDeRlf3f0Y+lip7M2rxAe3N1atBcXm3faV7bi8uIVEqv7NaYuITZd2IJskmKn3um22y+USoTOxcRZ6pw15QROR4bmfHXGYHDx7UHl4FCxaUgIAAmTRpkmZgrl+//iD7TOQQ1phNPorB/PZfuJXscfwe2Xg0RG83KXf/qSpcnZ1MYwelbDYz9jArX4BBPBFRZjllJupChsjIy8tLR39GFEZEqYVFxeoYQGCc7HTVv1eSrXM8JEInCcV8YPVL/jci9L38V1j937+9kPBoCY1ImgesbP6kHmZERJRFRdWrV6/WVFTKXmHIHhm1b98+M5skslsYJRq9vlDk3KdhcRkyf6+sPnhF5wkzNpttuJsdwiSlGZ0ktFqRpH+DGEk65YCMJYK82U2eiCirA6K0xvFBwbIR/shjZGgiEllzOKm5rHmFfPJY+Xw6d9ip0Eg5ERIhZe5mcYwTrDYpe//mspSF1Ycu3ZbY+ESd3sM0ICMLqomIsrbJDNmg+10YDBElQaCy8W6w06JCfvH1cJUGpZOaxFbfbTaLiImXHWeSxidqmoH6IaOigV46IWtsQqIpEDJmiJCNIiIiKw7MiIBoxYoVltocUY627fR1DXjy+Liban5a350I1VhH9M+JUJ3AFQFOift0tzeHTGzKARoPG8cgYoaIiMg6AREGR8TkqsHBwdKxY8eH3RyRXVh7OKk2qEWFfKaxglpUzK+zxGNcogs3o2TDsWum7FBmB1I0FmnvPX9Lu+KfCGGGiIgo2wMiDM74/fffS+PGjaVcuXLyzz//6MCHFy4kTT9A5MjQld7Y3b55haRBRgHZotrFkwZmXP3vVVOTWmaay4yqF/mv6/3JaxGaafJ1d5HgAE8LvQsiIseSqYBox44dWkSNyVcxzcaTTz6pv2wx+/zLL79sGmGayJFhCg3MSI+u9I1K/zdUBbS622w2a/NpXQcF0fVLJl8nI6rebTJDMLTj9A3ThK6csoOIKIsDoqpVq0qXLl0kKChIM0K7d++WESNG8A8wObT4hETTxMJGf97NDj1aJk+qLvCtKiX9aEAwBPVKBD5QN3lkm5ANwktjbjPggIxERNkQEB09elSbyJo1ayYVK1Z8iJcksg+/7b8sj4xfJw0mrJPp609IaESMLv/zbnd79C5LKTjAS6rcHWk6s93t06sjMo5+jQwRERFlcUB06tQprRd65ZVXtID61Vdf1ZnkmSEiR4PAp//cXTJg3m69jSk5Jq0+Kg3Gr5NBP+2RfXcDFIw9lBZjlgialkt7nYyofrfZzIgZIiKibAiIChcuLKNHj9ZeZT/88IPOTt+wYUOJj4+XOXPmyLFjxx5iN4iybrb5UYv3y/a7dTYPA01jy/ddkpafbpLfD1wRF6dcMrh5GZn8dDXN1mBcIDwOuJ/PzyPN7TxRtZDWF2EQxVJ5M97dPr0MkVE59jAjIsr62e7TgnnM5s6dK7NmzdKaosqVK8v+/fslp+Ns9/Zh07FrMnDebrkdHS/5/dxl42vNMjw9hrlbd+Lk9wOX5eddF2TX3bnJ0L394y7VTBOtGqfS+P6fMzrj/DvtKknbqgXT3ebZ65E6WGOgt9sDvjuRqNh4nfk+0SBSLMhL3x8RkSO7/RDn70xN3ZESXrR///562bt3r8yYMeNhNkdkEYjxZ/19Rsb+dkiDBbh6O0YW7TwvPeoXz9A2MLYPAqrFey5qF3qMPA3ICg1oVlov6CGWckqN6s9Uz9D2iwU9eGbIyMvNRSdyRa82jj9ERPRwHiogMoqJiZF169bJ0qVL5auvvrLEJokeSEx8gry15KCp59VTtYKlXH5fGfv7YZmx4aQ8U6doqkDGPJBCgfKSPRe16et6ZKzpsbL5faRTzWDpUL2wFMiddlOYNaCXGgKi6kUCrL0rRESOERAh6Hn33XdlzZo14ubmJq+//rp06NBBZs+erbVFzs7OMmzYsKzdW6J7uBObID1nbZftZ27oiNCj21bUWeZj4hPl679OyaVb0fLL7gvSrW7RVIHQj1vPyux/zsipa5Gm5Xl83KR9tcLSqWZhqVTIzyY7EAxvWU4qFvKTJ6sXtvauEBE5Rg3RG2+8odmfFi1a6DhE165dk969e8vWrVt16g6MUYSgyB6whijnSUg0aM8vjADt6+Ei05+tKY3NurRjIMT3VxzSsXvWv9pUXJ3/yxJ9tfGkjF95RG+j2LllpQLSqUZhHUfIxWw9IiKybdlSQ7Ro0SKdrqN9+/Zy8OBBHagRPcz27dtnk7+cybGM//2wBkNuzk7ybc86UrdE0hQZRsgKfbHhpFy4eUebxJ6uXUSXo2nMGAwNfqy0vNi4pBY7ExGRY8nwz1/MU1arVi29jd5k7u7u2kTGYIis7bt/zsg3m0/r7UldqqYKhgCjQb/UuKTexiCKGGF626nrMmLhPl3Wq0FxGfa/sgyGiIgcVIYDooSEBK0dMnJxcREfH5+s2i+iDME0Ge8t/1dvv9aq3D1rabo/UlS7uZ+9HiWf/XlcXvx+p44dhIESxzxRkcE9EZEDy3CTGUqNevXqpZkhiI6O1gldvb2Tdx9evHix5feSKA0HLtzSkaHRtb5rnSLSv2mp+3ZTf/HRkjJx1RGZtv6ELqtZ1F8+71pDnFGFTUREDivDAVHPnj2T3X/uueeyYn+IMjwCda/Z2+VOXIIWP3/QoXKGMjw96heTrzadlLCoOCke5CXf9KzzQIM1EhGRfXmokartFXuZ2bart6Ol84x/tEAa3eHn93skU7U/mJR18e4L8na7ihYZIJGIiHL++dtifYqPHDkiZcuWtdTmiNJ0KypOnv92uwZDyPDM6V0304XQmFLj2151GAwREZHlAyIM3Hjy5ElLbY4ozYEX+363Q45eDZd8vu7yQ996ktc3qaaNiIjoYXDUOcoRMJfYgHm7ZefZm+Ln4SLf960rRQK9rL1bRERkJywylxlRVoqIiZdXftwlfx0P1ZGk0dxVvgBru4iIyI4yRNOnT5fixYuLh4eH1KtXT7Zv337P9T/77DMpV66ceHp6SpEiRXRwSAwBYO7ixYvaCy4oKEjXq1KliuzcuTOL3wllhZDb0fLMV1s0GPJ0dZaZz9eWOsVTD7xIRESULRmigICAe3ZrxjQembVgwQIZPny4fPnllxoMIdhp1aqVHD16VPLly5dq/Xnz5snIkSNl1qxZ0qBBAzl27JiOjYT9mjx5sq5z8+ZNadiwoTRr1kxWrlwpefPmlePHj+v+U85yIiRCJ2u9GHZHJ1qd1auOVA32t/ZuERGRI3e7/+677x5ovKJ7QRBUp04dmTZtmt5PTEzUrM+gQYM08Elp4MCBcvjwYVm7dq1p2YgRI2Tbtm2yefNmvY/n/f333/LXX3/Jg2K3e+vbdfaG9P1up2m8oO/61GWvMCIisv7krpkJdDIiNjZWdu3aJaNGjTItc3JykhYtWsiWLVvSfA6yQj/++KM2q9WtW1dOnTolv//+u/To0cO0zrJlyzTL1KVLF9m4caMULlxY+vfvLy+++OI9e8jhYn5AyToQn3+/5ayM/e2wTqtRrYi/zOpZW4J82JuMiIjssIYoNDRU50fLnz9/suW4f+XKlTSf8+yzz8r7778vjRo1EldXVylVqpQ0bdpU3nzzTdM6CJJmzJghZcqUkdWrV8srr7wigwcPvmeGa/z48RpRGi/IUlH2C4+Ok4Hz9sg7y/7VYKh1pQLy04v1GAwREZH9F1VnxoYNG2TcuHHyxRdfyO7du3XetN9++00++OAD0zpodqtZs6auV6NGDenXr59mh1CnlB5kqZBeM17Onz+fTe+IjA5dui3tpm6W3w5cFhenXDrZ6oznaur8Y0RERFnNamebPHnyiLOzs1y9ejXZctwvUKBAms8ZM2aMNo+98MILeh+9xyIjIzXoGT16tDa5FSxYUCpWrJjseRUqVJBffvkl3X3BhLXGSWsp+609fFVembtbxxoqlNtDpnWvKTWLsgieiIgcIEPk5uYmtWrVSlYgjewO7tevXz/N50RFRWnQYw5BFRhrw9HDDL3UzKE3WrFixbLgXZAlxhgaufiABkPNyuWV3wY/ymCIiIiynVXbI9DlHsXatWvX1iJpdLtHxqd37976+PPPP69F0ajxgXbt2mn3ejSFoYfaiRMnNGuE5cbACOMSofgaTWZPP/20FmDPnDlTL2R7pq07IdfCY6RYkJd82aOWuLtw5nkiIsoBAREKoefMmaOZnJCQEM3qmFu3bl2Gt/XMM8/ItWvX5O2339ZC6urVq8uqVatMhdbnzp1LlhF66623dMwhXGPwRYwxhGBo7NixpnXQjX/JkiVaF4QC7BIlSmig1b1798y+VcpiZ0IjZdbm03p7TNuKDIaIiMj2xyEyHwsIAVHbtm21XiflYI2ffvqp5HQch+jB7Dl3UyavOSaDHisjdUvcfzTpF77bIX8eDpHGZfPKd73r3HPgTyIiIpsYh8ho/vz5snDhQnn88ccz+1Syc1PWHtcpNvacC5P5/R6RyoVzp7vuhqMhGgyhR9nbT1RkMERERDmrqBrF0KVLl86avaEc63Z0nGw+EWoqlO41e4ecvR6Z5roooH5/xSG93atBcSmdzydb95WIiOihAyJMlfH555+benURwbrDIRKXYNBpNioW9JPQiBjp8e12CQlPPvEufL/ljJy6Fqnzkw1uUcYq+0tERPRQTWaYM2z9+vU6cWqlSpV0xGhzGCyRHM/Kg5f1+omqheT5BsXkqRlb5NyNKOk1a4fMf+kRiYtPlO2nb8jWU9fl510XdN3XW5UXP4/k3x8iIqIcERD5+/tLx44ds2ZvKEeKjImXDUev6e3WlQtIPl8P+aFvXek84x85dPm2PDpxvdy6E5fsOXWKB8hTtYKttMdEREQPGRDNnj07s08hO4dgKCY+UYoEekqlQklV/ZiZfk7vutJ15lZTMFQuv688UjJQHikZJM3K5xMnJxZSExFRDh+YEeMHGUeELleunI4JRI7dXNamcvJhGNDLbOnAhlovVKtYgAR6u1lxL4mIiCxYVI2RpPv06aNjEDVu3FgvhQoVkr59++rUGuRYouMSZP2REFNzWUql8vrI/yrmZzBERET2FRBhuo2NGzfK8uXLJSwsTC9Lly7VZeiBRo4F4w5FxiZIAT8PqR7sb+3dISIiyp4mM8wa//PPP0vTpk1NyzBIo6enp84dNmPGjAfbE8rRzWXIDrEmiIiIHCZDhGYx41xj5vLly8cmMweDARb/PHQ13eYyIiIiuw2I6tevL++8845ER/834N6dO3fkvffe08fIcWw5dV1uR8frAIt1it9/7jIiIiK7aTLDKNWtWrWS4OBgqVatmi7bt2+feHh4yOrVq7NiH8lGrbrbXPa/igXEmc1lRETkSAFR5cqV5fjx4zJ37lw5cuSILuvWrZt0795d64jIMcQnJMof/yY1lz1ehc1lRETkgOMQeXl5yYsvvmj5vaEc09V+2IK9cj0yVvy9XHWgRSIiIrsPiJYtWyZt2rTRectw+17at29vqX0jG3QtPEZe/H6n7D0fJm7OTjKuYxVxdc50KRoREZFNyWXIwLT1Tk5OcuXKFe1JhtvpbixXLklISJCc7vbt25I7d265deuW+PklTUVBIsevhkvvOTvkws07mhn66rlaUo/ZISIisoPzd4YyRImJiWneJsex7dR1eeH7nRIeHS/Fgrxkdq86UjKvj7V3i4iIyCIy3dbx/fffS0xMTKrlsbGx+hjZHyQRhy/cp8FQ7WIBsqR/QwZDRETk2AFR7969NRWVUnh4uD5G9ufIlXC5GHZHPFyd5Pu+dTkvGRER2R2nB8kWmM9obnThwgVttyP7s/5o0uStDUrlES+3B+qYSEREZNMyfHarUaOGBkK4NG/eXFxc/nsqCqlPnz4trVu3zqr9JCsyzmbfrHw+a+8KERGRdQOiDh066PXevXt1pGofn/9qSNzc3KR48eLSuXPnrNlLsppbUXGy6+xNvd2sXF5r7w4REZF1AyLMXwYIfJ555hmdqoPs38bj1yTRIFI2v48EB3hZe3eIiIiyRKYLQnr27Jk1e0I2aQOby4iIyAFkOiBCvdCnn34qCxculHPnzml3e3M3btyw5P6RFSUkGmTDsWt6u1k5BkRERGS/Mt3L7L333pPJkydrsxm63w8fPlw6deqkI1i/++67WbOXZBX7L4TJjchY8fVwkVrFAqy9O0RERLYTEGGW+6+//lpGjBihPc0w0/0333wjb7/9tmzdujVr9pKs2ruscZm8nK+MiIjsWqbPcpjTrEqVKnobPc2MgzQ+8cQT8ttvvz3QTkyfPl2LtVGoXa9ePdm+ffs91//ss8+kXLly4unpKUWKFJFhw4ZJdHR0mutOmDBBhwoYOnToA+2bI1t3d/wh1g8REZG9y3RAFBwcLJcvX9bbpUqVkj/++ENv79ixQ9zd3TO9AwsWLNBmN/Ri2717t1SrVk279YeEJJ2MU5o3b56MHDlS1z98+LB8++23uo0333wz1brYp6+++kqqVq2a6f1ydCG3o+Xgxdt6u0lZdrcnIiL7lumAqGPHjrJ27Vq9PWjQIBkzZoyUKVNGnn/+eenTp0+mdwD1SC+++KJO+1GxYkX58ssvxcvLS2bNmpXm+v/88480bNhQnn32Wc0qtWzZUpvtUmaVIiIipHv37tq8FxDA+pfM2nA0qZi6WnBuyeub+UCXiIjIrnuZoQnKCIXVRYsWlS1btmhQ1K5du0xtCz3Udu3aJaNGjTItQ3F2ixYtdJtpadCggfz4448aANWtW1dOnTolv//+u/To0SPZegMGDJC2bdvqtj788MN77gcmqzWfsPb27aTMiCMzTtfB5jIiInIEDz0xVf369fXyIEJDQ7Ubf/78+ZMtx/0jR46k+RxkhvC8Ro0a6bxq8fHx8vLLLydrMps/f742v6HJLCPGjx+vvecoSWx8ovx1PFRvs7s9ERE5ggwFRMuWLcvwBtu3by9ZacOGDTJu3Dj54osvtAD7xIkTMmTIEPnggw+0+e78+fN6f82aNRkeTRsZKtQxmWeIUKztqHaevSERMfGSx8dNqhTmhL1ERGT/XDIzj5kRem0hO5NyGSDjk1F58uQRZ2dnuXr1arLluF+gQIE0n4OgB81jL7zwgt5Hj7fIyEjp16+fjB49WpvgUJBds2ZN03OwT5s2bZJp06Zp0xhe0xyKwR+kINxeLdp5Qa+blM0nTk5JnysREZE4elF1YmKi6YJeZdWrV5eVK1dKWFiYXnAbAciqVasy9eKYFLZWrVqmIm3ja+F+es1wUVFRWmdkzhjgIEhr3ry5HDhwQCehNV5q166tBda4nTIYouSOXLktv+69qLd7NShu7d0hIiKyzRoijOeDnmCo4TFCN3n0DEOWBl3hMwNNVZgfDUELiqQxxhAyPuh1Bui9VrhwYa3zARRuo2dajRo1TE1myBphOYIdX19fqVy5crLX8Pb2lqCgoFTLKbWPVx8TJP8er1JAqgSzuYyIiBxDpgOikydPir+/f6rluXPnljNnzmR6B9BT7dq1azrSNQZ9RPYJmSZjoTXmSzPPCL311lvaPIfrixcvSt68eTUYGjt2bKZfm5Lbdfam/Hn4qqCVbPj/yll7d4iIiLJNLkPKYqD7aNy4sRYr//DDD6agBTU/yORgtOiNGzdKToeiagR4GIXbz89PHAG+Bl1nbpVtp2/IM7WLyMSnOJglERE5zvk70wMzYsBEjFSN8YdKly6tF9xGtgajRlPOtOl4qAZDbi5OMqRFGWvvDhERkW03mSEA2r9/v3ZrN44VVKFCBR0A0djTjHKWxESDTFqd9Fn2eKSYFPL3tPYuERER2f7AjAh8MGUGLpTzrTx4Rect83Zzlv5NS1l7d4iIiGwzIJoyZYr2IEPtEG7fy+DBgy21b5QNYuIT5JM/jurtFx4tKUE+HI+JiIgcT4aKqkuUKCE7d+7Uruu4ne7GcuXSucVyOkcpqsZHP3TBXlm695IEervJxteaiq+Hq7V3i4iIKNvP3xnKEJ0+fTrN25SzTV5zTIMhF6dcMqVrDQZDRETksDLdy4zsw8Kd52XquhN6e1zHKtKoTB5r7xIREZHVZChDZD7x6f1gFGmybX+fCJU3Fx/Q2wOalZKn6zjuRLZEREQZDoj27NmToaPFbve27/jVcHn5x10Sn2iQdtUKyQiOSE1ERJSxgGj9+vVZvyeU5SJi4uXF73dKeHS81C4WIJOeqsrZ7ImIiFhD5DjQo2z0kgNy5nqUFMrtITOfry0ers7W3i0iIqKcOzAjuuAvXLhQJ16NjY1N9tjixYsttW90nwAnOi5RPN0yFtQs2nlBe5Q5o0dZtxrazZ6IiIgeMEM0f/58adCggRw+fFiWLFkicXFx8u+//8q6deu07z9lj7d+PSjV3v9D/jx09b7rnggJl7eXHdTbw/9XVmoXD8yGPSQiIrLjgGjcuHHy6aefyvLly8XNzU0+//xzndPs6aef1kleKeuFRcVqxic2PlGGzN8jR67cTnfd6LgEGTB3j2aTGpXOI6804dQcREREDx0QnTx5Utq2bau3ERBFRkZq77Jhw4bJzJkzM7s5SsOFm1FyKexOuo8v23dJYhMS9XZkbIK88N1OuR4Rk+a67684JEevhkseHzeZ/Ew1FlETERFZIiAKCAiQ8PBwvV24cGE5eDCpKSYsLEyioqIyuzkyE5eQKJ+uOSZNJm2Q1p9tktB0gpyfd13Q60GPlZZiQV5y4eYdeWXubs0YGV0Lj5G3fj0g87ad0/uTn64u+Xw9sumdEBER2XlA1LhxY1mzZo3e7tKliwwZMkRefPFF6datmzRv3jwr9tEhnLoWIU/N+Ec+X3tcEhINcjs6XmZtTj1NytEr4bL/wi2dbqNXg+LyzfO1xcfdRbafviHvLDsokTHx8tmfx6TppPXy49akYGhI8zLSuGxeK7wrIiIiO5rcFZAJqly5sty4cUOio6OlUKFCkpiYKB999JH8888/UqZMGXnrrbc0g5TTZefkrjj8P247J2N/O6R1Pn4eLtKxRmH5bstZDXQ2v9FM/L3+6xE27vfDMnPTKWlZMb92nYf1R0Kkz3c7BJ+kr4eLjjME1YJzy6jHK8gjJYOy9D0QERE5xOSuULVqValTp4688MIL0rVrV13m5OQkI0eOzPwek8kXG07KpNVH9XbD0kHycZdqkt/XQ7adviFHroTLnH/OyNAWZfXx+IREWbz7ot5+qlawaRvNyueTN9tUkLG/H9ZgCM1or7UqJ22rFOTo4URERJZsMtu4caNUqlRJRowYIQULFpSePXvKX3/9ldGnUzpWHbximlPshz71pGBuTy18HvhYaV0+++8zEh4dp7c3Hb+mdUVB3m4aBJl74dESMr5TFZnYuYqsGdZEnqhaiMEQERGRpQOiRx99VGbNmiWXL1+WqVOnypkzZ6RJkyZStmxZmThxoly5knRip4xLTDTIyWsRertjjeBkPcDaVC4opfJ6y607caZaIGMx9ZPVC4urc/KPDsFPt7pF5Zk6RcXNhQOQExERZUamz5ze3t7Su3dvzRgdO3ZMC6unT5+uYxC1b98+a/bSTl2+HS1RsQlaII1mLnMYUXpAs6Qs0Td/ndJu+H8eCknVXEZEREQP76FSCaVLl5Y333xTi6l9fX3lt99+s8AuOY4TIUnZoeJ5vFNlfKB9tUJSNNBLrkfG6lhDGHuoUiE/qVgoawu9iYiIHM0DzWUGmzZt0ia0X375RYurMVJ13759Lbt3DhIQlc7rk+bjLs5O0r9pKRm5+IAcupw0GjWzQ0Rka9BbNj4+XhISEqy9K2TnnJ2dxcXFJUtqZDMVEF26dEnmzJmjlxMnTuicZlOmTNFgCE1p9GABUal86R+7TjWDdWyiy7eixdU5l9YPERHZCkzwjdpSDsxL2cXLy0s7d2G2DKsERG3atJE///xT8uTJI88//7z06dNHypUrZ9GdcTTGgurS+dLOEAEKpNHjbPSSg9K6ckHOUk9ENgNj0Z0+fVp/tWNsOpyg2LuVsjITiQD82rVr+r3D+Idoocr2gMjV1VV+/vlneeKJJ/TLTw/vpKnJzPee6z1bt6iUyefL2iEisik4OSEoKlKkiP5qJ8pqnp6eGo+cPXtWv38eHpabkirDodWyZcvkySefzJJgCL3Uihcvrm+sXr16sn379nuu/9lnn2l2CgcG/xAxsSxGzzYaP368DiKJQu98+fJJhw4d5OjRpMEPbcXNyFgtlr5fkxngF1fdEoE6cjURka2x5K90Imt936z+LV6wYIEMHz5c3nnnHdm9e7dUq1ZNWrVqJSEhSV3MU5o3b56Ojo31Dx8+LN9++61uA73djDAkwIABA2Tr1q0671pcXJy0bNlSIiMjxVacuNtcVtjfU7zcGOgQERFZk9UDosmTJ+vksBjbqGLFivLll19q6hU92NKCedMaNmwozz77rGaVEOhgYlnzrNKqVaukV69eOrI2AiwUgZ87d0527doltldQnX79EBER2a6mTZvK0KFDTfdxTkILxv0y/r/++utDv7altkM2EhCh/Q9BSosWLf7bIScnvb9ly5Y0n4OebXiOMQA6deqU/P777/L444+n+zqY5A0CAwMlp3S5JyKirNGuXTtp3bp1mo9hSioEG/v378/0dnfs2CH9+vUTS3r33XelevXqqZajZx86O2WlOXPmiL+/f7qPI/GAY4UL6npKlCghr7/+erISFqMLFy5o0T0miU+LcTu4YHJWJD7WrVsnDhMQhYaG6rgV+fPnT7Yc99ObCgSZoffff18aNWqkH0CpUqU0SjdvMjOHgj9E8Di46X0QMTExOkOu+cUWutwTEZHlYcw8lFPgJJ3S7NmzpXbt2jqheWblzZs324rLCxQoIO7u7mJtrVu31uAMyYlPP/1UvvrqKy1pSSu4whA9OL9u27YtzW3h2GNbf//9t/ZoRycubNdhmswya8OGDTJu3Dj54osvtOZo8eLFOkL2Bx98kOb6qCU6ePCgzJ8/P91toggbEanxgkLtrMYMERGRdeBEi+AFJ2lzERERsmjRIg2Yrl+/ruUYhQsX1iCnSpUq8tNPP91zuymbzI4fPy6NGzfWDkMoCUEQltIbb7yhc4LiNUqWLCljxozRulfA/r333nuyb98+U/bEuM8pm8wOHDggjz32mHY2CgoK0kwV3o95NgcdjD7++GMdwwfr4PxofK0HhaAMwRnOm9g+WnhSvk90l0ew06NHD01qoPY3LchGYVtIXsyYMUPu3LmT5jHLKlat5kUEiF5rV69eTbYc93FQ0oIvCw7qCy+8oPfxJUWxND780aNHJ6s+HzhwoKxYsUJH1Q4OTn+E51GjRmlhtxEi2KwMiu7EJsjFsDv3HYOIiCinwcnvTpx1Rqz2dHXO0DhIGOkY4+khuMB5w/gcBENotUAghGCiVq1aGrD4+fnpD2+ce9AqUbdu3fu+BlonOnXqpC0eyIigdMO83sgIvaGxHxjHCUENamqxDE1PzzzzjP6gR10sxgEE/GhPCedAdEaqX7++NtuhUxLOkTgHmgd969ev12AI1xhcGdtHcxxe0xKwr6jzLVasWLLleD0M3IlgCQEmSl+QTbrXgM4I7IylNQ4REKE9EV+4tWvXamRp/BLhPj7ItOCgpuxyZxwKAP8QjdeDBg2SJUuWaEYJ7Zr3i3CzM/VoHJAxwMtVgnysn/IkIrIUBEMV315tldc+9H6rDPfaxeDCkyZN0l7JKLsAZDE6d+5sai149dVXTevjnLJ69WpZuHBhhgIiBDBHjhzR5yDYAbRupKz7wVyg5hkmvCZaNBAQISjw8fHRAC69JIGx9zXqdr7//ntTkDFt2jStlZo4caKpLCUgIECX45xZvnx5adu2rZ5vHyYgWrFihe4jpm5B+QnOz3gNc8gIde3aVV8X2R9kwhB8ImuV3nkexwXrN2nSRLKL1ft7IzPTs2dPbbPFlwzpRkS76HUGiOIRUaJZC/ABo2dajRo1dMwiRLnIGmG5MTBCGhBfkKVLl2qkbaxHwhfcGHXa+gjVRESUdRAQIFOBHs0IiHAuQUE1alQBmSIEMAiALl68qJkKnPAzWiOEYWHQ0mAMhgAZnJQwbAymwDp58qRmpRBYICOVGXgt9Kg2z7igbhYJBozBZwyI0PPafCxBZIuQlXoYzZo10+YtnLeR9UHwhqDSKCwsTEtbNm/ebFr23HPPaZCUMiBCZg77h6YyNGlinQep5cqxARFSdhiG++2339bABek7pAeNHyC6y5tnhBA1Ir2Ja3xJcdAQDI0dO9a0Dj4cMEb9Roj+04tIs5OpfogBERHZGTRbIVNjrdfODNQKIfODwYFxfkBzmDEjgezR559/rj/SUZqBYANNXpZswkFv6u7du2udEJq88KMd2aFPPvlEsgI6IpnDuRRB08Pw9vaW0qVL620ElwjMEMgYJ3s3Zq+QwDBCKw5e99ixY1o/ZYSACs1qOA44t2c3qwdEgOax9JrI0ORlDtEnKtjTqmI3Mjad2SpTDzMWVBORncFJNqcMNoteT0OGDNGTNpqbXnnlFVM9EXo6YXYGZDPAeAJHcXRGVKhQQc6fP6+9ppCJAQwWbM5Yb4M6JiNMSZGytATZqvu9FmqFkKUxZomw/0gmZOeco05OTtrjGy0/KJ5GiwyCoxEjRqRKRvTv318DqAkTJpiWoVnQGFxZQ47rZWYPmCEiIrI+1L6glQIdaxC4mJ+0MXEoejghaEGT1EsvvZSqA9C9INOB7AdKQtBLDM1x5oGP8TXQCoKsEJrM0HSG2ldzqCvCRKZ79+7VoWrQbJcSskzoyYbXQmEzipiR+UIReMphbTIrISFBX9v8guORni5dumizF7JuWBe9wVHgjdoh8wuax7777jttIrQVDIiyWXxCopy5njSFCDNERETWhaadmzdvapOVeb0PyjJq1qypy1F+geyFsfNPRrMlCG5QD4P6WAQF5qUd0L59e52LEy0kKBdB8IWaWHOox8FYP6jVQTNSWl3/UdeE4u0bN27oPJ5PPfWUNG/ePFVx84OIiIjQml3zC8pU0oNWHLyfjz76SIMiZNRQr5VSx44dtTccBla2FbkMtt6+ZAXodo82THSTzGxxW0YKqpt/slHbuv99r5U4Od2/iygRkS1CbQiyF+jJa8lZx4ke9Hv3MOdvZoiy2cm7zWUl83ozGCIiIrIRDIisNMs964eIiIhsBwOibMYpO4iIiGwPAyIrNZkxQ0RERGQ7GBBlI9Svn7yW1MOMAREREZHtYECUja7cjpaImHhxdsolxYLSn9SOiCgnYWdlsofvGwMiK9QPFQv0EjcXHnoiytmMU0FgMk6i7GL8vqWciuRh5Yzx1e3E6dC7AzKyuYyI7ABGJPb399cB9owDBBqnviDKiswQgiF83/C9M5+o1hIYEGWjHo8Uk/9VzC+x8Q83mR4Rka3ACM5gDIqIshqCIeP3zpIYEGUj/HIqmNvT2rtBRGTZv2sFC0q+fPkkLi7O2rtDds7V1dXimSEjBkRERPTQcJLKqhMVUXZgZS8RERE5PAZERERE5PAYEBEREZHDYw3RPQZ9un37trV3hYiIiDLIeN5+kMEbGRClITw8XK+LFCli7V0hIiKiBziP586dO1PPyWXgmOupJCYmyqVLl8TX19fig4whekWgdf78efHz87PotiltPObZj8c8+/GYZz8ec9s75ghpEAwVKlRInJwyVxXEDFEacBCDg4Oz9DXwQfIfUPbiMc9+PObZj8c8+/GY29Yxz2xmyIhF1UREROTwGBARERGRw2NAlM3c3d3lnXfe0WvKHjzm2Y/HPPvxmGc/HnP7OuYsqiYiIiKHxwwREREROTwGREREROTwGBARERGRw2NARERERA6PAVE2mj59uhQvXlw8PDykXr16sn37dmvvkt0YP3681KlTR0cXz5cvn3To0EGOHj2abJ3o6GgZMGCABAUFiY+Pj3Tu3FmuXr1qtX22NxMmTNCR3YcOHWpaxmNueRcvXpTnnntOj6mnp6dUqVJFdu7caXoc/WTefvttKViwoD7eokULOX78uFX3OSdLSEiQMWPGSIkSJfR4lipVSj744INkc2XxmD+cTZs2Sbt27XR0afwN+fXXX5M9npHje+PGDenevbsO1ujv7y99+/aViIiITO0HA6JssmDBAhk+fLh2F9y9e7dUq1ZNWrVqJSEhIdbeNbuwceNGPfFu3bpV1qxZI3FxcdKyZUuJjIw0rTNs2DBZvny5LFq0SNfH9CydOnWy6n7bix07dshXX30lVatWTbacx9yybt68KQ0bNhRXV1dZuXKlHDp0SD755BMJCAgwrfPRRx/JlClT5Msvv5Rt27aJt7e3/q1BcEqZN3HiRJkxY4ZMmzZNDh8+rPdxjKdOnWpah8f84eDvNM6JSBqkJSPHF8HQv//+q3//V6xYoUFWv379Mrcj6HZPWa9u3bqGAQMGmO4nJCQYChUqZBg/frxV98tehYSE4OebYePGjXo/LCzM4Orqali0aJFpncOHD+s6W7ZsseKe5nzh4eGGMmXKGNasWWNo0qSJYciQIbqcx9zy3njjDUOjRo3SfTwxMdFQoEABw6RJk0zL8Dm4u7sbfvrpp2zaS/vStm1bQ58+fZIt69Spk6F79+56m8fcsvD3YcmSJab7GTm+hw4d0uft2LHDtM7KlSsNuXLlMly8eDHDr80MUTaIjY2VXbt2aZrPfL403N+yZYtV981e3bp1S68DAwP1GscfWSPzz6B8+fJStGhRfgYPCZm5tm3bJju2wGNuecuWLZPatWtLly5dtGm4Ro0a8vXXX5seP336tFy5ciXZMce8Tmii5zF/MA0aNJC1a9fKsWPH9P6+fftk8+bN0qZNG73PY561MnJ8cY1mMvzbMML6OM8io5RRnNw1G4SGhmo7dP78+ZMtx/0jR45Ybb/sVWJiotaxoGmhcuXKugz/oNzc3PQfTcrPAI/Rg5k/f742AaPJLCUec8s7deqUNt+g+f3NN9/U4z548GA9zj179jQd17T+1vCYP5iRI0fqDOsI5p2dnfVv+dixY7WJBnjMs1ZGji+u8QPBnIuLi/4gzsxnwICI7DJjcfDgQf0VR1nn/PnzMmTIEG2zR0cByp5gH7+Cx40bp/eRIcJ3HbUVCIjI8hYuXChz586VefPmSaVKlWTv3r36gwsFwDzm9oVNZtkgT548+ssiZe8a3C9QoIDV9sseDRw4UAvq1q9fL8HBwablOM5ougwLC0u2Pj+DB4cmMXQKqFmzpv4awwWF0yh+xG38guMxtyz0sqlYsWKyZRUqVJBz587pbeNx5d8ay3nttdc0S9S1a1ft0dejRw/tLICercBjnrUycnxxnbKDUnx8vPY8y8xnwIAoGyCdXatWLW2HNv+lh/v169e36r7ZC9TiIRhasmSJrFu3TrvImsPxR88c888A3fJxIuFn8GCaN28uBw4c0F/MxguyF2hKMN7mMbcsNAOnHE4CtS3FihXT2/je4wRgfszR3IM6Ch7zBxMVFaW1KObwAxd/w4HHPGtl5PjiGj+88CPNCOcBfEaoNcowi5WG0z3Nnz9fq+LnzJmjFfH9+vUz+Pv7G65cuWLtXbMLr7zyiiF37tyGDRs2GC5fvmy6REVFmdZ5+eWXDUWLFjWsW7fOsHPnTkP9+vX1QpZj3ssMeMwta/v27QYXFxfD2LFjDcePHzfMnTvX4OXlZfjxxx9N60yYMEH/tixdutSwf/9+w5NPPmkoUaKE4c6dO1bd95yqZ8+ehsKFCxtWrFhhOH36tGHx4sWGPHnyGF5//XXTOjzmD99Tdc+ePXpBWDJ58mS9ffbs2Qwf39atWxtq1Khh2LZtm2Hz5s3a87Vbt26Z2g8GRNlo6tSpenJwc3PTbvhbt2619i7ZDfwjSusye/Zs0zr4x9O/f39DQECAnkQ6duyoQRNlXUDEY255y5cvN1SuXFl/YJUvX94wc+bMZI+jm/KYMWMM+fPn13WaN29uOHr0qNX2N6e7ffu2fqfxt9vDw8NQsmRJw+jRow0xMTGmdXjMH8769evT/PuNYDSjx/f69esaAPn4+Bj8/PwMvXv31kArM3Lhf5ZNcBERERHlLKwhIiIiIofHgIiIiIgcHgMiIiIicngMiIiIiMjhMSAiIiIih8eAiIiIiBweAyIiIiJyeAyIiMgqzpw5I7ly5dJpPmzFkSNH5JFHHtHJaqtXry62DMfu119/tfZuENkNBkREDqpXr156Up0wYUKy5TjJYrkjeuedd8Tb21vnCzOfOymt45by0rp162zfXyKyHAZERA4MmZCJEyfKzZs3xV7ExsY+8HNPnjwpjRo10slSg4KC0l0Pwc/ly5eTXX766acHfl0isj4GREQOrEWLFjqT9Pjx49Nd5913303VfPTZZ59J8eLFk2VNOnToIOPGjZP8+fOLv7+/vP/++xIfHy+vvfaaBAYGSnBwsMyePTvNZqoGDRpocFa5cmXZuHFjsscPHjwobdq0ER8fH912jx49JDQ01PR406ZNZeDAgTJ06FDJkyePtGrVKs33gZmvsU/YD3d3d31Pq1atMj2OLA9my8Y6uI33nR48H8fN/BIQEJBsWzNmzND99vT0lJIlS8rPP/+cbBsHDhyQxx57TB9H8NWvXz+JiIhIts6sWbOkUqVK+noFCxbU92kOx6Fjx47i5eUlZcqUkWXLlpkeQ5DbvXt3yZs3r74GHk/r+BNREgZERA7M2dlZg5ipU6fKhQsXHmpb69atk0uXLsmmTZtk8uTJ2vz0xBNPaKCwbds2efnll+Wll15K9ToImEaMGCF79uyR+vXrS7t27eT69ev6WFhYmAYNNWrUkJ07d2oAc/XqVXn66aeTbeO7774TNzc3+fvvv+XLL79Mc/8+//xz+eSTT+Tjjz+W/fv3a+DUvn17OX78uD6OLA+CD+wLbr/66qsPdTzGjBkjnTt3ln379mlg0rVrVzl8+LA+FhkZqa+PY7Njxw5ZtGiR/Pnnn8kCHgRUAwYM0EAJwROCndKlSyd7jffee0+PBd7P448/rq9z48YN0+sfOnRIVq5cqa+L7SFgJKJ0POQktUSUQ2Em6SeffFJvP/LII4Y+ffro7SVLluhM00bvvPOOoVq1asme++mnnxqKFSuWbFu4n5CQYFpWrlw5w6OPPmq6Hx8fb/D29jb89NNPev/06dP6OhMmTDCtExcXZwgODjZMnDhR73/wwQeGli1bJnvt8+fP6/OMs103adLEUKNGjfu+30KFChnGjh2bbFmdOnUM/fv3N93H+8T7vRe8V2dnZ30v5hfzbWP/Xn755WTPq1evnuGVV17R25ihPiAgwBAREWF6/LfffjM4OTkZrly5YtpfzKqeHrzGW2+9ZbqPbWHZypUr9X67du10xm8iyhiX9AIlInIcqCNCJuZhsiLIrjg5/Zd0RvMWmsDMs1FoGgoJCUn2PGSFjFxcXKR27dqmTAqyK+vXr9fmsrTqfcqWLau3a9Wqdc99u337tmavGjZsmGw57uM1MqtZs2aacTGHZsH03pfxvrFHHd5ftWrVtIDbfF/QrIeCbjS5YX+bN29+z/2oWrWq6Ta25efnZzq+r7zyimaodu/eLS1bttQmTTRNElHaGBARkTRu3FibcEaNGqX1QOYQ5CQlJP4TFxeXahuurq7J7uOkntYynPQzCjU1aEJDwJYSamqMzAOL7IDXS9l8ZUmo+cmIex1f1C+dPXtWfv/9d1mzZo0GV2iCQ5MhEaXGGiIiUuh+v3z5ctmyZUuy5SjKvXLlSrKgyJJjB23dutV0G0XYKGyuUKGC3q9Zs6b8+++/WsCNAMT8kpkgCJmTQoUKaY2ROdyvWLGiZAXz92W8b3xfuEZmCrVE5vuC4LNcuXLi6+ur7zm9rv8Zhc+uZ8+e8uOPP2oh/MyZMx9qe0T2jAEREakqVapoUe6UKVOSLUcvrmvXrslHH32kzVTTp0/XQl1LwfaWLFmivc2QwUDvqD59+uhjuI8i4W7dumnxMV5/9erV0rt3b0lISMjU66B4G5mmBQsWaLPUyJEjNbAbMmRIpvc5JiZGg0Tzi3nPN0ChNHqJHTt2TAvMt2/fbiqaxnFGrzoEK+hFh2bBQYMGaQ86NDUCermhCByfBwq/0fSF4veMevvtt2Xp0qVy4sQJDSpXrFhhCsiIKDUGRERkgi7nKZu0cBL94osvNHBB3QtO7A/bAytlZgoXbHvz5s3am8rYG8qY1UHwgzoYBG3oXo9u/eb1ShkxePBgGT58uPYiw3bQYw2vhe7omYXnosnO/ILxi1L2AJs/f77W+Xz//fc6TpExG4Vu8gjsEOzVqVNHnnrqKW3SmjZtmun5CJaQ1cGxR30WeuwZe8RlBHrdoQkUr48mUdRwYX+IKG25UFmdzmNERPQAUMuDrBcKmYkoZ2CGiIiIiBweAyIiIiJyeOx2T0RkYaxEIMp5mCEiIiIih8eAiIiIiBweAyIiIiJyeAyIiIiIyOExICIiIiKHx4CIiIiIHB4DIiIiInJ4DIiIiIjI4TEgIiIiInF0/wfR/rjdqY2zNwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from glycowork.ml import model_training\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "dataloaders = glycan_loaders\n",
    "model = prep_model('SweetNet', classes) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes)\n",
    "model_ft = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 100, mode = 'multilabel',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Try with my new glm-infused data\n",
    "\n",
    "classes = 15 # 15 kingdoms in the dataset, should read that from the data instead\n",
    "dataloaders = multilabel_kingdom_loaders_emb\n",
    "model = prep_model('SweetNet', classes, use_external_embeddings = True) \n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, 0.0005, num_classes = classes) #changed to 0.01 from 0.005 from 0.0005\n",
    "model_ft = train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 1, mode = 'multilabel', use_external_embeddings = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5025834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweetnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
