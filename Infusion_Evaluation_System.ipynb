{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b80efd",
   "metadata": {},
   "source": [
    "# Infusion Evaluation System\n",
    "\n",
    "Used to Evaluate the performance of Infused, vs noninfused models over several training runs to compare different metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d390a",
   "metadata": {},
   "source": [
    "### ||RUN ON RESTART||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ba33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "\n",
    "from utils import build_multilabel_dataset, multilabel_split, prep_infused_sweetnet\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from glycowork.ml.processing import split_data_to_train\n",
    "from glycowork.ml import model_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab54e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            glm_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433df162",
   "metadata": {},
   "source": [
    "## Evaluation Loop\n",
    "Change parameters here for each trial run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aad01d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 262 unique individual classes/labels.\n",
      "Number of unique glycans left after filtering rare classes (size >= 6): 5248/6560\n",
      "Number of unique labels left after filtering: 96\n"
     ]
    }
   ],
   "source": [
    "# Load part of dataset to train the model on\n",
    "\n",
    "glycans, labels, label_names = build_multilabel_dataset(glycan_dataset='df_tissue', \n",
    "                                                        glycan_class='tissue_sample', \n",
    "                                                        min_class_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead4650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize all_training_histories which is used to save training data\n",
    "all_training_histories = {} \n",
    "# Only run this cell when you run an entirely new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74cb5056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run settings\n",
    "\n",
    "# file to save the run data to\n",
    "saved_run_data = \"evaluation_run_dump\"\n",
    "\n",
    "trial_seed = 1\n",
    "#increment each trial by 1\n",
    "\n",
    "config_description = 'Kindomd'\n",
    "# baseline, infused_train, or infused\n",
    "\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f4a46f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete!\n",
      "Train set size: 3673\n",
      "Validation set size: 787\n",
      "Test set size: 788\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "train_glycans, val_glycans, test_glycans, \\\n",
    "    train_labels, val_labels, test_labels = multilabel_split(glycans, labels, train_size=0.7, \n",
    "                                                             random_state=trial_seed)\n",
    "\n",
    "# Load into dataloders for training and validation\n",
    "dataloaders = split_data_to_train(\n",
    "    glycan_list_train = train_glycans, glycan_list_val = val_glycans, labels_train = train_labels, labels_val = val_labels,\n",
    "    batch_size = 128,  # 32 or 128 seem to work well on this system\n",
    "    drop_last = False,\n",
    "    augment_prob = 0.0,  # Adjust if you want augmentation for training\n",
    "    generalization_prob = 0.2  # Adjust if you want generalization for training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7aca0a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SweetNet model instantiated with lib_size=2565, num_classes=96, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 4.4966 LRAP: 0.0000 NDCG: 0.1701\n",
      "val Loss: 4.3607 LRAP: 0.0000 NDCG: 0.1764\n",
      "Validation loss decreased (0.000000 --> 4.360734).  Saving model ...\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 3.9080 LRAP: 0.0008 NDCG: 0.2086\n",
      "val Loss: 4.3394 LRAP: 0.0025 NDCG: 0.2004\n",
      "Validation loss decreased (4.360734 --> 4.339397).  Saving model ...\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 3.7470 LRAP: 0.0071 NDCG: 0.2227\n",
      "val Loss: 3.6905 LRAP: 0.0064 NDCG: 0.2268\n",
      "Validation loss decreased (4.339397 --> 3.690483).  Saving model ...\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 3.6604 LRAP: 0.0106 NDCG: 0.2297\n",
      "val Loss: 3.5760 LRAP: 0.0076 NDCG: 0.2448\n",
      "Validation loss decreased (3.690483 --> 3.576006).  Saving model ...\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 3.5641 LRAP: 0.0090 NDCG: 0.2335\n",
      "val Loss: 3.4915 LRAP: 0.0076 NDCG: 0.2339\n",
      "Validation loss decreased (3.576006 --> 3.491524).  Saving model ...\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 3.5045 LRAP: 0.0215 NDCG: 0.2361\n",
      "val Loss: 3.3776 LRAP: 0.0102 NDCG: 0.2513\n",
      "Validation loss decreased (3.491524 --> 3.377636).  Saving model ...\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 3.4384 LRAP: 0.0133 NDCG: 0.2401\n",
      "val Loss: 3.5595 LRAP: 0.0102 NDCG: 0.2315\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 3.3401 LRAP: 0.0272 NDCG: 0.2489\n",
      "val Loss: 3.5479 LRAP: 0.0064 NDCG: 0.2251\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 3.3082 LRAP: 0.0302 NDCG: 0.2466\n",
      "val Loss: 3.4709 LRAP: 0.0089 NDCG: 0.2735\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 3.2494 LRAP: 0.0384 NDCG: 0.2481\n",
      "val Loss: 3.5774 LRAP: 0.1233 NDCG: 0.2263\n",
      "EarlyStopping counter: 4 out of 50\n",
      "\n",
      "Training complete in 0m 14s\n",
      "Best val loss: 3.377636, best LRAP score: 0.5382\n",
      "Saved training histories to evaluation_run_dump.pkl\n"
     ]
    }
   ],
   "source": [
    "# model training \n",
    "\n",
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "\n",
    "model =  prep_infused_sweetnet(\n",
    "            initialization_method = 'random', # random or external\n",
    "            num_classes = classes,\n",
    "            embeddings_dict = glm_embeddings, \n",
    "            trainable_embeddings = True, # True or False\n",
    "            ) \n",
    "\n",
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, learning_rate, num_classes = classes)\n",
    "\n",
    "model_ft, current_run_metrics = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 10, mode = 'multilabel', return_metrics = True)\n",
    "\n",
    "run_identifier = f\"{config_description}_{trial_seed}\"\n",
    "all_training_histories[run_identifier] = current_run_metrics\n",
    "\n",
    "saved_run_data_path = (f\"{saved_run_data}.pkl\")\n",
    "\n",
    "# Save the entire collection at the end (or periodically)\n",
    "with open(saved_run_data_path, 'wb') as f:\n",
    "    pickle.dump(all_training_histories, f)\n",
    "print(f\"Saved training histories to {saved_run_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cfde29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_training_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46656f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trial data\n",
    "\n",
    "pickle_file_path = 'evaluation_run_dump.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading data from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            user_data_string_from_input = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Data loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_data_string_from_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673f2aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print some keys to check in the Sanity Cheker\n",
    "print(list(loaded_embeddings.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d023d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Embedding Sanity Checker ---\n",
    "\n",
    "# 1. Choose a token to check \n",
    "token_to_check = '!GlcNAc' \n",
    "\n",
    "\n",
    "from glycowork.glycan_data.loader import  lib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "glycowork_lib = lib\n",
    "\n",
    "if token_to_check not in glycowork_lib:\n",
    "    print(f\"Error: Token '{token_to_check}' not found in glycowork_lib. Choose another token.\")\n",
    "elif glm_embeddings is None or token_to_check not in glm_embeddings:\n",
    "    print(f\"Error: Token '{token_to_check}' not found in glm_embeddings dictionary or dictionary not loaded.\")\n",
    "else:\n",
    "    print(f\"--- Checking embedding for token: '{token_to_check}' ---\")\n",
    "    \n",
    "    # 3. Get the index and the vector from the dictionary\n",
    "    token_index = glycowork_lib[token_to_check]\n",
    "    vector_from_dict = glm_embeddings[token_to_check]\n",
    "    print(f\"Index for '{token_to_check}': {token_index}\")\n",
    "    print(f\"Vector from glm_embeddings dict (first 5 elements): {vector_from_dict[:5]}\")\n",
    "\n",
    "    # 4. Prepare a model instance using the 'external' method\n",
    "    print(\"\\nPreparing a temporary model instance with external embeddings...\")\n",
    "    try:\n",
    "        # Use parameters relevant for checking the embedding layer\n",
    "        temp_model = prep_infused_sweetnet(\n",
    "            num_classes=len(labels[0]), # Needs a valid class number\n",
    "            initialization_method='external',\n",
    "            embeddings_dict=glm_embeddings,\n",
    "            trainable_embeddings=False, # Trainable doesn't matter for checking initial state\n",
    "            hidden_dim=vector_from_dict.shape[0], # Ensure hidden_dim matches embedding dim\n",
    "            libr=glycowork_lib\n",
    "        )\n",
    "        \n",
    "\n",
    "        # 5. Get the vector from the model's embedding layer\n",
    "        with torch.no_grad(): # No need for gradients here\n",
    "            model_embedding_layer = temp_model.item_embedding\n",
    "            # Ensure index is valid for the layer\n",
    "            if token_index < model_embedding_layer.weight.shape[0]:\n",
    "                vector_from_model = model_embedding_layer.weight[token_index].cpu().numpy()\n",
    "                print(f\"Vector from model's layer (index {token_index}, first 5 elements): {vector_from_model[:5]}\")\n",
    "\n",
    "                # 6. Compare the vectors\n",
    "                if np.allclose(vector_from_dict, vector_from_model, atol=1e-6): # Use allclose for float comparison\n",
    "                    print(f\"\\nSUCCESS: Vectors for '{token_to_check}' match between dictionary and model layer.\")\n",
    "                else:\n",
    "                    print(f\"\\nFAILURE: Vectors for '{token_to_check}' DO NOT match.\")\n",
    "                    # Optional: print more elements or the difference\n",
    "                    # print(f\"Difference (sum of absolute diff): {np.sum(np.abs(vector_from_dict - vector_from_model))}\")\n",
    "            else:\n",
    "                print(f\"Error: Index {token_index} is out of bounds for the model's embedding layer (size {model_embedding_layer.weight.shape[0]})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during model preparation or vector comparison: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28f0882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SweetNet model instantiated with lib_size=2565, num_classes=96, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: True.\n"
     ]
    }
   ],
   "source": [
    "classes = len(labels[0]) # number of classes in the dataset\n",
    "\n",
    "model =  prep_infused_sweetnet(\n",
    "            initialization_method = 'random', # random or external\n",
    "            num_classes = classes,\n",
    "            embeddings_dict = glm_embeddings, \n",
    "            trainable_embeddings = True, # True or False\n",
    "            ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99f2cc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4525,  0.3075,  1.4349,  0.3794, -1.7839,  0.3885,  1.2002, -2.1629,\n",
      "         0.5717, -0.6538,  1.5382,  0.6430, -0.1368,  0.7357, -0.9986,  1.1028,\n",
      "         1.1380,  1.6883, -0.4352, -0.2213,  0.3439, -1.3767,  0.3486,  0.4144,\n",
      "         1.0753,  1.1903,  0.6938,  2.1160, -1.0378,  1.6642,  0.7488, -0.6732,\n",
      "         0.8369, -0.4839, -0.0633,  0.1429, -0.1653, -0.5991,  0.4693, -1.3164,\n",
      "        -1.2417,  0.5104, -0.0872, -0.3140, -1.1179,  0.6367,  1.5762, -0.5266,\n",
      "        -0.0107, -0.1291, -1.1057, -1.3789, -1.7769, -0.0630, -1.4919,  0.3969,\n",
      "         0.5373, -0.9781, -0.0049,  0.6047,  0.3725, -0.1161, -0.7335,  1.4174,\n",
      "         1.1244,  0.2782,  0.3702, -0.5335,  0.7336, -0.7596,  2.1775,  1.5075,\n",
      "         0.0878,  0.1304,  1.0939, -0.9240, -0.3225,  2.2266, -0.0524, -0.3247,\n",
      "         0.9508,  1.1304,  0.8215, -0.5020, -0.2016,  0.5250,  0.4758,  0.3991,\n",
      "        -0.7320,  0.6299,  1.4124,  1.0406, -1.6504,  0.3421, -1.1384, -0.5006,\n",
      "        -2.8388, -0.1219,  0.4013, -1.3960, -1.1451,  1.3919,  0.4108,  1.4154,\n",
      "         0.0233, -0.4347,  0.7090, -0.5979,  3.8240, -2.6722,  1.1312, -0.5098,\n",
      "        -0.4212, -0.9150,  2.5155,  2.6606,  1.5248, -0.4136,  0.8950,  0.3788,\n",
      "         0.1548, -0.2871,  2.2560,  1.8228, -0.4042,  0.8137,  0.8122, -0.4495,\n",
      "         0.8944,  1.2428, -0.0702,  2.0030, -0.6359, -2.6067, -1.1664,  0.2808,\n",
      "        -0.5870,  0.1071, -0.5199,  0.2564,  1.8624,  0.1962, -0.8389,  1.3069,\n",
      "        -0.1878, -0.6467, -1.2363,  0.4205, -0.0831,  0.2980,  0.7495,  0.7285,\n",
      "         0.8626,  0.7969,  0.1800, -0.5361, -0.4910, -0.2664, -0.6760,  0.6206,\n",
      "         0.7971,  0.7876,  0.9703,  0.8703,  0.1281,  0.4932,  0.1433,  0.8023,\n",
      "        -0.9340,  0.6567,  0.8514, -0.2217,  0.5267,  1.3327, -1.3049,  0.1016,\n",
      "         0.2679, -0.3604,  0.9395, -0.2832,  0.9303,  2.2608,  0.3104, -0.2399,\n",
      "         0.3305, -0.3856, -0.7950,  1.0664,  0.2928,  0.3054,  0.7743, -0.4132,\n",
      "        -1.2473,  0.5226, -0.1605,  0.6767,  3.2382,  1.4164,  0.1497, -0.1524,\n",
      "        -1.1132, -0.5140, -0.9554,  0.6446,  0.1907, -1.3350,  0.8744, -0.5697,\n",
      "         0.4462, -0.2393, -0.2544,  0.5518,  0.3637,  2.2104,  0.7299,  0.5414,\n",
      "         0.0431,  0.8958, -1.0412,  1.5573,  0.9538, -1.5379, -0.8342,  1.7338,\n",
      "        -0.9361,  0.4433,  0.2628, -0.5208,  0.0060,  0.1234,  0.2797, -0.8186,\n",
      "         1.1324, -0.0724, -1.2036,  1.3020, -0.3627,  1.9268,  0.1734,  0.1979,\n",
      "        -0.7173,  1.2514, -0.7069, -0.0505, -0.1955,  0.6666,  1.6162, -1.2619,\n",
      "         1.7805, -0.0448,  1.2681, -0.2675, -1.0268, -0.7161, -1.0541, -2.2473,\n",
      "         0.7606,  1.2682, -0.3083, -0.0098, -0.1641,  2.4837,  0.8422, -0.2010,\n",
      "        -0.0694, -0.1971, -0.1761, -0.6628, -0.6164,  0.4215,  0.2450, -0.0676,\n",
      "        -0.2844, -0.8142, -0.5471,  1.3026,  0.7163, -1.1403,  1.8915, -1.0158,\n",
      "         0.6698,  0.1817,  2.0648, -0.4221, -0.3673, -0.3215,  0.7389, -1.1230,\n",
      "         1.0882, -0.1148, -0.0736, -1.1344,  0.2725, -0.7484,  0.6577,  0.2593,\n",
      "        -1.3638, -0.5708,  0.4072, -1.7620, -1.0206,  0.6619,  0.1841,  0.7009,\n",
      "        -0.5390, -0.8679,  0.6309,  0.2506,  0.6462,  0.5402,  1.6812,  0.6443,\n",
      "         0.4791,  0.8471,  0.8281, -1.9903,  0.9472, -0.7136,  0.0571,  0.2029],\n",
      "       device='cuda:0')\n",
      "tensor([ 4.3067e-01,  2.2513e+00, -1.4887e+00,  2.5979e-01,  2.7102e-01,\n",
      "        -7.5215e-01,  9.0644e-01,  1.4575e+00,  7.0780e-01,  1.6731e+00,\n",
      "        -1.0249e+00,  6.7283e-01, -9.5648e-01, -4.4058e-01,  3.2475e-01,\n",
      "        -1.8118e+00, -7.4032e-02, -3.2155e-01, -1.6493e-01,  1.6971e+00,\n",
      "        -3.6711e-02,  8.2921e-01,  2.0283e-01,  2.7039e-01, -1.0019e+00,\n",
      "        -1.4912e-01, -8.9476e-01, -1.5330e+00,  4.1843e-01,  2.0505e+00,\n",
      "        -4.2662e-01,  4.0897e-01, -2.3802e-01, -1.4560e+00,  3.7806e-01,\n",
      "         2.0069e-01, -5.6078e-01, -1.7825e+00,  1.4205e+00,  6.3064e-01,\n",
      "         8.9669e-02,  7.7216e-01, -1.6918e-01,  1.3843e+00,  9.4988e-01,\n",
      "         1.5825e+00, -1.2001e+00, -9.9204e-01, -3.1202e-01, -1.9944e+00,\n",
      "         5.5316e-01,  3.9078e-01,  1.1686e+00,  4.1386e-01, -3.1412e-01,\n",
      "         5.5439e-01,  7.8325e-01,  2.0930e-01, -5.3138e-01, -1.6670e+00,\n",
      "         7.4871e-01,  5.2782e-01, -3.8134e-01, -1.4820e+00,  4.3735e-01,\n",
      "         4.1929e-01,  1.3008e+00, -6.4442e-02,  1.2763e+00, -4.8517e-01,\n",
      "         6.9938e-01,  3.0112e-01, -5.1539e-01,  1.0683e+00,  1.1785e+00,\n",
      "         8.9514e-01,  3.9300e-01,  6.9941e-01,  1.3292e-02,  4.9254e-01,\n",
      "         1.8079e-01, -6.5895e-02,  7.9642e-03, -1.8501e-01,  1.0669e+00,\n",
      "         6.7442e-01,  5.6909e-01, -3.3308e-01,  2.5513e-01, -9.9793e-01,\n",
      "        -2.1189e-01, -7.6123e-01,  3.1129e-01,  6.1587e-01, -6.9883e-01,\n",
      "        -4.0668e-01, -9.9232e-01, -6.7022e-01, -7.3829e-02,  4.2736e-01,\n",
      "        -6.3419e-02, -1.7484e+00,  9.2603e-01, -3.0997e-01,  1.0917e+00,\n",
      "        -4.8708e-01, -1.5000e+00,  2.0633e-01, -2.3586e-01, -6.4405e-01,\n",
      "         1.1584e+00, -1.9793e-01, -1.3119e-01,  2.2585e-02, -2.2866e-01,\n",
      "        -1.0805e-01,  1.1481e-01,  9.2016e-01,  8.9922e-02, -4.8520e-01,\n",
      "        -6.8233e-01, -6.7338e-01, -4.0499e-01,  3.7944e-01, -1.0288e+00,\n",
      "         1.0760e+00, -8.6597e-02, -7.6980e-01, -8.9696e-01,  2.9535e-01,\n",
      "        -2.1401e-01, -1.9137e+00,  2.3577e+00,  2.9162e-01,  1.6611e-01,\n",
      "        -8.8061e-01, -1.6128e+00,  7.7834e-01,  1.1716e-01, -7.0405e-01,\n",
      "         1.8852e+00, -6.6511e-01,  1.3512e+00,  2.4343e+00,  9.5242e-01,\n",
      "        -6.9822e-02,  2.6791e-01, -1.3211e+00, -3.0461e-01, -3.4359e-01,\n",
      "         8.9842e-02, -8.4796e-01, -3.7399e-01, -1.1078e-01, -2.3103e+00,\n",
      "        -1.1232e+00,  1.1117e-02, -1.0120e+00,  1.3800e+00, -4.0779e-01,\n",
      "         3.8198e-01, -1.1425e+00,  8.0206e-01, -3.2974e-01, -2.6352e+00,\n",
      "        -8.5553e-01,  1.6558e+00, -4.6789e-01,  9.9914e-01,  2.3302e+00,\n",
      "        -2.2144e-01,  9.5085e-02, -2.8442e-01, -2.0370e+00, -9.8999e-01,\n",
      "         1.5580e+00,  4.1583e-01,  1.8078e+00, -1.4631e+00,  2.3361e-01,\n",
      "        -3.7950e-01, -1.7371e+00,  2.4898e+00,  6.3356e-01,  1.3954e+00,\n",
      "         3.2258e-01,  5.2636e-01, -4.5698e-01,  1.0449e-01, -5.4529e-01,\n",
      "        -1.0304e+00, -2.0693e-01,  4.1962e-01, -7.0470e-01,  5.7315e-02,\n",
      "         1.0535e-01, -8.0128e-03, -1.8843e+00, -1.2119e+00, -8.0853e-02,\n",
      "         5.3779e-01, -7.1864e-01,  1.8876e-01, -1.2683e+00, -1.4981e-01,\n",
      "        -1.0413e+00, -2.5853e+00,  9.1319e-02,  5.7964e-01,  8.0785e-02,\n",
      "        -9.5665e-01,  6.6270e-01, -7.0317e-01, -1.4608e+00, -1.7841e-01,\n",
      "         2.4391e+00,  7.2028e-01,  2.0032e-01,  1.8409e+00,  3.7892e-01,\n",
      "         1.8720e-01, -1.0996e+00, -7.4340e-01,  5.4543e-01,  7.7672e-01,\n",
      "        -5.1743e-01, -6.6914e-01,  1.9883e+00, -7.7308e-01,  4.3847e-01,\n",
      "        -1.0213e+00, -3.0473e-01, -4.4598e-01, -7.1403e-01,  9.7865e-01,\n",
      "         1.2295e+00,  1.3597e+00,  9.4784e-02,  2.8482e-01,  2.6380e-02,\n",
      "         1.3562e-01,  8.1182e-01,  3.4400e-01,  5.6164e-01,  1.3678e-02,\n",
      "        -9.5013e-01, -2.9591e-01,  1.7373e+00, -9.7451e-01,  6.6790e-01,\n",
      "        -1.0191e+00, -5.8826e-01, -1.2075e-01,  9.6539e-01,  1.2250e-01,\n",
      "         1.6653e+00, -2.1806e+00,  9.0850e-03,  3.7744e-01, -5.5095e-01,\n",
      "        -1.1926e+00,  1.7110e+00,  8.3782e-01,  9.8534e-01, -9.0720e-01,\n",
      "         7.3452e-01, -1.3603e+00,  5.7797e-01,  7.5823e-01, -5.2557e-01,\n",
      "        -5.0901e-01,  7.9090e-02,  2.4368e+00,  2.0060e-01,  1.4558e+00,\n",
      "         6.5029e-01, -2.0148e-01,  1.2318e+00,  2.1385e-01,  1.5260e+00,\n",
      "         8.1976e-01,  8.1907e-01,  6.7588e-01,  8.8499e-02, -1.3891e+00,\n",
      "        -1.2125e-01,  7.7699e-01, -7.6691e-02, -7.0504e-01, -8.9046e-01,\n",
      "         1.1725e+00,  1.1258e-01, -1.3337e-01,  6.8609e-01,  5.8030e-01,\n",
      "        -7.7801e-01, -7.7244e-01,  1.0477e+00,  1.8128e+00,  1.4519e+00,\n",
      "         5.9400e-04, -4.2591e-01,  2.5755e-01,  7.5339e-01,  7.4083e-01,\n",
      "         1.1799e-01,  7.7211e-01, -2.1287e+00, -7.0461e-01, -1.1493e+00,\n",
      "        -1.0364e-01, -2.8180e-01,  1.0134e-01, -1.6046e+00,  7.7966e-01,\n",
      "        -1.1719e+00, -6.5783e-02, -1.1140e+00,  4.6381e-02,  1.1528e+00],\n",
      "       device='cuda:0')\n",
      "tensor([ 2.8210,  0.2433,  0.1473,  0.3327, -0.9763,  0.6587,  0.5699,  0.4836,\n",
      "        -0.8395, -0.8251,  0.8842,  2.2154,  0.4694, -0.6740,  0.6077, -1.4291,\n",
      "        -0.4855,  0.8531,  1.5994, -0.0211, -0.8484,  1.8470, -1.4403,  1.2241,\n",
      "        -0.0244, -0.2589,  1.5156,  0.1733,  1.2133, -0.6884,  0.2810,  0.3250,\n",
      "        -2.0788,  0.1335, -0.5235,  0.2171, -0.5421, -1.4286, -1.2137,  0.7118,\n",
      "        -1.3002,  0.0515, -0.1174, -1.8367,  1.4707, -0.8113,  0.1475,  1.2713,\n",
      "        -0.1767, -0.4941, -0.9242,  1.2122, -0.0989, -0.1896,  0.0897, -0.2729,\n",
      "         0.0103, -0.7587,  1.0701, -0.4028,  1.1471,  0.7709,  0.1971, -1.6458,\n",
      "         0.7066,  0.1512, -0.1328, -0.3567,  0.0258, -0.1803,  0.9263,  0.7649,\n",
      "        -0.6744,  1.1304,  0.0741,  0.4259,  0.2916, -0.2658,  0.7953, -1.0056,\n",
      "         0.1019,  0.7450, -0.1386, -0.6352, -2.0024, -1.2005,  1.1584,  0.2002,\n",
      "        -0.0081, -2.1766,  0.7677, -0.4029,  0.3711, -0.4853, -2.1389,  1.1729,\n",
      "        -0.1099, -1.0122,  0.3024,  0.9052,  0.8468, -0.5104,  0.0717, -0.1335,\n",
      "        -1.1232, -1.0923, -1.0310,  0.5803,  1.6580,  0.5498,  0.9334,  1.1312,\n",
      "        -1.0205,  0.3981,  2.1884, -0.1285, -0.1999, -1.2686,  0.8188, -1.0752,\n",
      "         0.2979, -0.0652,  0.6505, -1.8671,  0.3160,  0.6258, -0.4511,  0.7692,\n",
      "        -1.4881, -1.1747, -1.0401,  0.4291, -0.0263, -1.6829, -0.3325,  1.7792,\n",
      "         0.4412, -0.7343, -1.0095,  2.5671, -0.4641,  0.5323, -0.9470,  0.6668,\n",
      "         0.8217,  0.3476,  0.2841, -0.3108, -2.0139,  0.5738, -0.1833,  1.9927,\n",
      "        -0.5187,  1.3250, -1.5138, -0.5410, -0.5484,  1.1095, -1.0798,  0.7902,\n",
      "        -2.2756, -1.0345, -0.5942, -1.2664,  1.6655,  0.0563,  0.1131, -0.0576,\n",
      "         0.3313,  2.0752,  0.8285, -0.9631, -1.1957,  0.2977, -1.5486, -1.0024,\n",
      "         1.6100, -1.3926, -0.2204,  0.8846, -0.0677, -1.0289,  0.4455, -0.7309,\n",
      "         1.1402, -0.4748,  1.0230,  0.2603,  0.2919,  0.6174, -0.3754, -1.8117,\n",
      "         0.9458,  0.1167,  0.5265,  0.1704,  0.7575, -1.2996,  0.0760, -0.2232,\n",
      "         0.1032,  0.5765, -0.5624, -0.2078, -0.6003,  0.0860,  1.3911, -1.1757,\n",
      "         2.4456,  1.6239,  0.4167, -1.3724, -0.3457, -0.1840, -0.3221,  0.3718,\n",
      "         1.6844, -0.8696, -1.8900,  0.7608, -0.1869,  0.7233, -1.7153,  0.7742,\n",
      "        -0.7257,  0.1451,  2.0536, -0.1977, -0.6691,  0.3764,  0.8081, -0.7765,\n",
      "         0.1595,  1.0833,  0.2374, -0.0757, -0.4088,  1.5567, -0.8547,  0.2674,\n",
      "        -0.8449,  1.7476, -0.2433,  0.7932, -0.8176, -0.1734, -0.0757,  0.8243,\n",
      "         0.7897,  0.4336, -0.7877,  1.2112,  1.0954,  0.9047, -1.4371,  0.8833,\n",
      "        -0.1004, -0.8546, -0.9456,  0.9183, -1.7201, -1.1994,  0.1833,  1.1558,\n",
      "        -0.2987, -0.8974,  2.1222, -1.6619,  0.6439,  0.3032,  0.4041, -0.4664,\n",
      "        -0.7153,  1.5181, -1.2714, -2.5344, -0.8082,  1.1492,  2.8159,  0.0895,\n",
      "        -0.0882,  0.5989,  0.6827, -0.7469,  1.3963,  0.0660, -0.1566,  1.3609,\n",
      "         0.2723, -1.2580,  0.7399, -0.4168, -0.3468,  1.0588, -1.0077,  1.9888,\n",
      "         0.6147, -0.2547, -0.7069,  0.2738, -0.1352, -0.4906,  0.3062, -0.0041,\n",
      "         0.2430, -0.8347,  0.3221, -0.2774, -0.6294,  0.0141, -0.1570,  0.5738,\n",
      "        -0.3783, -0.0099, -0.5010,  0.2407,  1.3902, -0.4447,  0.3958,  0.7738],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.item_embedding.weight.data[3])\n",
    "print(model.item_embedding.weight.data[10])\n",
    "print(model.item_embedding.weight.data[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e2e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 4.4550 LRAP: 0.0000 NDCG: 0.1664\n",
      "val Loss: 4.2388 LRAP: 0.0000 NDCG: 0.1875\n",
      "Validation loss decreased (0.000000 --> 4.238810).  Saving model ...\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 3.9701 LRAP: 0.0005 NDCG: 0.1916\n",
      "val Loss: 3.9027 LRAP: 0.0089 NDCG: 0.2076\n",
      "Validation loss decreased (4.238810 --> 3.902740).  Saving model ...\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 3.8381 LRAP: 0.0033 NDCG: 0.2049\n",
      "val Loss: 3.7819 LRAP: 0.0089 NDCG: 0.2345\n",
      "Validation loss decreased (3.902740 --> 3.781872).  Saving model ...\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 3.7427 LRAP: 0.0054 NDCG: 0.2100\n",
      "val Loss: 3.6963 LRAP: 0.0089 NDCG: 0.2336\n",
      "Validation loss decreased (3.781872 --> 3.696251).  Saving model ...\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 3.6751 LRAP: 0.0063 NDCG: 0.2203\n",
      "val Loss: 3.8380 LRAP: 0.0292 NDCG: 0.2333\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 3.6109 LRAP: 0.0084 NDCG: 0.2132\n",
      "val Loss: 3.9702 LRAP: 0.0038 NDCG: 0.2085\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 3.5668 LRAP: 0.0204 NDCG: 0.2144\n",
      "val Loss: 3.8991 LRAP: 0.0140 NDCG: 0.2220\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "optimizer_ft, scheduler, criterion = model_training.training_setup(model, learning_rate, num_classes = classes)\n",
    "\n",
    "model_ft, current_run_metrics = model_training.train_model(model, dataloaders, criterion, optimizer_ft, scheduler,\n",
    "                   num_epochs = 10, mode = 'multilabel', return_metrics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "292a7007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.2740e-01, -2.0471e+00, -1.4139e+00, -1.0333e+00, -1.1648e+00,\n",
      "        -1.2794e+00,  2.2036e+00, -1.0167e+00, -2.5458e+00,  5.7299e-01,\n",
      "         7.6437e-01,  3.6878e-01, -1.6637e+00,  1.8681e+00, -2.1661e-01,\n",
      "         3.9253e-01, -4.1259e-02,  7.3389e-01,  3.6420e-01, -5.1863e-01,\n",
      "        -3.9298e-01, -2.4760e-01,  8.1618e-01, -7.1385e-01, -3.0701e-02,\n",
      "         1.7010e+00, -1.3852e-01,  6.1011e-01,  1.7432e+00,  6.6272e-01,\n",
      "         2.5717e-01, -4.0061e-01,  1.3637e+00, -1.7826e+00, -7.0668e-01,\n",
      "         1.4412e+00, -5.5786e-01, -3.9710e-01, -7.9893e-01,  1.2903e+00,\n",
      "        -5.6667e-02,  7.5806e-01, -2.0977e+00,  1.4146e+00,  1.6569e-01,\n",
      "        -3.8940e-02, -3.0771e-01,  4.4825e-01,  4.0977e-01,  7.6066e-01,\n",
      "        -1.0494e+00,  1.8453e+00, -7.0803e-01, -4.2688e-01,  9.6372e-01,\n",
      "         1.5386e+00,  7.5607e-01, -1.3538e-01, -1.2501e+00, -5.6516e-01,\n",
      "         9.0650e-01,  1.5950e+00, -3.3288e-01,  8.4733e-01,  7.3296e-01,\n",
      "         2.6609e-01,  1.1626e-01, -3.9144e-01,  1.3806e-01,  1.2996e+00,\n",
      "        -5.5460e-01, -1.0251e+00,  2.6641e-01, -4.4459e-01, -1.2623e+00,\n",
      "        -3.3688e-02, -3.6208e-01, -2.3200e-01,  1.1496e+00, -1.2805e-01,\n",
      "        -1.3968e+00, -5.5236e-01,  1.5814e+00,  1.3721e+00,  2.5195e-02,\n",
      "         1.5529e+00, -4.7751e-01, -6.4176e-01,  6.6467e-01, -1.3065e+00,\n",
      "        -3.4586e-01,  1.6062e+00,  1.7099e+00,  2.3191e+00, -5.1207e-01,\n",
      "         3.6242e-01, -4.2641e-01, -1.0113e-01,  3.9715e-01, -3.4812e-01,\n",
      "         6.1831e-01, -1.3221e+00, -3.9018e-03, -3.0425e-01, -3.6250e-02,\n",
      "         1.0016e+00, -1.6201e+00, -2.2495e-02, -1.5176e-01, -2.1074e-01,\n",
      "         3.2721e-01, -9.9902e-01,  4.5024e-01,  9.4832e-01,  3.7608e-01,\n",
      "        -3.6034e-01,  1.0678e+00, -9.9506e-01, -1.7890e-01,  6.5615e-01,\n",
      "         5.7530e-01,  7.7533e-01, -7.8813e-01, -1.7131e-01, -1.0796e+00,\n",
      "        -1.7184e+00, -3.2897e-01, -7.2116e-01,  2.3716e+00,  1.4294e+00,\n",
      "         2.7530e+00, -2.0264e+00, -1.3169e-01, -7.5397e-01,  1.4049e-01,\n",
      "         2.6834e-01, -3.4506e-01,  4.0185e-01, -3.8472e-01,  3.9414e-01,\n",
      "         1.7872e+00, -2.0002e+00, -1.7726e+00, -3.1370e-01, -8.1113e-01,\n",
      "        -2.0290e+00, -3.9607e-01,  3.6881e-01,  1.5271e+00,  2.1837e+00,\n",
      "        -1.3864e+00,  9.0972e-01, -1.3341e+00, -1.2204e+00,  8.4522e-01,\n",
      "         1.1820e+00, -3.6873e-01, -1.9152e+00, -7.8925e-01,  5.7629e-01,\n",
      "        -4.7772e-01,  1.1418e+00,  1.8196e-01,  3.1709e-01, -1.0977e-01,\n",
      "         1.5754e+00,  2.4098e-01, -5.1377e-02, -2.5829e-01,  6.3542e-01,\n",
      "        -1.6365e-01,  1.6750e+00, -2.8896e-01,  4.0356e-01,  7.1955e-01,\n",
      "         1.2200e-01,  8.8086e-02,  1.3596e+00, -9.9422e-01,  2.3744e-02,\n",
      "         1.2942e+00,  7.0973e-01,  9.0789e-01, -2.7489e-01,  4.6101e-01,\n",
      "        -1.2105e+00, -1.3346e+00, -1.0218e+00,  1.6908e+00, -1.1927e+00,\n",
      "         1.3170e-03,  4.9147e-01, -1.0905e+00,  1.5827e+00,  5.5272e-01,\n",
      "        -3.7180e-01,  3.5002e-02,  1.8930e-01,  6.4103e-01,  2.2539e-01,\n",
      "        -4.7387e-01,  1.4436e+00, -5.2984e-01, -5.3985e-01,  8.9149e-01,\n",
      "         9.1340e-01, -9.7955e-01, -1.2369e+00, -6.8266e-01, -1.1885e+00,\n",
      "        -7.2535e-02,  1.5606e+00, -6.9199e-01, -1.7285e+00, -5.9507e-01,\n",
      "        -1.2036e+00,  8.9449e-01,  2.4245e+00, -1.8650e+00, -8.1202e-01,\n",
      "        -9.7756e-01, -1.2288e+00,  2.2586e+00,  6.0682e-01, -5.7554e-01,\n",
      "         9.4571e-01,  2.8280e-01, -1.7276e-01, -2.5804e-01, -1.3384e+00,\n",
      "         9.4125e-01, -1.8132e+00, -2.7872e-01, -1.2732e+00, -1.2578e-01,\n",
      "        -5.2944e-01,  8.6928e-01,  5.9201e-02, -1.0565e-01,  1.0964e+00,\n",
      "        -3.5174e-01,  2.1765e+00, -9.5382e-01, -1.6464e+00,  9.8570e-01,\n",
      "        -1.1127e+00, -5.1760e-01,  3.0293e-01,  1.0853e+00,  1.9629e+00,\n",
      "        -1.3777e+00,  3.1404e-01, -7.3786e-02,  6.1139e-01, -7.7270e-01,\n",
      "        -3.6754e-01, -1.6076e+00, -1.4868e+00, -1.5044e+00, -6.2243e-01,\n",
      "        -1.9005e+00,  5.8543e-01,  5.1201e-01, -6.4658e-01,  8.2711e-01,\n",
      "        -1.2368e+00,  1.5820e+00,  3.9966e-01,  3.7249e-01,  2.7418e-01,\n",
      "         2.0490e-01, -1.0490e+00,  1.7806e+00,  1.2037e-02,  4.8277e-01,\n",
      "         1.3417e+00,  9.8613e-01, -5.0611e-02,  2.5455e-01, -5.1029e-01,\n",
      "         9.0054e-02,  4.1564e-01,  8.3001e-01,  4.6994e-01,  2.5846e-01,\n",
      "        -1.2480e-01, -1.1421e-01, -1.4745e+00, -1.7899e-02, -3.4230e-01,\n",
      "        -7.7064e-01, -3.5745e-01,  1.5583e-02, -1.6447e+00,  1.0199e+00,\n",
      "        -7.6786e-01, -7.8675e-01, -1.5775e+00, -7.1616e-01, -5.7792e-01,\n",
      "         1.2162e+00,  2.7110e-01,  1.4882e-01, -5.1490e-01, -7.0103e-01,\n",
      "         5.2760e-01,  1.4558e-02, -2.0848e+00, -1.1480e-01, -4.5876e-01,\n",
      "         9.4254e-01,  6.1139e-01,  1.1890e+00,  1.2465e-02,  2.9459e-01,\n",
      "         1.2022e+00, -6.7502e-01, -6.0280e-01,  1.5261e+00, -1.1302e+00],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model_ft.item_embedding.weight.data[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweetnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
