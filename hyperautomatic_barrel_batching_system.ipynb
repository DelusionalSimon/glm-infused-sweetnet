{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b80efd",
   "metadata": {},
   "source": [
    "# Hyperautomatic Barrel Batching System (HBBS)\n",
    "\n",
    "This notebook automates the process of running multiple user defined experiments with different datsets and settings sequentially. Each experiment runs a user defined number of trials for different user defined model configurations (baseline, infused-fixed, infused-trainable) to evaluate and compare their performance systematically. It collects validation metrics for each run, aggregates them, and saves the results.\n",
    "\n",
    "If you just want to run one experiment, try the automated_barrel_batching_system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d390a",
   "metadata": {},
   "source": [
    "### ||RUN ON RESTART||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ba33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "\n",
    "from utils import build_multilabel_dataset, multilabel_split, prep_infused_sweetnet, seed_everything\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import yaml\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "from glycowork.ml.processing import split_data_to_train, dataset_to_dataloader\n",
    "from glycowork.ml import model_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab54e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            glm_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433df162",
   "metadata": {},
   "source": [
    "## Experimental setup\n",
    "Change parameters here to define each Experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb39f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- immutable parameters ---\n",
    "BASE_RANDOM_STATE = 42  # Initial seed for reproducibility of the entire sequence of experiments\n",
    "DATA_DIR = \"Hyperoptimization\"   # Directory where the datasets are stored\n",
    "os.makedirs(DATA_DIR, exist_ok=True) # Create the directory if it doesn't exist\n",
    "\n",
    "# Mutable parameters that can be changed for each experiment below\n",
    "settings = {\n",
    "    \"glycan_dataset\":    'df_disease', # The glycowork dataset to use\n",
    "    \"glycan_class\":      'disease_association', # The class to predict from the chosen dataset\n",
    "    \"num_runs\":          2,  # Number of trials per configuration (e.g., 5 or 10)\n",
    "    \"epochs\":            2, # Number of training epochs per run\n",
    "    \"batch_size\":        128, # 32 or 128 seems to work well\n",
    "    \"train_size\":        0.7, # Fraction of data to use for training (0.7 = 70% train, 15% val, 15% test)\n",
    "    \"learning_rate\":     0.005, # Learning rate for the optimizer\n",
    "    \"drop_last\":         False, # Whether to drop the last batch if it's smaller than the batch size\n",
    "    \"augment_prob\":      0.0,  # Adjust if you want augmentation for training\n",
    "    \"generaliz_prob\":    0.2,  # Adjust if you want generalization for training\n",
    "    \"patience\":          25, # number of epochs without improvement before EarlyStop kicks in \n",
    "    \"min_class_size\":     2, # Minimum number of samples in a class to be included in the dataset\n",
    "}\n",
    "# ---- Datasets and num_classes within them ------\n",
    "#  'df_species': 'Species', 'Genus', 'Family', 'Order', 'Class', 'Phylum', 'Kingdom', 'Domain', 'ref'\n",
    "#  'df_tissue': 'tissue_sample', 'tissue_species', 'tissue_id', 'tissue_ref'\n",
    "#  'df_disease': 'disease_association', 'disease_sample', 'disease_direction', 'disease_species', 'disease_id', 'disease_ref'\n",
    "\n",
    "# --- Define Experiments ---\n",
    "# This list defines all sets of parameters you want to change for each experiment, they override the default parameters\n",
    "# Can be used to test different datasets and num_classes\n",
    "# Or for crude hyperparameter tuning\n",
    "experiments = [\n",
    "    # --- learning_rate variations ---\n",
    "    # Disease dataset\n",
    "    { \"name\": \"disease_lr_0005\", \"glycan_dataset\": \"df_disease\", \"glycan_class\": \"disease_association\", \"learning_rate\": 0.0005},\n",
    "    { \"name\": \"disease_lr_001\", \"glycan_dataset\": \"df_disease\", \"glycan_class\": \"disease_association\", \"learning_rate\": 0.001},\n",
    "    { \"name\": \"disease_lr_01\", \"glycan_dataset\": \"df_disease\", \"glycan_class\": \"disease_association\", \"learning_rate\": 0.01},\n",
    "    { \"name\": \"disease_lr_05\", \"glycan_dataset\": \"df_disease\", \"glycan_class\": \"disease_association\", \"learning_rate\": 0.05},\n",
    "    # Kingdom dataset\n",
    "    { \"name\": \"kingdom_lr_0005\", \"glycan_dataset\": \"df_species\", \"glycan_class\": \"Kingdom\", \"learning_rate\": 0.0005},\n",
    "    { \"name\": \"kingdom_lr_001\", \"glycan_dataset\": \"df_species\", \"glycan_class\": \"Kingdom\", \"learning_rate\": 0.001},\n",
    "    { \"name\": \"kingdom_lr_01\", \"glycan_dataset\": \"df_species\", \"glycan_class\": \"Kingdom\", \"learning_rate\": 0.01},\n",
    "    { \"name\": \"kingdom_lr_05\", \"glycan_dataset\": \"df_species\", \"glycan_class\": \"Kingdom\", \"learning_rate\": 0.05},\n",
    "    # Tissue dataset\n",
    "    { \"name\": \"tissue_lr_0005\", \"glycan_dataset\": \"df_tissue\", \"glycan_class\": \"tissue_sample\", \"learning_rate\": 0.0005 },\n",
    "    { \"name\": \"tissue_lr_001\", \"glycan_dataset\": \"df_tissue\", \"glycan_class\": \"tissue_sample\", \"learning_rate\": 0.001 },\n",
    "    { \"name\": \"tissue_lr_01\", \"glycan_dataset\": \"df_tissue\", \"glycan_class\": \"tissue_sample\", \"learning_rate\": 0.01 },\n",
    "    { \"name\": \"tissue_lr_05\", \"glycan_dataset\": \"df_tissue\", \"glycan_class\": \"tissue_sample\", \"learning_rate\": 0.05 },\n",
    "\n",
    "    # --- batch_size variations ---\n",
    "    # Disease dataset\n",
    "    { \"name\": \"disease_bs_32\", \"glycan_dataset\": \"df_disease\", \"glycan_class\": \"disease_association\", \"batch_size\": 32, },\n",
    "    # Kingdom dataset\n",
    "    { \"name\": \"kingdom_bs_32\", \"glycan_dataset\": \"df_species\", \"glycan_class\": \"Kingdom\", \"batch_size\": 32, },\n",
    "    # Tissue dataset\n",
    "    { \"name\": \"tissue_bs_32\", \"glycan_dataset\": \"df_tissue\", \"glycan_class\": \"tissue_sample\", \"batch_size\": 32, },\n",
    "\n",
    "    # --- min_class_size variations ---\n",
    "    # Disease dataset\n",
    "    { \"name\": \"disease_bs_32\", \"glycan_dataset\": \"df_disease\", \"glycan_class\": \"disease_association\", \"min_class_size\": 32, },\n",
    "    # Kingdom dataset\n",
    "    { \"name\": \"kingdom_bs_32\", \"glycan_dataset\": \"df_species\", \"glycan_class\": \"Kingdom\", \"min_class_size\": 32, },\n",
    "    # Tissue dataset\n",
    "    { \"name\": \"tissue_bs_32\", \"glycan_dataset\": \"df_tissue\", \"glycan_class\": \"tissue_sample\", \"min_class_size\": 32, }\n",
    "]\n",
    "\n",
    "# --- Define Experiment Configurations for each run of the experiment ---\n",
    "# This list defines all sets of parameters you want to test for all the runs of each experiment\n",
    "experiment_configs = [\n",
    "    {\n",
    "        \"name\": \"baseline_trainable\",\n",
    "        \"initialization_method\": \"random\", \n",
    "        \"trainable_embeddings\": True\n",
    "    },\n",
    "    #{\n",
    "    #    \"name\": \"baseline_fixed\",\n",
    "    #    \"initialization_method\": \"random\", \n",
    "    #    \"trainable_embeddings\": False\n",
    "    #},\n",
    "    {\n",
    "        \"name\": \"infused_trainable\",\n",
    "        \"initialization_method\": \"external\", \n",
    "        \"trainable_embeddings\": True\n",
    "    }#,\n",
    "    #{\n",
    "    #    \"name\": \"infused_fixed\",\n",
    "    #    \"initialization_method\": \"external\", \n",
    "    #    \"trainable_embeddings\": False\n",
    "    #}  \n",
    "    # add more configurations as needed     \n",
    "]\n",
    "\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed_everything(BASE_RANDOM_STATE)\n",
    "\n",
    "print()\n",
    "print(f\"A batch of {len(experiments)} experiments with {len(experiment_configs)} configurations have been set up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35662d1f",
   "metadata": {},
   "source": [
    "## Run Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperautomation loop ---\n",
    "\n",
    "# silence weird warnings for the sigmoid function\n",
    "numpy.seterr(over='ignore')\n",
    "\n",
    "all_run_summary_results = {} # For storing best metrics from each run\n",
    "all_run_epoch_histories = {} # For storing full epoch-wise histories\n",
    "\n",
    "print(f\"initializing hyperautomation loop for {len(experiments)} experiments\")\n",
    "\n",
    "# Loop through each experiment\n",
    "for experiment in tqdm(experiments,desc=\"Running hyperautomation loop\"):\n",
    "    \n",
    "    # load the settings for this experiment\n",
    "    for setting in settings:\n",
    "        if setting in experiment:\n",
    "            settings[setting] = experiment[setting]\n",
    "       \n",
    "    # get name of the current experiment\n",
    "    experiment_name = experiment[\"name\"]\n",
    "    num_runs = settings['num_runs']\n",
    "\n",
    "    #print(\"----------------\")\n",
    "    #print(f\"Initializing {experiment_name} experiment with {len(experiment_configs)} configurations and {num_runs} runs per configuration.\")\n",
    "    #print()\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    # generate the paths for saving results\n",
    "    results_summary_path =      os.path.join(DATA_DIR,f\"summary_{experiment_name}.csv\")\n",
    "    epoch_data_path =           os.path.join(DATA_DIR,f\"epoch_data_{experiment_name}.pkl\")\n",
    "    test_set_path =             os.path.join(DATA_DIR,f\"test_set_{experiment_name}.pkl\")\n",
    "    saved_models_dir =          os.path.join(DATA_DIR,f\"saved_models_{experiment_name}\")\n",
    "    experiment_settings_path =  os.path.join(DATA_DIR,f\"experiment_settings_{experiment_name}.yaml\")\n",
    "\n",
    "    # generate yaml file to save the settings for this experiment\n",
    "    experiment_log_data = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"global_parameters_for_this_experiment\": settings, # This holds updated settings\n",
    "        \"experiment_specific_configs\": experiment, # The specific entry from the 'experiments' list\n",
    "        \"all_model_configurations_tested\": experiment_configs, # All configs tested within this experiment\n",
    "        \"base_random_state_for_experiment_batch\": BASE_RANDOM_STATE # Log the base seed\n",
    "    }\n",
    "    try:\n",
    "        with open(experiment_settings_path, 'w') as f:\n",
    "            yaml.dump(experiment_log_data, f, default_flow_style=False, sort_keys=False) # sort_keys=False to preserve order\n",
    "\n",
    "        #print(f\"--- Settings for experiment '{experiment_name}' saved to {experiment_settings_path} ---\")\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving settings for experiment '{experiment_name}' to YAML: {e}\")\n",
    "\n",
    "    #print()\n",
    "\n",
    "    # Set the initial random seed for this experiment\n",
    "    seed_everything(BASE_RANDOM_STATE, True)\n",
    "    #print()\n",
    "    \n",
    "    # Load dataset for the experiment\n",
    "    glycans, labels, label_names = build_multilabel_dataset(glycan_dataset = 'df_disease', \n",
    "                                                        glycan_class = 'disease_association', \n",
    "                                                        min_class_size = settings['min_class_size'], \n",
    "                                                        silent= True)\n",
    "\n",
    "    \n",
    "    num_classes = len(labels[0]) # Number of classes in the dataset\n",
    "\n",
    "    # calculate the split ratio for train/val/test\n",
    "    test_split_ratio = 1 - ((1 - settings['train_size'])/2)\n",
    "    val_split_ratio = 1- ((1 - settings['train_size']) / (1 + settings['train_size']))\n",
    "\n",
    "    # split out the test set outside of the loop to stop data leakage\n",
    "    temp_glycans, test_glycans, _, temp_labels, test_labels, _ = multilabel_split(glycans, labels, train_size = test_split_ratio, \n",
    "                                                                random_state = BASE_RANDOM_STATE, no_test = True, silent= True)\n",
    "\n",
    "    # load the test set into a dataloader\n",
    "    test_dataloader = dataset_to_dataloader(glycan_list = test_glycans, \n",
    "                                            labels = test_labels,\n",
    "                                            batch_size = settings['batch_size'],\n",
    "                                            drop_last = settings['drop_last'],\n",
    "                                            augment_prob = settings['augment_prob'], \n",
    "                                            generalization_prob = settings['generaliz_prob']\n",
    "    )\n",
    "    # Save the test set to a pickle file\n",
    "    with open(test_set_path, 'wb') as f:\n",
    "        pickle.dump(test_dataloader, f) \n",
    "    #print(f\"Saved test set to {test_set_path}\")\n",
    "\n",
    "    for i in tqdm(range(num_runs), desc=f\"{experiment_name} runs\"):\n",
    "\n",
    "        # Print the current run number\n",
    "        #print(\"----------------\")\n",
    "        #print(f\"Run {i+1}/{num_runs}\")\n",
    "        \n",
    "\n",
    "        # Increment the random state for each run\n",
    "        random_state = BASE_RANDOM_STATE + i\n",
    "\n",
    "        # Set the random seed for reproducibility for this run\n",
    "        seed_everything(random_state, True)    \n",
    "\n",
    "        # --- split data for this run outside of core loop for efficiency ---\n",
    "        # Split the remainding dataset into training and validation\n",
    "        train_glycans, val_glycans, _, train_labels, val_labels, _ = multilabel_split(temp_glycans, temp_labels, train_size=val_split_ratio, \n",
    "                                                                    random_state=random_state, no_test = True, silent= True)\n",
    "\n",
    "        # Load into dataloders for training and validation\n",
    "        dataloaders = split_data_to_train(\n",
    "            glycan_list_train = train_glycans, glycan_list_val = val_glycans, labels_train = train_labels, labels_val = val_labels,\n",
    "            batch_size = settings['batch_size'],\n",
    "            drop_last = settings['drop_last'],\n",
    "            augment_prob = settings['augment_prob'], \n",
    "            generalization_prob = settings['generaliz_prob']\n",
    "        )\n",
    "        \n",
    "        # --- Loop through each configuration & train model ---\n",
    "        for config in tqdm(experiment_configs, desc=\"Training models for each config\", leave=False):\n",
    "\n",
    "            # Extract the configuration parameters\n",
    "            config_name = config[\"name\"]\n",
    "            initialization_method = config[\"initialization_method\"]\n",
    "            trainable_embeddings = config[\"trainable_embeddings\"]\n",
    "\n",
    "            # Print the current configuration being used\n",
    "            #print(\"----------------\")\n",
    "            #print(f\"Running configuration: {config_name}\")\n",
    "            \n",
    "\n",
    "            # --- Model Training ---\n",
    "            # Initialize the model with the specified parameters\n",
    "            model =  prep_infused_sweetnet(\n",
    "                        initialization_method = initialization_method,\n",
    "                        num_classes = num_classes,\n",
    "                        embeddings_dict = glm_embeddings, \n",
    "                        trainable_embeddings = trainable_embeddings, \n",
    "                        silent= True\n",
    "                        ) \n",
    "            \n",
    "            \n",
    "            \n",
    "            # Run the training setup function to prepare the model for training\n",
    "            optimizer, scheduler, criterion = model_training.training_setup(model, settings['learning_rate'], num_classes = num_classes)\n",
    "\n",
    "            # Silence all of the epoch-wise prints for the training process\n",
    "            original_stdout = sys.stdout # Save the original stdout\n",
    "            sys.stdout = open(os.devnull, 'w') # Redirect stdout to /dev/null (or equivalent on Windows)\n",
    "            \n",
    "            # Start the timer for the training process\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Run the training process\n",
    "            model_ft, current_run_metrics = model_training.train_model(model, dataloaders, criterion, optimizer, scheduler,\n",
    "                                                                        num_epochs = settings['epochs'], mode = 'multilabel', \n",
    "                                                                        return_metrics = True, patience = settings['patience'])\n",
    "            \n",
    "            # Save the time taken for this run\n",
    "            end_time = time.time() # Record time after training\n",
    "            time_elapsed_seconds = end_time - start_time \n",
    "            \n",
    "\n",
    "            sys.stdout.close() # Close the null device file\n",
    "            sys.stdout = original_stdout # Restore original stdout \n",
    "\n",
    "      \n",
    "            # Collect the epoch-wise metrics from this configuration\n",
    "            config_run_identifier = f\"{config_name}_{i+1}\"\n",
    "            all_run_epoch_histories[config_run_identifier] = current_run_metrics\n",
    "\n",
    "\n",
    "            # Save the best model to a file\n",
    "            os.makedirs(saved_models_dir, exist_ok=True) # make sure the directory exists\n",
    "\n",
    "            model_filename = f\"{config_name}_run_{i+1}_state_dict.pth\"\n",
    "            model_filepath = os.path.join(saved_models_dir, model_filename)\n",
    "            torch.save(model_ft.state_dict(), model_filepath)\n",
    "            #print(f\"Saved model to {model_filepath}\")\n",
    "        \n",
    "            # --- Save the best metrics from this run to the summary results dictionary ---\n",
    "            # I should add a way to turn certain metrics off for some runs, when doing hyperparameter optimization\n",
    "            # generate keys for the summary results\n",
    "            loss_key = f\"{config_name}_loss\"\n",
    "            lrap_key = f\"{config_name}_lrap\"\n",
    "            ndcg_key = f\"{config_name}_ndcg\"\n",
    "            time_key = f\"{config_name}_time\"\n",
    "            metric_keys = [loss_key, lrap_key, ndcg_key,time_key]\n",
    "            \n",
    "            # add keys to the summary results dictionary if they don't exist\n",
    "            for key in metric_keys:\n",
    "                if key not in all_run_summary_results:\n",
    "                    all_run_summary_results[key] = []\n",
    "\n",
    "            # Find the best metrics from the current run\n",
    "            best_loss = min(current_run_metrics['val']['loss'])\n",
    "            best_lrap = max(current_run_metrics['val']['lrap'])\n",
    "            best_ndcg = max(current_run_metrics['val']['ndcg'])\n",
    "            train_time = time_elapsed_seconds\n",
    "            best_metrics = [best_loss, best_lrap, best_ndcg, train_time]\n",
    "\n",
    "            for key, metric in zip(metric_keys, best_metrics):\n",
    "                # Append the best metric to the summary results dictionary\n",
    "                all_run_summary_results[key].append(metric)\n",
    "            \n",
    "        # --- Export metrics at end of each run, in case of early termination ---\n",
    "\n",
    "        # Save the epoch-wise metrics for this run to a pickle file\n",
    "        with open(epoch_data_path, 'wb') as f:\n",
    "            pickle.dump(all_run_epoch_histories, f)\n",
    "        #print(f\"Saved training histories to {epoch_data_path}\")\n",
    "\n",
    "        # Save the summary results to a CSV file\n",
    "        with open(results_summary_path, 'w') as f:\n",
    "            # Write the header\n",
    "            f.write(','.join(all_run_summary_results.keys()) + '\\n')\n",
    "            \n",
    "            # Write the data\n",
    "            any_key = next(iter(all_run_summary_results.keys()))\n",
    "            num_runs_done = len(all_run_summary_results[any_key])\n",
    "            for i in range(num_runs_done):\n",
    "                row = [str(all_run_summary_results[key][i]) for key in all_run_summary_results.keys()]\n",
    "                f.write(','.join(row) + '\\n')\n",
    "        #print(f\"Saved summary results to {results_summary_path}\")\n",
    "\n",
    "print(\"All experiments completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df92aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x: float, # input value\n",
    "            cutoff: float = 700 # cutoff value for sigmoid transformation\n",
    "           ) -> float: # sigmoid transformed value\n",
    "    \"Apply sigmoid transformation to input\"\n",
    "    print(f\"DEBUG: Sigmoid input x type: {type(x)}, dtype: {x.dtype if hasattr(x, 'dtype') else 'N/A'}\")\n",
    "    print(f\"DEBUG: Sigmoid input x min: {np.min(x)}, max: {np.max(x)}\") # If x is an array\n",
    "\n",
    "    if cutoff is not None:\n",
    "        x_clipped = np.clip(x, -cutoff, cutoff)\n",
    "        print(f\"DEBUG: x_clipped type: {type(x_clipped)}, dtype: {x_clipped.dtype if hasattr(x_clipped, 'dtype') else 'N/A'}\")\n",
    "        print(f\"DEBUG: x_clipped min: {np.min(x_clipped)}, max: {np.max(x_clipped)}\") # If x_clipped is an array\n",
    "        print(f\"DEBUG: np.exp(-x_clipped) input value: {-x_clipped}\") # Print the actual value going into exp\n",
    "\n",
    "        # You can temporarily change RuntimeWarning to an error to stop execution right at the source:\n",
    "        # import numpy as np\n",
    "        # np.seterr(all='raise') \n",
    "\n",
    "        return 1 / (1 + np.exp(-x_clipped))\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ee995",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(-11111100000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ad6214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from glycowork.ml.model_training import sigmoid # Assuming this is the correct import path\n",
    "\n",
    "print(inspect.getsource(sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef96d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test_dataloader\n",
    "from glycowork.ml.processing import dataset_to_dataloader\n",
    "test_dataloader = dataset_to_dataloader(glycan_list = test_glycans, labels = test_labels, generaliz_prob = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Developing function to test model here before migrating to utils.py \n",
    "\n",
    "\n",
    "#NOT DONE YET, Need to add stuff to generate metrics\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn \n",
    "from typing import Dict # Import Dict for type hinting\n",
    "from glycowork.ml.model_training import sigmoid\n",
    "from sklearn.metrics import label_ranking_average_precision_score, ndcg_score\n",
    "\n",
    "import torch\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "def test_model(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               criterion: torch.nn.Module) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluates a multi-label model on a test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: torch.nn.Module\n",
    "        The trained model to evaluate.\n",
    "    dataloader: torch.utils.data.DataLoader\n",
    "        DataLoader containing the test split.\n",
    "    criterion: torch.nn.Module\n",
    "        The loss function to calculate average loss during evaluation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing calculated evaluation metrics\n",
    "        for the multi-label task (Loss, LRAP, NDCG).\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        total_loss = 0.\n",
    "        raw_output = []\n",
    "        true_labels = []\n",
    "\n",
    "        for data in dataloader:\n",
    "            # Get all relevant node attributes\n",
    "            x, y, edge_index, batch = data.labels, data.y, data.edge_index, data.batch\n",
    "            prot = getattr(data, 'train_idx', None)\n",
    "            if prot is not None:\n",
    "                prot = prot.view(max(batch) + 1, -1).to(device)\n",
    "            x = x.to(device)            \n",
    "            y = y.view(max(batch) + 1, -1).to(device)\n",
    "            edge_index = edge_index.to(device)\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # --- Forward Pass ---\n",
    "            # Call the model with the extracted data, similar to train_model\n",
    "            if prot is not None:\n",
    "                 pred = model(prot, x, edge_index, batch)\n",
    "            else:\n",
    "                 pred = model(x, edge_index, batch)\n",
    "\n",
    "            # --- Calculate and Accumulate Loss ---\n",
    "            # Use the criterion to calculate loss and accumulate total loss\n",
    "            \n",
    "            loss = criterion(pred, y.float())\n",
    "            # Accumulate loss, weighted by the number of graphs in the batch\n",
    "            # max(batch) + 1 gives the number of graphs in a PyG Batch object\n",
    "            total_loss += loss.item() * (max(batch) + 1)\n",
    "\n",
    "            # --- Collect Outputs and Labels ---\n",
    "            # Append the raw model outputs and true labels to lists\n",
    "            # Keep them as tensors on the device for now\n",
    "            raw_output.append(pred)\n",
    "            true_labels.append(y)\n",
    "\n",
    "    # NOT DONE YET, Need to add stuff to generate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model_ft, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46656f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trial data\n",
    "Dir = \"Datasets\"   # Directory where the datasets are stored\n",
    "pickle_file_path = os.path.join(Dir, \"test_set_kingdom1.pkl\")\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading data from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            user_data_string_from_input = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Data loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(user_data_string_from_input)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweetnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
