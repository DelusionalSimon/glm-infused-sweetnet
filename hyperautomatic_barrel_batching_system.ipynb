{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b80efd",
   "metadata": {},
   "source": [
    "# Hyperautomatic Barrel Batching System\n",
    "\n",
    "This notebook automates the process of running multiple user defined experiments with different datsets and settings sequentially. Each experiment runs a user defined number of trials for different user defined model configurations (baseline, infused-fixed, infused-trainable) to evaluate and compare their performance systematically. It collects validation metrics for each run, aggregates them, and saves the results.\n",
    "\n",
    "If you just want to run one experiment, try the automated_barrel_batching_system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66269a1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "512d390a",
   "metadata": {},
   "source": [
    "### ||RUN ON RESTART||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "719ba33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "\n",
    "from utils import build_multilabel_dataset, multilabel_split, prep_infused_sweetnet, seed_everything\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from glycowork.ml.processing import split_data_to_train, dataset_to_dataloader\n",
    "from glycowork.ml import model_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ab54e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: glm_embeddings_1.pkl\n",
      "Embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "\n",
    "pickle_file_path = 'glm_embeddings_1.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading embeddings from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            glm_embeddings = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Embeddings loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433df162",
   "metadata": {},
   "source": [
    "## Experimental setup\n",
    "Change parameters here to define each Experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fb39f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All random seeds set to: 42\n",
      "\n",
      "A batch of 2 experiments with 4 configurations have been set up\n"
     ]
    }
   ],
   "source": [
    "# --- immutable parameters ---\n",
    "BASE_RANDOM_STATE = 42  # Initial seed for reproducibility of the entire sequence of experiments\n",
    "DATA_DIR = \"Datasets\"   # Directory where the datasets are stored\n",
    "os.makedirs(DATA_DIR, exist_ok=True) # Create the directory if it doesn't exist\n",
    "\n",
    "# Mutable parameters that can be changed for each experiment below\n",
    "settings = {\n",
    "    \"glycan_dataset\":    'df_disease', # The glycowork dataset to use\n",
    "    \"glycan_class\":      'disease_association', # The class to predict from the chosen dataset\n",
    "    \"num_runs\":          5,  # Number of trials per configuration (e.g., 5 or 10)\n",
    "    \"epochs\":            5, # Number of training epochs per run\n",
    "    \"batch_size\":        128, # 32 or 128 seems to work well\n",
    "    \"train_size\":        0.7, # Fraction of data to use for training (0.7 = 70% train, 15% val, 15% test)\n",
    "    \"learning_rate\":     0.005, # Learning rate for the optimizer\n",
    "    \"drop_last\":         False, # Whether to drop the last batch if it's smaller than the batch size\n",
    "    \"augment_prob\":      0.0,  # Adjust if you want augmentation for training\n",
    "    \"generaliz_prob\":    0.2,  # Adjust if you want generalization for training\n",
    "    \"patience\":          25, # number of epochs without improvement before EarlyStop kicks in \n",
    "}\n",
    "# ---- Datasets and num_classes within them ------\n",
    "#  'df_species': 'Species', 'Genus', 'Family', 'Order', 'Class', 'Phylum', 'Kingdom', 'Domain', 'ref'\n",
    "#  'df_tissue': 'tissue_sample', 'tissue_species', 'tissue_id', 'tissue_ref'\n",
    "#  'df_disease': 'disease_association', 'disease_sample', 'disease_direction', 'disease_species', 'disease_id', 'disease_ref'\n",
    "\n",
    "# --- Define Experiments ---\n",
    "# This list defines all sets of parameters you want to change for each experiment, they override the default parameters\n",
    "# Can be used to test different datasets and num_classes\n",
    "# Or for crude hyperparameter tuning\n",
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"Test\",\n",
    "        \"glycan_dataset\": \"df_species\", \n",
    "        \"glycan_class\": \"Kingdom\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"test2\",\n",
    "        \"glycan_dataset\": \"df_tissue\", \n",
    "        \"glycan_class\": \"tissue_sample\"\n",
    "    }\n",
    "    # add more experiments as needed\n",
    "]\n",
    "\n",
    "# --- Define Experiment Configurations for each run of the experiment ---\n",
    "# This list defines all sets of parameters you want to test for all the runs of each experiment\n",
    "experiment_configs = [\n",
    "    {\n",
    "        \"name\": \"baseline_trainable\",\n",
    "        \"initialization_method\": \"random\", \n",
    "        \"trainable_embeddings\": True\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"baseline_fixed\",\n",
    "        \"initialization_method\": \"random\", \n",
    "        \"trainable_embeddings\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"infused_trainable\",\n",
    "        \"initialization_method\": \"external\", \n",
    "        \"trainable_embeddings\": True\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"infused_fixed\",\n",
    "        \"initialization_method\": \"external\", \n",
    "        \"trainable_embeddings\": False\n",
    "    }  \n",
    "    # add more configurations as needed     \n",
    "]\n",
    "\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed_everything(BASE_RANDOM_STATE)\n",
    "\n",
    "# Initialize lists/dicts to store results globally for the notebook session\n",
    "all_run_summary_results = {} # For storing best metrics from each run\n",
    "all_run_epoch_histories = {} # For storing full epoch-wise histories\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "print(f\"A batch of {len(experiments)} experiments with {len(experiment_configs)} configurations have been set up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35662d1f",
   "metadata": {},
   "source": [
    "## Run Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a46f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Test experiment with 4 configurations and 5 runs per configuration.\n",
      "\n",
      "--- Settings for experiment 'Test' saved to experiment_settings_Test.yaml ---\n",
      "All random seeds set to: 42\n",
      "Found 60 unique individual classes/labels.\n",
      "Number of unique glycans left after filtering rare classes (size >= 2): 1557/1648\n",
      "Number of unique labels left after filtering: 39\n",
      "Split complete!\n",
      "Train set size: 1324\n",
      "Validation set size: 233\n",
      "Test set size: 0\n",
      "Saved test set to test_set_Test.pkl\n",
      "----------------\n",
      "Run 1/5\n",
      "\n",
      "All random seeds set to: 42\n",
      "Split complete!\n",
      "Train set size: 1091\n",
      "Validation set size: 233\n",
      "Test set size: 0\n",
      "----------------\n",
      "Running configuration: baseline_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.9591 LRAP: 0.0064 NDCG: 0.3326\n",
      "val Loss: 13.5645 LRAP: 0.0086 NDCG: 0.4868\n",
      "Validation loss decreased (0.000000 --> 13.564498).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.4903 LRAP: 0.1503 NDCG: 0.5445\n",
      "val Loss: 2.3404 LRAP: 0.0215 NDCG: 0.5389\n",
      "Validation loss decreased (13.564498 --> 2.340428).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.2059 LRAP: 0.1687 NDCG: 0.5659\n",
      "val Loss: 2.4026 LRAP: 0.0086 NDCG: 0.4872\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.1165 LRAP: 0.0532 NDCG: 0.5251\n",
      "val Loss: 2.3794 LRAP: 0.0086 NDCG: 0.4588\n",
      "EarlyStopping counter: 2 out of 25\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.0345 LRAP: 0.0898 NDCG: 0.5327\n",
      "val Loss: 2.2239 LRAP: 0.0215 NDCG: 0.5182\n",
      "Validation loss decreased (2.340428 --> 2.223854).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.223854, best LRAP score: 0.7873\n",
      "\n",
      "Saved model to saved_models_Test\\baseline_trainable_run_1_state_dict.pth\n",
      "----------------\n",
      "Running configuration: baseline_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8568 LRAP: 0.0165 NDCG: 0.4040\n",
      "val Loss: 13.7225 LRAP: 0.1545 NDCG: 0.5595\n",
      "Validation loss decreased (0.000000 --> 13.722523).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.3167 LRAP: 0.1430 NDCG: 0.5602\n",
      "val Loss: 5.9738 LRAP: 0.4120 NDCG: 0.6188\n",
      "Validation loss decreased (13.722523 --> 5.973777).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.2165 LRAP: 0.1338 NDCG: 0.5628\n",
      "val Loss: 4.2448 LRAP: 0.0215 NDCG: 0.5357\n",
      "Validation loss decreased (5.973777 --> 4.244763).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.1738 LRAP: 0.0367 NDCG: 0.5113\n",
      "val Loss: 2.3446 LRAP: 0.1159 NDCG: 0.5481\n",
      "Validation loss decreased (4.244763 --> 2.344602).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.1369 LRAP: 0.0962 NDCG: 0.5494\n",
      "val Loss: 2.5447 LRAP: 0.0601 NDCG: 0.5424\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.344602, best LRAP score: 0.7760\n",
      "\n",
      "Saved model to saved_models_Test\\baseline_fixed_run_1_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.7753 LRAP: 0.0156 NDCG: 0.4267\n",
      "val Loss: 75.9138 LRAP: 0.6609 NDCG: 0.6611\n",
      "Validation loss decreased (0.000000 --> 75.913753).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.5109 LRAP: 0.2246 NDCG: 0.5581\n",
      "val Loss: 38.9977 LRAP: 0.6609 NDCG: 0.6611\n",
      "Validation loss decreased (75.913753 --> 38.997691).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.3422 LRAP: 0.0587 NDCG: 0.5202\n",
      "val Loss: 13.5173 LRAP: 0.0043 NDCG: 0.4498\n",
      "Validation loss decreased (38.997691 --> 13.517264).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.2419 LRAP: 0.0055 NDCG: 0.4786\n",
      "val Loss: 3.7969 LRAP: 0.0043 NDCG: 0.4745\n",
      "Validation loss decreased (13.517264 --> 3.796949).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.1693 LRAP: 0.1036 NDCG: 0.5257\n",
      "val Loss: 2.9900 LRAP: 0.4678 NDCG: 0.5846\n",
      "Validation loss decreased (3.796949 --> 2.989996).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.989996, best LRAP score: 0.7643\n",
      "\n",
      "Saved model to saved_models_Test\\infused_trainable_run_1_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8570 LRAP: 0.0467 NDCG: 0.3946\n",
      "val Loss: 52.8286 LRAP: 0.6652 NDCG: 0.6626\n",
      "Validation loss decreased (0.000000 --> 52.828616).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.4187 LRAP: 0.2640 NDCG: 0.5635\n",
      "val Loss: 23.4637 LRAP: 0.0043 NDCG: 0.4703\n",
      "Validation loss decreased (52.828616 --> 23.463663).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.2789 LRAP: 0.0724 NDCG: 0.5322\n",
      "val Loss: 7.8655 LRAP: 0.0472 NDCG: 0.4917\n",
      "Validation loss decreased (23.463663 --> 7.865460).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.2118 LRAP: 0.0229 NDCG: 0.5038\n",
      "val Loss: 4.0499 LRAP: 0.0043 NDCG: 0.4357\n",
      "Validation loss decreased (7.865460 --> 4.049874).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.1469 LRAP: 0.0559 NDCG: 0.5227\n",
      "val Loss: 3.3547 LRAP: 0.0086 NDCG: 0.4462\n",
      "Validation loss decreased (4.049874 --> 3.354724).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 3.354724, best LRAP score: 0.7505\n",
      "\n",
      "Saved model to saved_models_Test\\infused_fixed_run_1_state_dict.pth\n",
      "Saved training histories to epoch_data_Test.pkl\n",
      "Saved summary results to summary_Test.csv\n",
      "----------------\n",
      "Run 2/5\n",
      "\n",
      "All random seeds set to: 43\n",
      "Split complete!\n",
      "Train set size: 1091\n",
      "Validation set size: 233\n",
      "Test set size: 0\n",
      "----------------\n",
      "Running configuration: baseline_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.7696 LRAP: 0.0119 NDCG: 0.4214\n",
      "val Loss: 14.3773 LRAP: 0.0215 NDCG: 0.5267\n",
      "Validation loss decreased (0.000000 --> 14.377310).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.2911 LRAP: 0.2676 NDCG: 0.5873\n",
      "val Loss: 6.3245 LRAP: 0.6309 NDCG: 0.6870\n",
      "Validation loss decreased (14.377310 --> 6.324483).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.1708 LRAP: 0.2291 NDCG: 0.5547\n",
      "val Loss: 2.4199 LRAP: 0.1373 NDCG: 0.5477\n",
      "Validation loss decreased (6.324483 --> 2.419869).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.0299 LRAP: 0.1421 NDCG: 0.5286\n",
      "val Loss: 2.1895 LRAP: 0.0944 NDCG: 0.4950\n",
      "Validation loss decreased (2.419869 --> 2.189527).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 1.9703 LRAP: 0.1952 NDCG: 0.5508\n",
      "val Loss: 2.0584 LRAP: 0.1931 NDCG: 0.5433\n",
      "Validation loss decreased (2.189527 --> 2.058382).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.058382, best LRAP score: 0.7990\n",
      "\n",
      "Saved model to saved_models_Test\\baseline_trainable_run_2_state_dict.pth\n",
      "----------------\n",
      "Running configuration: baseline_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.9110 LRAP: 0.0055 NDCG: 0.3943\n",
      "val Loss: 12.2704 LRAP: 0.0129 NDCG: 0.4654\n",
      "Validation loss decreased (0.000000 --> 12.270408).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.3417 LRAP: 0.0247 NDCG: 0.5154\n",
      "val Loss: 4.0851 LRAP: 0.0129 NDCG: 0.4654\n",
      "Validation loss decreased (12.270408 --> 4.085131).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.2256 LRAP: 0.0211 NDCG: 0.5097\n",
      "val Loss: 2.9121 LRAP: 0.0215 NDCG: 0.4877\n",
      "Validation loss decreased (4.085131 --> 2.912082).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.0827 LRAP: 0.0357 NDCG: 0.5126\n",
      "val Loss: 2.2980 LRAP: 0.0172 NDCG: 0.4905\n",
      "Validation loss decreased (2.912082 --> 2.298046).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 1.9915 LRAP: 0.1457 NDCG: 0.5484\n",
      "val Loss: 2.2445 LRAP: 0.1159 NDCG: 0.5105\n",
      "Validation loss decreased (2.298046 --> 2.244518).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.244518, best LRAP score: 0.7917\n",
      "\n",
      "Saved model to saved_models_Test\\baseline_fixed_run_2_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8167 LRAP: 0.0376 NDCG: 0.4064\n",
      "val Loss: 109.5756 LRAP: 0.6609 NDCG: 0.6585\n",
      "Validation loss decreased (0.000000 --> 109.575613).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.4542 LRAP: 0.2631 NDCG: 0.5764\n",
      "val Loss: 28.3373 LRAP: 0.0086 NDCG: 0.4690\n",
      "Validation loss decreased (109.575613 --> 28.337295).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3931 LRAP: 0.1797 NDCG: 0.5492\n",
      "val Loss: 8.0485 LRAP: 0.0000 NDCG: 0.4687\n",
      "Validation loss decreased (28.337295 --> 8.048489).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.3006 LRAP: 0.0037 NDCG: 0.4938\n",
      "val Loss: 5.4725 LRAP: 0.0000 NDCG: 0.4689\n",
      "Validation loss decreased (8.048489 --> 5.472538).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.2210 LRAP: 0.0156 NDCG: 0.5102\n",
      "val Loss: 2.4789 LRAP: 0.0815 NDCG: 0.5267\n",
      "Validation loss decreased (5.472538 --> 2.478862).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.478862, best LRAP score: 0.7718\n",
      "\n",
      "Saved model to saved_models_Test\\infused_trainable_run_2_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.9697 LRAP: 0.0348 NDCG: 0.3897\n",
      "val Loss: 110.8612 LRAP: 0.6609 NDCG: 0.6585\n",
      "Validation loss decreased (0.000000 --> 110.861168).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.5543 LRAP: 0.2475 NDCG: 0.5711\n",
      "val Loss: 32.9177 LRAP: 0.0086 NDCG: 0.4702\n",
      "Validation loss decreased (110.861168 --> 32.917721).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3071 LRAP: 0.0981 NDCG: 0.5318\n",
      "val Loss: 5.4925 LRAP: 0.0300 NDCG: 0.4749\n",
      "Validation loss decreased (32.917721 --> 5.492537).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.2410 LRAP: 0.0128 NDCG: 0.4878\n",
      "val Loss: 3.2291 LRAP: 0.0343 NDCG: 0.4739\n",
      "Validation loss decreased (5.492537 --> 3.229100).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.1830 LRAP: 0.0412 NDCG: 0.5141\n",
      "val Loss: 3.2528 LRAP: 0.0515 NDCG: 0.4667\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 3.229100, best LRAP score: 0.7438\n",
      "\n",
      "Saved model to saved_models_Test\\infused_fixed_run_2_state_dict.pth\n",
      "Saved training histories to epoch_data_Test.pkl\n",
      "Saved summary results to summary_Test.csv\n",
      "----------------\n",
      "Run 3/5\n",
      "\n",
      "All random seeds set to: 44\n",
      "Split complete!\n",
      "Train set size: 1091\n",
      "Validation set size: 233\n",
      "Test set size: 0\n",
      "----------------\n",
      "Running configuration: baseline_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 4.0050 LRAP: 0.0018 NDCG: 0.2968\n",
      "val Loss: 5.3057 LRAP: 0.0215 NDCG: 0.5402\n",
      "Validation loss decreased (0.000000 --> 5.305684).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.4867 LRAP: 0.0431 NDCG: 0.5316\n",
      "val Loss: 3.1979 LRAP: 0.0300 NDCG: 0.5408\n",
      "Validation loss decreased (5.305684 --> 3.197854).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.1742 LRAP: 0.1210 NDCG: 0.5550\n",
      "val Loss: 2.5085 LRAP: 0.0258 NDCG: 0.4836\n",
      "Validation loss decreased (3.197854 --> 2.508532).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.0783 LRAP: 0.1448 NDCG: 0.5355\n",
      "val Loss: 2.2096 LRAP: 0.0300 NDCG: 0.4611\n",
      "Validation loss decreased (2.508532 --> 2.209622).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 1.9845 LRAP: 0.2227 NDCG: 0.5496\n",
      "val Loss: 2.3690 LRAP: 0.1717 NDCG: 0.4616\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.209622, best LRAP score: 0.7819\n",
      "\n",
      "Saved model to saved_models_Test\\baseline_trainable_run_3_state_dict.pth\n",
      "----------------\n",
      "Running configuration: baseline_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.7712 LRAP: 0.0156 NDCG: 0.4147\n",
      "val Loss: 20.6056 LRAP: 0.0215 NDCG: 0.5402\n",
      "Validation loss decreased (0.000000 --> 20.605643).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.4082 LRAP: 0.0917 NDCG: 0.5405\n",
      "val Loss: 7.7708 LRAP: 0.6352 NDCG: 0.6546\n",
      "Validation loss decreased (20.605643 --> 7.770770).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.2284 LRAP: 0.2126 NDCG: 0.5590\n",
      "val Loss: 4.8587 LRAP: 0.3176 NDCG: 0.5973\n",
      "Validation loss decreased (7.770770 --> 4.858690).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.1272 LRAP: 0.0284 NDCG: 0.5047\n",
      "val Loss: 2.7431 LRAP: 0.1717 NDCG: 0.5482\n",
      "Validation loss decreased (4.858690 --> 2.743062).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.0866 LRAP: 0.1017 NDCG: 0.5403\n",
      "val Loss: 2.2463 LRAP: 0.4292 NDCG: 0.5902\n",
      "Validation loss decreased (2.743062 --> 2.246308).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.246308, best LRAP score: 0.7849\n",
      "\n",
      "Saved model to saved_models_Test\\baseline_fixed_run_3_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.9260 LRAP: 0.0101 NDCG: 0.3953\n",
      "val Loss: 39.0459 LRAP: 0.0000 NDCG: 0.4806\n",
      "Validation loss decreased (0.000000 --> 39.045891).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.5629 LRAP: 0.1448 NDCG: 0.5472\n",
      "val Loss: 23.7993 LRAP: 0.6652 NDCG: 0.6655\n",
      "Validation loss decreased (39.045891 --> 23.799334).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3425 LRAP: 0.2035 NDCG: 0.5574\n",
      "val Loss: 6.7152 LRAP: 0.0000 NDCG: 0.4741\n",
      "Validation loss decreased (23.799334 --> 6.715154).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.2703 LRAP: 0.0064 NDCG: 0.4960\n",
      "val Loss: 2.7524 LRAP: 0.0043 NDCG: 0.4827\n",
      "Validation loss decreased (6.715154 --> 2.752425).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.2141 LRAP: 0.0917 NDCG: 0.5378\n",
      "val Loss: 3.2101 LRAP: 0.0000 NDCG: 0.4451\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.752425, best LRAP score: 0.7577\n",
      "\n",
      "Saved model to saved_models_Test\\infused_trainable_run_3_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8892 LRAP: 0.0055 NDCG: 0.3649\n",
      "val Loss: 52.4042 LRAP: 0.0000 NDCG: 0.4806\n",
      "Validation loss decreased (0.000000 --> 52.404226).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.5527 LRAP: 0.1109 NDCG: 0.5360\n",
      "val Loss: 29.9915 LRAP: 0.6652 NDCG: 0.6655\n",
      "Validation loss decreased (52.404226 --> 29.991530).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3426 LRAP: 0.0907 NDCG: 0.5335\n",
      "val Loss: 9.1634 LRAP: 0.0000 NDCG: 0.4711\n",
      "Validation loss decreased (29.991530 --> 9.163360).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.3182 LRAP: 0.0101 NDCG: 0.5020\n",
      "val Loss: 3.1061 LRAP: 0.0343 NDCG: 0.4834\n",
      "Validation loss decreased (9.163360 --> 3.106084).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.2480 LRAP: 0.0302 NDCG: 0.5128\n",
      "val Loss: 2.5404 LRAP: 0.0944 NDCG: 0.5176\n",
      "Validation loss decreased (3.106084 --> 2.540430).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.540430, best LRAP score: 0.7757\n",
      "\n",
      "Saved model to saved_models_Test\\infused_fixed_run_3_state_dict.pth\n",
      "Saved training histories to epoch_data_Test.pkl\n",
      "Saved summary results to summary_Test.csv\n",
      "----------------\n",
      "Run 4/5\n",
      "\n",
      "All random seeds set to: 45\n",
      "Split complete!\n",
      "Train set size: 1091\n",
      "Validation set size: 233\n",
      "Test set size: 0\n",
      "----------------\n",
      "Running configuration: baseline_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.9639 LRAP: 0.0119 NDCG: 0.3607\n",
      "val Loss: 7.8699 LRAP: 0.6567 NDCG: 0.6625\n",
      "Validation loss decreased (0.000000 --> 7.869889).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.3706 LRAP: 0.2337 NDCG: 0.5743\n",
      "val Loss: 3.1905 LRAP: 0.3863 NDCG: 0.5994\n",
      "Validation loss decreased (7.869889 --> 3.190540).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.1796 LRAP: 0.2466 NDCG: 0.5825\n",
      "val Loss: 2.5068 LRAP: 0.0300 NDCG: 0.4882\n",
      "Validation loss decreased (3.190540 --> 2.506762).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.0752 LRAP: 0.1531 NDCG: 0.5607\n",
      "val Loss: 2.7987 LRAP: 0.0172 NDCG: 0.5127\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 1.9895 LRAP: 0.2090 NDCG: 0.5745\n",
      "val Loss: 2.3201 LRAP: 0.0472 NDCG: 0.4763\n",
      "Validation loss decreased (2.506762 --> 2.320086).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.320086, best LRAP score: 0.7844\n",
      "\n",
      "Saved model to saved_models_Test\\baseline_trainable_run_4_state_dict.pth\n",
      "----------------\n",
      "Running configuration: baseline_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.7746 LRAP: 0.0156 NDCG: 0.3791\n",
      "val Loss: 5.6782 LRAP: 0.1245 NDCG: 0.5676\n",
      "Validation loss decreased (0.000000 --> 5.678153).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.3819 LRAP: 0.0752 NDCG: 0.5371\n",
      "val Loss: 4.1603 LRAP: 0.2489 NDCG: 0.5711\n",
      "Validation loss decreased (5.678153 --> 4.160342).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.1715 LRAP: 0.1091 NDCG: 0.5176\n",
      "val Loss: 2.5916 LRAP: 0.0086 NDCG: 0.4576\n",
      "Validation loss decreased (4.160342 --> 2.591561).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.0684 LRAP: 0.0632 NDCG: 0.5122\n",
      "val Loss: 2.2563 LRAP: 0.1931 NDCG: 0.4998\n",
      "Validation loss decreased (2.591561 --> 2.256283).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.0161 LRAP: 0.2997 NDCG: 0.5593\n",
      "val Loss: 2.6444 LRAP: 0.1717 NDCG: 0.4643\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.256283, best LRAP score: 0.7838\n",
      "\n",
      "Saved model to saved_models_Test\\baseline_fixed_run_4_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.9143 LRAP: 0.0321 NDCG: 0.3995\n",
      "val Loss: 49.8984 LRAP: 0.6652 NDCG: 0.6656\n",
      "Validation loss decreased (0.000000 --> 49.898371).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.4622 LRAP: 0.2484 NDCG: 0.5613\n",
      "val Loss: 31.3160 LRAP: 0.0043 NDCG: 0.4723\n",
      "Validation loss decreased (49.898371 --> 31.315996).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3181 LRAP: 0.0907 NDCG: 0.5239\n",
      "val Loss: 11.6687 LRAP: 0.0000 NDCG: 0.3865\n",
      "Validation loss decreased (31.315996 --> 11.668721).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.1919 LRAP: 0.0073 NDCG: 0.4861\n",
      "val Loss: 4.3108 LRAP: 0.0258 NDCG: 0.4879\n",
      "Validation loss decreased (11.668721 --> 4.310824).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.1333 LRAP: 0.1146 NDCG: 0.5486\n",
      "val Loss: 2.7420 LRAP: 0.1717 NDCG: 0.5539\n",
      "Validation loss decreased (4.310824 --> 2.741963).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.741963, best LRAP score: 0.7775\n",
      "\n",
      "Saved model to saved_models_Test\\infused_trainable_run_4_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.7772 LRAP: 0.0293 NDCG: 0.4097\n",
      "val Loss: 66.7082 LRAP: 0.6609 NDCG: 0.6640\n",
      "Validation loss decreased (0.000000 --> 66.708196).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.4465 LRAP: 0.2062 NDCG: 0.5544\n",
      "val Loss: 23.8241 LRAP: 0.0086 NDCG: 0.4729\n",
      "Validation loss decreased (66.708196 --> 23.824121).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3275 LRAP: 0.0880 NDCG: 0.5239\n",
      "val Loss: 8.5329 LRAP: 0.0000 NDCG: 0.3830\n",
      "Validation loss decreased (23.824121 --> 8.532924).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.2590 LRAP: 0.0082 NDCG: 0.4753\n",
      "val Loss: 2.7344 LRAP: 0.0858 NDCG: 0.4822\n",
      "Validation loss decreased (8.532924 --> 2.734387).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.2080 LRAP: 0.0797 NDCG: 0.5221\n",
      "val Loss: 3.2725 LRAP: 0.0086 NDCG: 0.4689\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.734387, best LRAP score: 0.7595\n",
      "\n",
      "Saved model to saved_models_Test\\infused_fixed_run_4_state_dict.pth\n",
      "Saved training histories to epoch_data_Test.pkl\n",
      "Saved summary results to summary_Test.csv\n",
      "----------------\n",
      "Run 5/5\n",
      "\n",
      "All random seeds set to: 46\n",
      "Split complete!\n",
      "Train set size: 1091\n",
      "Validation set size: 233\n",
      "Test set size: 0\n",
      "----------------\n",
      "Running configuration: baseline_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8579 LRAP: 0.0247 NDCG: 0.4144\n",
      "val Loss: 24.0235 LRAP: 0.0215 NDCG: 0.5365\n",
      "Validation loss decreased (0.000000 --> 24.023510).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.3574 LRAP: 0.0898 NDCG: 0.5139\n",
      "val Loss: 6.3617 LRAP: 0.6652 NDCG: 0.6600\n",
      "Validation loss decreased (24.023510 --> 6.361661).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3164 LRAP: 0.3181 NDCG: 0.5784\n",
      "val Loss: 5.8620 LRAP: 0.6352 NDCG: 0.6509\n",
      "Validation loss decreased (6.361661 --> 5.862045).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.1842 LRAP: 0.1457 NDCG: 0.5285\n",
      "val Loss: 2.4812 LRAP: 0.2961 NDCG: 0.5663\n",
      "Validation loss decreased (5.862045 --> 2.481229).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.0809 LRAP: 0.1146 NDCG: 0.5217\n",
      "val Loss: 2.1363 LRAP: 0.2747 NDCG: 0.5691\n",
      "Validation loss decreased (2.481229 --> 2.136338).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.136338, best LRAP score: 0.7810\n",
      "\n",
      "Saved model to saved_models_Test\\baseline_trainable_run_5_state_dict.pth\n",
      "----------------\n",
      "Running configuration: baseline_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.9276 LRAP: 0.0119 NDCG: 0.3701\n",
      "val Loss: 19.7119 LRAP: 0.0215 NDCG: 0.5365\n",
      "Validation loss decreased (0.000000 --> 19.711860).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.5328 LRAP: 0.0229 NDCG: 0.5320\n",
      "val Loss: 6.4930 LRAP: 0.6009 NDCG: 0.6426\n",
      "Validation loss decreased (19.711860 --> 6.493002).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.2562 LRAP: 0.1677 NDCG: 0.5545\n",
      "val Loss: 3.6968 LRAP: 0.5794 NDCG: 0.5979\n",
      "Validation loss decreased (6.493002 --> 3.696805).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.1632 LRAP: 0.1934 NDCG: 0.5429\n",
      "val Loss: 2.4100 LRAP: 0.4034 NDCG: 0.5486\n",
      "Validation loss decreased (3.696805 --> 2.410040).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.1006 LRAP: 0.2099 NDCG: 0.5340\n",
      "val Loss: 2.2460 LRAP: 0.2318 NDCG: 0.4746\n",
      "Validation loss decreased (2.410040 --> 2.245958).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.245958, best LRAP score: 0.7754\n",
      "\n",
      "Saved model to saved_models_Test\\baseline_fixed_run_5_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.9252 LRAP: 0.0092 NDCG: 0.4203\n",
      "val Loss: 72.3782 LRAP: 0.0000 NDCG: 0.4768\n",
      "Validation loss decreased (0.000000 --> 72.378241).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.5048 LRAP: 0.2145 NDCG: 0.5551\n",
      "val Loss: 31.7191 LRAP: 0.6652 NDCG: 0.6600\n",
      "Validation loss decreased (72.378241 --> 31.719055).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.4217 LRAP: 0.3437 NDCG: 0.5883\n",
      "val Loss: 8.5764 LRAP: 0.0043 NDCG: 0.4684\n",
      "Validation loss decreased (31.719055 --> 8.576428).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.3631 LRAP: 0.1036 NDCG: 0.5298\n",
      "val Loss: 6.6880 LRAP: 0.0000 NDCG: 0.4667\n",
      "Validation loss decreased (8.576428 --> 6.687963).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.2586 LRAP: 0.0651 NDCG: 0.5211\n",
      "val Loss: 2.8065 LRAP: 0.1073 NDCG: 0.5058\n",
      "Validation loss decreased (6.687963 --> 2.806456).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.806456, best LRAP score: 0.7587\n",
      "\n",
      "Saved model to saved_models_Test\\infused_trainable_run_5_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8646 LRAP: 0.0284 NDCG: 0.4153\n",
      "val Loss: 76.2010 LRAP: 0.6652 NDCG: 0.6600\n",
      "Validation loss decreased (0.000000 --> 76.200955).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.5147 LRAP: 0.3006 NDCG: 0.5779\n",
      "val Loss: 17.8310 LRAP: 0.0000 NDCG: 0.4679\n",
      "Validation loss decreased (76.200955 --> 17.830959).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3174 LRAP: 0.1512 NDCG: 0.5501\n",
      "val Loss: 5.4214 LRAP: 0.0043 NDCG: 0.4681\n",
      "Validation loss decreased (17.830959 --> 5.421353).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.2638 LRAP: 0.0073 NDCG: 0.4882\n",
      "val Loss: 3.2184 LRAP: 0.0000 NDCG: 0.4658\n",
      "Validation loss decreased (5.421353 --> 3.218446).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.1999 LRAP: 0.0678 NDCG: 0.5315\n",
      "val Loss: 3.2454 LRAP: 0.0000 NDCG: 0.4613\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 3.218446, best LRAP score: 0.7550\n",
      "\n",
      "Saved model to saved_models_Test\\infused_fixed_run_5_state_dict.pth\n",
      "Saved training histories to epoch_data_Test.pkl\n",
      "Saved summary results to summary_Test.csv\n",
      "\n",
      "Initializing test2 experiment with 4 configurations and 5 runs per configuration.\n",
      "\n",
      "--- Settings for experiment 'test2' saved to experiment_settings_test2.yaml ---\n",
      "All random seeds set to: 42\n",
      "Found 60 unique individual classes/labels.\n",
      "Number of unique glycans left after filtering rare classes (size >= 2): 1557/1648\n",
      "Number of unique labels left after filtering: 39\n",
      "Split complete!\n",
      "Train set size: 1324\n",
      "Validation set size: 233\n",
      "Test set size: 0\n",
      "Saved test set to test_set_test2.pkl\n",
      "----------------\n",
      "Run 1/5\n",
      "\n",
      "All random seeds set to: 42\n",
      "Split complete!\n",
      "Train set size: 1091\n",
      "Validation set size: 233\n",
      "Test set size: 0\n",
      "----------------\n",
      "Running configuration: baseline_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8735 LRAP: 0.0183 NDCG: 0.3252\n",
      "val Loss: 12.2975 LRAP: 0.3305 NDCG: 0.4932\n",
      "Validation loss decreased (0.000000 --> 12.297493).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.4472 LRAP: 0.2145 NDCG: 0.5565\n",
      "val Loss: 2.8810 LRAP: 0.3433 NDCG: 0.5955\n",
      "Validation loss decreased (12.297493 --> 2.881030).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.2006 LRAP: 0.2411 NDCG: 0.5838\n",
      "val Loss: 3.9471 LRAP: 0.0129 NDCG: 0.5129\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.1144 LRAP: 0.0843 NDCG: 0.5517\n",
      "val Loss: 2.2476 LRAP: 0.0129 NDCG: 0.4961\n",
      "Validation loss decreased (2.881030 --> 2.247611).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.0584 LRAP: 0.1833 NDCG: 0.5654\n",
      "val Loss: 2.0963 LRAP: 0.3004 NDCG: 0.5831\n",
      "Validation loss decreased (2.247611 --> 2.096318).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.096318, best LRAP score: 0.7804\n",
      "\n",
      "Saved model to saved_models_test2\\baseline_trainable_run_1_state_dict.pth\n",
      "----------------\n",
      "Running configuration: baseline_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8599 LRAP: 0.0128 NDCG: 0.4039\n",
      "val Loss: 14.6665 LRAP: 0.2275 NDCG: 0.5532\n",
      "Validation loss decreased (0.000000 --> 14.666472).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.3128 LRAP: 0.1439 NDCG: 0.5609\n",
      "val Loss: 6.0628 LRAP: 0.3948 NDCG: 0.6239\n",
      "Validation loss decreased (14.666472 --> 6.062798).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.2226 LRAP: 0.1320 NDCG: 0.5613\n",
      "val Loss: 3.8844 LRAP: 0.0215 NDCG: 0.5382\n",
      "Validation loss decreased (6.062798 --> 3.884353).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.1782 LRAP: 0.0467 NDCG: 0.5174\n",
      "val Loss: 2.3095 LRAP: 0.2747 NDCG: 0.5816\n",
      "Validation loss decreased (3.884353 --> 2.309458).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.1159 LRAP: 0.0889 NDCG: 0.5461\n",
      "val Loss: 2.3318 LRAP: 0.0343 NDCG: 0.5297\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.309458, best LRAP score: 0.7753\n",
      "\n",
      "Saved model to saved_models_test2\\baseline_fixed_run_1_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.7620 LRAP: 0.0128 NDCG: 0.4245\n",
      "val Loss: 79.2738 LRAP: 0.6609 NDCG: 0.6611\n",
      "Validation loss decreased (0.000000 --> 79.273796).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.4969 LRAP: 0.2191 NDCG: 0.5563\n",
      "val Loss: 30.2013 LRAP: 0.6567 NDCG: 0.6596\n",
      "Validation loss decreased (79.273796 --> 30.201325).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3467 LRAP: 0.0669 NDCG: 0.5200\n",
      "val Loss: 15.3793 LRAP: 0.0000 NDCG: 0.4446\n",
      "Validation loss decreased (30.201325 --> 15.379344).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.2642 LRAP: 0.0064 NDCG: 0.4792\n",
      "val Loss: 3.7998 LRAP: 0.0043 NDCG: 0.4782\n",
      "Validation loss decreased (15.379344 --> 3.799827).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.1820 LRAP: 0.1173 NDCG: 0.5357\n",
      "val Loss: 3.8556 LRAP: 0.5794 NDCG: 0.6386\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 3.799827, best LRAP score: 0.7596\n",
      "\n",
      "Saved model to saved_models_test2\\infused_trainable_run_1_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8460 LRAP: 0.0477 NDCG: 0.3946\n",
      "val Loss: 47.5623 LRAP: 0.6652 NDCG: 0.6626\n",
      "Validation loss decreased (0.000000 --> 47.562312).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.3894 LRAP: 0.2887 NDCG: 0.5667\n",
      "val Loss: 20.8046 LRAP: 0.0043 NDCG: 0.4703\n",
      "Validation loss decreased (47.562312 --> 20.804640).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.2612 LRAP: 0.0999 NDCG: 0.5321\n",
      "val Loss: 7.2500 LRAP: 0.1030 NDCG: 0.4887\n",
      "Validation loss decreased (20.804640 --> 7.249987).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.2038 LRAP: 0.0357 NDCG: 0.5013\n",
      "val Loss: 3.7865 LRAP: 0.1931 NDCG: 0.5195\n",
      "Validation loss decreased (7.249987 --> 3.786521).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.1354 LRAP: 0.0962 NDCG: 0.5311\n",
      "val Loss: 3.0418 LRAP: 0.0215 NDCG: 0.4717\n",
      "Validation loss decreased (3.786521 --> 3.041808).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 3.041808, best LRAP score: 0.7550\n",
      "\n",
      "Saved model to saved_models_test2\\infused_fixed_run_1_state_dict.pth\n",
      "Saved training histories to epoch_data_test2.pkl\n",
      "Saved summary results to summary_test2.csv\n",
      "----------------\n",
      "Run 2/5\n",
      "\n",
      "All random seeds set to: 43\n",
      "Split complete!\n",
      "Train set size: 1091\n",
      "Validation set size: 233\n",
      "Test set size: 0\n",
      "----------------\n",
      "Running configuration: baseline_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.7773 LRAP: 0.0137 NDCG: 0.4227\n",
      "val Loss: 18.0305 LRAP: 0.0215 NDCG: 0.5365\n",
      "Validation loss decreased (0.000000 --> 18.030506).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.2974 LRAP: 0.2411 NDCG: 0.5836\n",
      "val Loss: 7.5578 LRAP: 0.5665 NDCG: 0.6709\n",
      "Validation loss decreased (18.030506 --> 7.557788).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.1746 LRAP: 0.1787 NDCG: 0.5455\n",
      "val Loss: 2.5218 LRAP: 0.0601 NDCG: 0.5290\n",
      "Validation loss decreased (7.557788 --> 2.521781).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.0343 LRAP: 0.1430 NDCG: 0.5351\n",
      "val Loss: 2.3171 LRAP: 0.0258 NDCG: 0.4949\n",
      "Validation loss decreased (2.521781 --> 2.317145).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 1.9850 LRAP: 0.1806 NDCG: 0.5517\n",
      "val Loss: 2.4019 LRAP: 0.0343 NDCG: 0.4929\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.317145, best LRAP score: 0.7807\n",
      "\n",
      "Saved model to saved_models_test2\\baseline_trainable_run_2_state_dict.pth\n",
      "----------------\n",
      "Running configuration: baseline_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.9132 LRAP: 0.0073 NDCG: 0.3966\n",
      "val Loss: 12.2601 LRAP: 0.0129 NDCG: 0.4658\n",
      "Validation loss decreased (0.000000 --> 12.260129).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.3343 LRAP: 0.0220 NDCG: 0.5166\n",
      "val Loss: 4.0610 LRAP: 0.0215 NDCG: 0.4700\n",
      "Validation loss decreased (12.260129 --> 4.061028).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.2183 LRAP: 0.0330 NDCG: 0.5119\n",
      "val Loss: 2.5986 LRAP: 0.0215 NDCG: 0.5012\n",
      "Validation loss decreased (4.061028 --> 2.598600).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.0673 LRAP: 0.0871 NDCG: 0.5289\n",
      "val Loss: 2.4867 LRAP: 0.0343 NDCG: 0.5124\n",
      "Validation loss decreased (2.598600 --> 2.486722).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 1.9841 LRAP: 0.1742 NDCG: 0.5564\n",
      "val Loss: 2.2177 LRAP: 0.1502 NDCG: 0.5113\n",
      "Validation loss decreased (2.486722 --> 2.217666).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.217666, best LRAP score: 0.7895\n",
      "\n",
      "Saved model to saved_models_test2\\baseline_fixed_run_2_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8213 LRAP: 0.0284 NDCG: 0.4022\n",
      "val Loss: 97.9898 LRAP: 0.6609 NDCG: 0.6585\n",
      "Validation loss decreased (0.000000 --> 97.989842).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.4508 LRAP: 0.2548 NDCG: 0.5730\n",
      "val Loss: 23.5152 LRAP: 0.0086 NDCG: 0.4702\n",
      "Validation loss decreased (97.989842 --> 23.515198).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3964 LRAP: 0.1558 NDCG: 0.5438\n",
      "val Loss: 7.5670 LRAP: 0.0000 NDCG: 0.4692\n",
      "Validation loss decreased (23.515198 --> 7.566979).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.2708 LRAP: 0.0055 NDCG: 0.4930\n",
      "val Loss: 4.0545 LRAP: 0.0300 NDCG: 0.4806\n",
      "Validation loss decreased (7.566979 --> 4.054530).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.1800 LRAP: 0.0275 NDCG: 0.5185\n",
      "val Loss: 3.6814 LRAP: 0.0644 NDCG: 0.4741\n",
      "Validation loss decreased (4.054530 --> 3.681446).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 3.681446, best LRAP score: 0.7476\n",
      "\n",
      "Saved model to saved_models_test2\\infused_trainable_run_2_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.9363 LRAP: 0.0376 NDCG: 0.3892\n",
      "val Loss: 118.4236 LRAP: 0.6609 NDCG: 0.6585\n",
      "Validation loss decreased (0.000000 --> 118.423588).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.5481 LRAP: 0.2667 NDCG: 0.5706\n",
      "val Loss: 34.8017 LRAP: 0.0086 NDCG: 0.4702\n",
      "Validation loss decreased (118.423588 --> 34.801677).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3127 LRAP: 0.0990 NDCG: 0.5284\n",
      "val Loss: 7.8640 LRAP: 0.0043 NDCG: 0.4540\n",
      "Validation loss decreased (34.801677 --> 7.864037).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.2688 LRAP: 0.0055 NDCG: 0.4830\n",
      "val Loss: 4.0985 LRAP: 0.0043 NDCG: 0.4694\n",
      "Validation loss decreased (7.864037 --> 4.098537).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.1987 LRAP: 0.0275 NDCG: 0.5111\n",
      "val Loss: 3.0241 LRAP: 0.0343 NDCG: 0.4893\n",
      "Validation loss decreased (4.098537 --> 3.024125).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 3.024125, best LRAP score: 0.7546\n",
      "\n",
      "Saved model to saved_models_test2\\infused_fixed_run_2_state_dict.pth\n",
      "Saved training histories to epoch_data_test2.pkl\n",
      "Saved summary results to summary_test2.csv\n",
      "----------------\n",
      "Run 3/5\n",
      "\n",
      "All random seeds set to: 44\n",
      "Split complete!\n",
      "Train set size: 1091\n",
      "Validation set size: 233\n",
      "Test set size: 0\n",
      "----------------\n",
      "Running configuration: baseline_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.9889 LRAP: 0.0018 NDCG: 0.2949\n",
      "val Loss: 5.3293 LRAP: 0.0215 NDCG: 0.5402\n",
      "Validation loss decreased (0.000000 --> 5.329313).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.3996 LRAP: 0.0513 NDCG: 0.5383\n",
      "val Loss: 3.0377 LRAP: 0.0172 NDCG: 0.4894\n",
      "Validation loss decreased (5.329313 --> 3.037657).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.1575 LRAP: 0.0917 NDCG: 0.5390\n",
      "val Loss: 2.4379 LRAP: 0.0215 NDCG: 0.4577\n",
      "Validation loss decreased (3.037657 --> 2.437912).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.0676 LRAP: 0.1164 NDCG: 0.5253\n",
      "val Loss: 2.2913 LRAP: 0.0386 NDCG: 0.4632\n",
      "Validation loss decreased (2.437912 --> 2.291308).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 1.9800 LRAP: 0.2246 NDCG: 0.5478\n",
      "val Loss: 2.2418 LRAP: 0.1588 NDCG: 0.4654\n",
      "Validation loss decreased (2.291308 --> 2.241774).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.241774, best LRAP score: 0.7856\n",
      "\n",
      "Saved model to saved_models_test2\\baseline_trainable_run_3_state_dict.pth\n",
      "----------------\n",
      "Running configuration: baseline_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.7655 LRAP: 0.0147 NDCG: 0.4133\n",
      "val Loss: 20.0890 LRAP: 0.0215 NDCG: 0.5402\n",
      "Validation loss decreased (0.000000 --> 20.089037).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.4001 LRAP: 0.0917 NDCG: 0.5387\n",
      "val Loss: 8.6897 LRAP: 0.6481 NDCG: 0.6590\n",
      "Validation loss decreased (20.089037 --> 8.689700).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.2375 LRAP: 0.2044 NDCG: 0.5533\n",
      "val Loss: 4.6102 LRAP: 0.3176 NDCG: 0.5992\n",
      "Validation loss decreased (8.689700 --> 4.610175).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.1276 LRAP: 0.0367 NDCG: 0.5072\n",
      "val Loss: 2.7280 LRAP: 0.2661 NDCG: 0.5541\n",
      "Validation loss decreased (4.610175 --> 2.727965).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.0818 LRAP: 0.1192 NDCG: 0.5424\n",
      "val Loss: 2.3007 LRAP: 0.4120 NDCG: 0.5915\n",
      "Validation loss decreased (2.727965 --> 2.300742).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.300742, best LRAP score: 0.7815\n",
      "\n",
      "Saved model to saved_models_test2\\baseline_fixed_run_3_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8838 LRAP: 0.0082 NDCG: 0.3935\n",
      "val Loss: 57.8005 LRAP: 0.0043 NDCG: 0.4818\n",
      "Validation loss decreased (0.000000 --> 57.800502).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.5458 LRAP: 0.1980 NDCG: 0.5602\n",
      "val Loss: 20.3753 LRAP: 0.6652 NDCG: 0.6655\n",
      "Validation loss decreased (57.800502 --> 20.375295).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3352 LRAP: 0.1797 NDCG: 0.5521\n",
      "val Loss: 7.1828 LRAP: 0.0000 NDCG: 0.4751\n",
      "Validation loss decreased (20.375295 --> 7.182843).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.2090 LRAP: 0.0128 NDCG: 0.5124\n",
      "val Loss: 2.7282 LRAP: 0.0043 NDCG: 0.4752\n",
      "Validation loss decreased (7.182843 --> 2.728228).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.1296 LRAP: 0.0999 NDCG: 0.5395\n",
      "val Loss: 3.5080 LRAP: 0.0558 NDCG: 0.5079\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.728228, best LRAP score: 0.7610\n",
      "\n",
      "Saved model to saved_models_test2\\infused_trainable_run_3_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8926 LRAP: 0.0037 NDCG: 0.3643\n",
      "val Loss: 50.3741 LRAP: 0.0000 NDCG: 0.4806\n",
      "Validation loss decreased (0.000000 --> 50.374082).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.5691 LRAP: 0.0871 NDCG: 0.5323\n",
      "val Loss: 27.9044 LRAP: 0.6652 NDCG: 0.6655\n",
      "Validation loss decreased (50.374082 --> 27.904376).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3317 LRAP: 0.0843 NDCG: 0.5313\n",
      "val Loss: 7.3067 LRAP: 0.1030 NDCG: 0.4938\n",
      "Validation loss decreased (27.904376 --> 7.306685).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.2842 LRAP: 0.0073 NDCG: 0.4938\n",
      "val Loss: 3.2134 LRAP: 0.0172 NDCG: 0.4853\n",
      "Validation loss decreased (7.306685 --> 3.213384).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.2152 LRAP: 0.0907 NDCG: 0.5225\n",
      "val Loss: 3.0485 LRAP: 0.1631 NDCG: 0.4942\n",
      "Validation loss decreased (3.213384 --> 3.048466).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 3.048466, best LRAP score: 0.7819\n",
      "\n",
      "Saved model to saved_models_test2\\infused_fixed_run_3_state_dict.pth\n",
      "Saved training histories to epoch_data_test2.pkl\n",
      "Saved summary results to summary_test2.csv\n",
      "----------------\n",
      "Run 4/5\n",
      "\n",
      "All random seeds set to: 45\n",
      "Split complete!\n",
      "Train set size: 1091\n",
      "Validation set size: 233\n",
      "Test set size: 0\n",
      "----------------\n",
      "Running configuration: baseline_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.9240 LRAP: 0.0128 NDCG: 0.3465\n",
      "val Loss: 10.5203 LRAP: 0.4549 NDCG: 0.6331\n",
      "Validation loss decreased (0.000000 --> 10.520276).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.3725 LRAP: 0.2191 NDCG: 0.5678\n",
      "val Loss: 4.9816 LRAP: 0.3734 NDCG: 0.6131\n",
      "Validation loss decreased (10.520276 --> 4.981597).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.1913 LRAP: 0.2356 NDCG: 0.5804\n",
      "val Loss: 2.6274 LRAP: 0.1159 NDCG: 0.5218\n",
      "Validation loss decreased (4.981597 --> 2.627378).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.1006 LRAP: 0.0962 NDCG: 0.5399\n",
      "val Loss: 3.1203 LRAP: 0.0258 NDCG: 0.4869\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.0448 LRAP: 0.1897 NDCG: 0.5621\n",
      "val Loss: 2.2057 LRAP: 0.0386 NDCG: 0.5128\n",
      "Validation loss decreased (2.627378 --> 2.205716).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.205716, best LRAP score: 0.7759\n",
      "\n",
      "Saved model to saved_models_test2\\baseline_trainable_run_4_state_dict.pth\n",
      "----------------\n",
      "Running configuration: baseline_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8082 LRAP: 0.0128 NDCG: 0.3773\n",
      "val Loss: 4.7234 LRAP: 0.0215 NDCG: 0.5432\n",
      "Validation loss decreased (0.000000 --> 4.723351).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.3729 LRAP: 0.0990 NDCG: 0.5453\n",
      "val Loss: 2.7658 LRAP: 0.0172 NDCG: 0.4818\n",
      "Validation loss decreased (4.723351 --> 2.765780).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.1592 LRAP: 0.1118 NDCG: 0.5258\n",
      "val Loss: 2.7571 LRAP: 0.0172 NDCG: 0.4705\n",
      "Validation loss decreased (2.765780 --> 2.757143).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.0756 LRAP: 0.0770 NDCG: 0.5134\n",
      "val Loss: 2.3794 LRAP: 0.0687 NDCG: 0.4635\n",
      "Validation loss decreased (2.757143 --> 2.379440).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.0094 LRAP: 0.3245 NDCG: 0.5716\n",
      "val Loss: 2.4617 LRAP: 0.2961 NDCG: 0.4914\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.379440, best LRAP score: 0.7820\n",
      "\n",
      "Saved model to saved_models_test2\\baseline_fixed_run_4_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8958 LRAP: 0.0330 NDCG: 0.3977\n",
      "val Loss: 56.2677 LRAP: 0.6652 NDCG: 0.6656\n",
      "Validation loss decreased (0.000000 --> 56.267743).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.4561 LRAP: 0.2411 NDCG: 0.5655\n",
      "val Loss: 35.0035 LRAP: 0.0043 NDCG: 0.4723\n",
      "Validation loss decreased (56.267743 --> 35.003481).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3001 LRAP: 0.1036 NDCG: 0.5177\n",
      "val Loss: 11.2205 LRAP: 0.0000 NDCG: 0.3872\n",
      "Validation loss decreased (35.003481 --> 11.220483).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.1872 LRAP: 0.0202 NDCG: 0.4942\n",
      "val Loss: 3.6445 LRAP: 0.1674 NDCG: 0.5213\n",
      "Validation loss decreased (11.220483 --> 3.644531).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.1185 LRAP: 0.1045 NDCG: 0.5480\n",
      "val Loss: 2.3892 LRAP: 0.3133 NDCG: 0.5990\n",
      "Validation loss decreased (3.644531 --> 2.389170).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.389170, best LRAP score: 0.7888\n",
      "\n",
      "Saved model to saved_models_test2\\infused_trainable_run_4_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.7864 LRAP: 0.0192 NDCG: 0.4104\n",
      "val Loss: 76.4801 LRAP: 0.6609 NDCG: 0.6640\n",
      "Validation loss decreased (0.000000 --> 76.480144).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.4499 LRAP: 0.2172 NDCG: 0.5655\n",
      "val Loss: 20.7950 LRAP: 0.2060 NDCG: 0.5113\n",
      "Validation loss decreased (76.480144 --> 20.794958).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3223 LRAP: 0.1155 NDCG: 0.5389\n",
      "val Loss: 4.9501 LRAP: 0.0129 NDCG: 0.4160\n",
      "Validation loss decreased (20.794958 --> 4.950118).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.2408 LRAP: 0.0037 NDCG: 0.4702\n",
      "val Loss: 3.0639 LRAP: 0.0086 NDCG: 0.4683\n",
      "Validation loss decreased (4.950118 --> 3.063895).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.1950 LRAP: 0.0605 NDCG: 0.5161\n",
      "val Loss: 3.1050 LRAP: 0.1202 NDCG: 0.4843\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 3.063895, best LRAP score: 0.7522\n",
      "\n",
      "Saved model to saved_models_test2\\infused_fixed_run_4_state_dict.pth\n",
      "Saved training histories to epoch_data_test2.pkl\n",
      "Saved summary results to summary_test2.csv\n",
      "----------------\n",
      "Run 5/5\n",
      "\n",
      "All random seeds set to: 46\n",
      "Split complete!\n",
      "Train set size: 1091\n",
      "Validation set size: 233\n",
      "Test set size: 0\n",
      "----------------\n",
      "Running configuration: baseline_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8510 LRAP: 0.0266 NDCG: 0.4133\n",
      "val Loss: 23.4384 LRAP: 0.0215 NDCG: 0.5365\n",
      "Validation loss decreased (0.000000 --> 23.438353).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.3500 LRAP: 0.1109 NDCG: 0.5260\n",
      "val Loss: 6.0361 LRAP: 0.6652 NDCG: 0.6600\n",
      "Validation loss decreased (23.438353 --> 6.036085).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.2953 LRAP: 0.3309 NDCG: 0.5823\n",
      "val Loss: 5.5464 LRAP: 0.6309 NDCG: 0.6522\n",
      "Validation loss decreased (6.036085 --> 5.546399).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.1571 LRAP: 0.0907 NDCG: 0.5182\n",
      "val Loss: 2.3402 LRAP: 0.3562 NDCG: 0.5773\n",
      "Validation loss decreased (5.546399 --> 2.340219).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.0664 LRAP: 0.1063 NDCG: 0.5179\n",
      "val Loss: 2.2100 LRAP: 0.3004 NDCG: 0.5729\n",
      "Validation loss decreased (2.340219 --> 2.209952).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.209952, best LRAP score: 0.7810\n",
      "\n",
      "Saved model to saved_models_test2\\baseline_trainable_run_5_state_dict.pth\n",
      "----------------\n",
      "Running configuration: baseline_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'random' initialization method (training from scratch).\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.9286 LRAP: 0.0128 NDCG: 0.3733\n",
      "val Loss: 20.4216 LRAP: 0.0215 NDCG: 0.5365\n",
      "Validation loss decreased (0.000000 --> 20.421551).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.5855 LRAP: 0.0211 NDCG: 0.5327\n",
      "val Loss: 6.6580 LRAP: 0.3133 NDCG: 0.5894\n",
      "Validation loss decreased (20.421551 --> 6.657964).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.2388 LRAP: 0.0990 NDCG: 0.5416\n",
      "val Loss: 3.7476 LRAP: 0.6180 NDCG: 0.6309\n",
      "Validation loss decreased (6.657964 --> 3.747575).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.1394 LRAP: 0.1641 NDCG: 0.5409\n",
      "val Loss: 2.3836 LRAP: 0.4292 NDCG: 0.5703\n",
      "Validation loss decreased (3.747575 --> 2.383648).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.0870 LRAP: 0.2044 NDCG: 0.5282\n",
      "val Loss: 2.2524 LRAP: 0.2575 NDCG: 0.5230\n",
      "Validation loss decreased (2.383648 --> 2.252356).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.252356, best LRAP score: 0.7780\n",
      "\n",
      "Saved model to saved_models_test2\\baseline_fixed_run_5_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_trainable\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: True.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.9294 LRAP: 0.0119 NDCG: 0.4226\n",
      "val Loss: 49.6451 LRAP: 0.0000 NDCG: 0.4768\n",
      "Validation loss decreased (0.000000 --> 49.645061).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.4689 LRAP: 0.1677 NDCG: 0.5460\n",
      "val Loss: 13.8621 LRAP: 0.6652 NDCG: 0.6600\n",
      "Validation loss decreased (49.645061 --> 13.862119).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3861 LRAP: 0.1632 NDCG: 0.5563\n",
      "val Loss: 14.8964 LRAP: 0.0000 NDCG: 0.4679\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.2867 LRAP: 0.0110 NDCG: 0.4986\n",
      "val Loss: 4.2516 LRAP: 0.0000 NDCG: 0.3850\n",
      "Validation loss decreased (13.862119 --> 4.251627).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.2129 LRAP: 0.0082 NDCG: 0.4941\n",
      "val Loss: 2.5739 LRAP: 0.0215 NDCG: 0.5154\n",
      "Validation loss decreased (4.251627 --> 2.573885).  Saving model ...\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.573885, best LRAP score: 0.7763\n",
      "\n",
      "Saved model to saved_models_test2\\infused_trainable_run_5_state_dict.pth\n",
      "----------------\n",
      "Running configuration: infused_fixed\n",
      "\n",
      "SweetNet model instantiated with lib_size=2565, num_classes=39, hidden_dim=320.\n",
      "Handling 'external' initialization method.\n",
      "SweetNet item_embedding layer set to trainable: False.\n",
      "\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.8639 LRAP: 0.0202 NDCG: 0.4148\n",
      "val Loss: 86.1275 LRAP: 0.6652 NDCG: 0.6600\n",
      "Validation loss decreased (0.000000 --> 86.127496).  Saving model ...\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "D:\\(███ ██)\\Barrel-aged SweetNet\\glycowork-glm-infused-sweetnet\\glycowork\\ml\\model_training.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.4944 LRAP: 0.2860 NDCG: 0.5759\n",
      "val Loss: 23.4407 LRAP: 0.0000 NDCG: 0.4679\n",
      "Validation loss decreased (86.127496 --> 23.440684).  Saving model ...\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3395 LRAP: 0.1622 NDCG: 0.5501\n",
      "val Loss: 6.7802 LRAP: 0.0000 NDCG: 0.4675\n",
      "Validation loss decreased (23.440684 --> 6.780198).  Saving model ...\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.2679 LRAP: 0.0101 NDCG: 0.4990\n",
      "val Loss: 2.9383 LRAP: 0.0000 NDCG: 0.4675\n",
      "Validation loss decreased (6.780198 --> 2.938345).  Saving model ...\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.2215 LRAP: 0.0330 NDCG: 0.5220\n",
      "val Loss: 3.2248 LRAP: 0.0043 NDCG: 0.4687\n",
      "EarlyStopping counter: 1 out of 25\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val loss: 2.938345, best LRAP score: 0.7566\n",
      "\n",
      "Saved model to saved_models_test2\\infused_fixed_run_5_state_dict.pth\n",
      "Saved training histories to epoch_data_test2.pkl\n",
      "Saved summary results to summary_test2.csv\n",
      "All experiments completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperautomation loop ---\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each experiment\n",
    "for experiment in experiments:\n",
    "    \n",
    "    # load the settings for this experiment\n",
    "    for setting in settings:\n",
    "        if setting in experiment:\n",
    "            settings[setting] = experiment[setting]\n",
    "       \n",
    "    # get name of the current experiment\n",
    "    experiment_name = experiment[\"name\"]\n",
    "    num_runs = settings['num_runs']\n",
    "\n",
    "    print()\n",
    "    print(f\"Initializing {experiment_name} experiment with {len(experiment_configs)} configurations and {num_runs} runs per configuration.\")\n",
    "    print()\n",
    "\n",
    "    # generate the paths for saving results\n",
    "    results_summary_path =      os.path.join(DATA_DIR,f\"summary_{experiment_name}.csv\")\n",
    "    epoch_data_path =           os.path.join(DATA_DIR,f\"epoch_data_{experiment_name}.pkl\")\n",
    "    test_set_path =             os.path.join(DATA_DIR,f\"test_set_{experiment_name}.pkl\")\n",
    "    saved_models_dir =          os.path.join(DATA_DIR,f\"saved_models_{experiment_name}\")\n",
    "    experiment_settings_path =  os.path.join(DATA_DIR,f\"experiment_settings_{experiment_name}.yaml\")\n",
    "\n",
    "    # generate yaml file to save the settings for this experiment\n",
    "    experiment_log_data = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"global_parameters_for_this_experiment\": settings, # This holds updated settings\n",
    "        \"experiment_specific_configs\": experiment, # The specific entry from the 'experiments' list\n",
    "        \"all_model_configurations_tested\": experiment_configs, # All configs tested within this experiment\n",
    "        \"base_random_state_for_experiment_batch\": BASE_RANDOM_STATE # Log the base seed\n",
    "    }\n",
    "    try:\n",
    "        with open(experiment_settings_path, 'w') as f:\n",
    "            yaml.dump(experiment_log_data, f, default_flow_style=False, sort_keys=False) # sort_keys=False to preserve order\n",
    "\n",
    "        print(f\"--- Settings for experiment '{experiment_name}' saved to {experiment_settings_path} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving settings for experiment '{experiment_name}' to YAML: {e}\")\n",
    "\n",
    "    # Set the initial random seed for this experiment\n",
    "    seed_everything(BASE_RANDOM_STATE)\n",
    "\n",
    "    # Load dataset for the experiment\n",
    "    glycans, labels, label_names = build_multilabel_dataset(glycan_dataset = 'df_disease', \n",
    "                                                        glycan_class = 'disease_association', \n",
    "                                                        min_class_size = 2)\n",
    "\n",
    "    \n",
    "    num_classes = len(labels[0]) # Number of classes in the dataset\n",
    "\n",
    "    # calculate the split ratio for train/val/test\n",
    "    test_split_ratio = 1 - ((1 - settings['train_size'])/2)\n",
    "    val_split_ratio = 1- ((1 - settings['train_size']) / (1 + settings['train_size']))\n",
    "\n",
    "    # split out the test set outside of the loop to stop data leakage\n",
    "    temp_glycans, test_glycans, _, temp_labels, test_labels, _ = multilabel_split(glycans, labels, train_size = test_split_ratio, \n",
    "                                                                random_state = BASE_RANDOM_STATE, no_test = True)\n",
    "\n",
    "    # load the test set into a dataloader\n",
    "    test_dataloader = dataset_to_dataloader(glycan_list = test_glycans, \n",
    "                                            labels = test_labels,\n",
    "                                            batch_size = settings['batch_size'],\n",
    "                                            drop_last = settings['drop_last'],\n",
    "                                            augment_prob = settings['augment_prob'], \n",
    "                                            generalization_prob = settings['generaliz_prob']\n",
    "    )\n",
    "    # Save the test set to a pickle file\n",
    "    with open(test_set_path, 'wb') as f:\n",
    "        pickle.dump(test_dataloader, f) \n",
    "    print(f\"Saved test set to {test_set_path}\")\n",
    "\n",
    "    for i in range(num_runs):\n",
    "\n",
    "        # Print the current run number\n",
    "        print(\"----------------\")\n",
    "        print(f\"Run {i+1}/{num_runs}\")\n",
    "        print()\n",
    "\n",
    "        # Increment the random state for each run\n",
    "        random_state = BASE_RANDOM_STATE + i\n",
    "\n",
    "        # Set the random seed for reproducibility for this run\n",
    "        seed_everything(random_state)    \n",
    "\n",
    "        # --- split data for this run outside of core loop for efficiency ---\n",
    "        # Split the remainding dataset into training and validation\n",
    "        train_glycans, val_glycans, _, train_labels, val_labels, _ = multilabel_split(temp_glycans, temp_labels, train_size=val_split_ratio, \n",
    "                                                                    random_state=random_state, no_test = True)\n",
    "\n",
    "        # Load into dataloders for training and validation\n",
    "        dataloaders = split_data_to_train(\n",
    "            glycan_list_train = train_glycans, glycan_list_val = val_glycans, labels_train = train_labels, labels_val = val_labels,\n",
    "            batch_size = settings['batch_size'],\n",
    "            drop_last = settings['drop_last'],\n",
    "            augment_prob = settings['augment_prob'], \n",
    "            generalization_prob = settings['generaliz_prob']\n",
    "        )\n",
    "        \n",
    "        # --- Loop through each configuration & train model ---\n",
    "        for config in experiment_configs:\n",
    "\n",
    "            # Extract the configuration parameters\n",
    "            config_name = config[\"name\"]\n",
    "            initialization_method = config[\"initialization_method\"]\n",
    "            trainable_embeddings = config[\"trainable_embeddings\"]\n",
    "\n",
    "            # Print the current configuration being used\n",
    "            print(\"----------------\")\n",
    "            print(f\"Running configuration: {config_name}\")\n",
    "            print()\n",
    "\n",
    "            # --- Model Training ---\n",
    "            # Initialize the model with the specified parameters\n",
    "            model =  prep_infused_sweetnet(\n",
    "                        initialization_method = initialization_method,\n",
    "                        num_classes = num_classes,\n",
    "                        embeddings_dict = glm_embeddings, \n",
    "                        trainable_embeddings = trainable_embeddings\n",
    "                        ) \n",
    "            \n",
    "            print()\n",
    "            \n",
    "            # Run the training setup function to prepare the model for training\n",
    "            optimizer, scheduler, criterion = model_training.training_setup(model, settings['learning_rate'], num_classes = num_classes)\n",
    "\n",
    "            # Run the training process\n",
    "            model_ft, current_run_metrics = model_training.train_model(model, dataloaders, criterion, optimizer, scheduler,\n",
    "                            num_epochs = settings['epochs'], mode = 'multilabel', return_metrics = True, patience = settings['patience'])\n",
    "            \n",
    "            print()\n",
    "            \n",
    "            # Collect the epoch-wise metrics from this configuration\n",
    "            config_run_identifier = f\"{config_name}_{i+1}\"\n",
    "            all_run_epoch_histories[config_run_identifier] = current_run_metrics\n",
    "\n",
    "            # Save the best model to a file\n",
    "            os.makedirs(saved_models_dir, exist_ok=True) # make sure the directory exists\n",
    "\n",
    "            model_filename = f\"{config_name}_run_{i+1}_state_dict.pth\"\n",
    "            model_filepath = os.path.join(saved_models_dir, model_filename)\n",
    "            torch.save(model_ft.state_dict(), model_filepath)\n",
    "            print(f\"Saved model to {model_filepath}\")\n",
    "        \n",
    "            # --- Save the best metrics from this run to the summary results dictionary ---\n",
    "            # I should add a way to turn certain metrics off for some runs, when doing hyperparameter optimization\n",
    "            # generate keys for the summary results\n",
    "            loss_key = f\"{config_name}_loss\"\n",
    "            lrap_key = f\"{config_name}_lrap\"\n",
    "            ndcg_key = f\"{config_name}_ndcg\"\n",
    "            metric_keys = [loss_key, lrap_key, ndcg_key]\n",
    "            \n",
    "            # add keys to the summary results dictionary if they don't exist\n",
    "            for key in metric_keys:\n",
    "                if key not in all_run_summary_results:\n",
    "                    all_run_summary_results[key] = []\n",
    "\n",
    "            # Find the best metrics from the current run\n",
    "            best_loss = min(current_run_metrics['val']['loss'])\n",
    "            best_lrap = max(current_run_metrics['val']['lrap'])\n",
    "            best_ndcg = max(current_run_metrics['val']['ndcg'])\n",
    "            best_metrics = [best_loss, best_lrap, best_ndcg]\n",
    "\n",
    "            for key, metric in zip(metric_keys, best_metrics):\n",
    "                # Append the best metric to the summary results dictionary\n",
    "                all_run_summary_results[key].append(metric)\n",
    "            \n",
    "        # --- Export metrics at end of each run, in case of early termination ---\n",
    "\n",
    "        # Save the epoch-wise metrics for this run to a pickle file\n",
    "        with open(epoch_data_path, 'wb') as f:\n",
    "            pickle.dump(all_run_epoch_histories, f)\n",
    "        print(f\"Saved training histories to {epoch_data_path}\")\n",
    "\n",
    "        # Save the summary results to a CSV file\n",
    "        with open(results_summary_path, 'w') as f:\n",
    "            # Write the header\n",
    "            f.write(','.join(all_run_summary_results.keys()) + '\\n')\n",
    "            \n",
    "            # Write the data\n",
    "            any_key = next(iter(all_run_summary_results.keys()))\n",
    "            num_runs_done = len(all_run_summary_results[any_key])\n",
    "            for i in range(num_runs_done):\n",
    "                row = [str(all_run_summary_results[key][i]) for key in all_run_summary_results.keys()]\n",
    "                f.write(','.join(row) + '\\n')\n",
    "        print(f\"Saved summary results to {results_summary_path}\")\n",
    "\n",
    "print(\"All experiments completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef96d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test_dataloader\n",
    "from glycowork.ml.processing import dataset_to_dataloader\n",
    "test_dataloader = dataset_to_dataloader(glycan_list = test_glycans, labels = test_labels, generaliz_prob = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Developing function to test model here before migrating to utils.py \n",
    "\n",
    "\n",
    "#NOT DONE YET, Need to add stuff to generate metrics\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn \n",
    "from typing import Dict # Import Dict for type hinting\n",
    "from glycowork.ml.model_training import sigmoid\n",
    "from sklearn.metrics import label_ranking_average_precision_score, ndcg_score\n",
    "\n",
    "import torch\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "def test_model(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               criterion: torch.nn.Module) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluates a multi-label model on a test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: torch.nn.Module\n",
    "        The trained model to evaluate.\n",
    "    dataloader: torch.utils.data.DataLoader\n",
    "        DataLoader containing the test split.\n",
    "    criterion: torch.nn.Module\n",
    "        The loss function to calculate average loss during evaluation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing calculated evaluation metrics\n",
    "        for the multi-label task (Loss, LRAP, NDCG).\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        total_loss = 0.\n",
    "        raw_output = []\n",
    "        true_labels = []\n",
    "\n",
    "        for data in dataloader:\n",
    "            # Get all relevant node attributes\n",
    "            x, y, edge_index, batch = data.labels, data.y, data.edge_index, data.batch\n",
    "            prot = getattr(data, 'train_idx', None)\n",
    "            if prot is not None:\n",
    "                prot = prot.view(max(batch) + 1, -1).to(device)\n",
    "            x = x.to(device)            \n",
    "            y = y.view(max(batch) + 1, -1).to(device)\n",
    "            edge_index = edge_index.to(device)\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # --- Forward Pass ---\n",
    "            # Call the model with the extracted data, similar to train_model\n",
    "            if prot is not None:\n",
    "                 pred = model(prot, x, edge_index, batch)\n",
    "            else:\n",
    "                 pred = model(x, edge_index, batch)\n",
    "\n",
    "            # --- Calculate and Accumulate Loss ---\n",
    "            # Use the criterion to calculate loss and accumulate total loss\n",
    "            \n",
    "            loss = criterion(pred, y.float())\n",
    "            # Accumulate loss, weighted by the number of graphs in the batch\n",
    "            # max(batch) + 1 gives the number of graphs in a PyG Batch object\n",
    "            total_loss += loss.item() * (max(batch) + 1)\n",
    "\n",
    "            # --- Collect Outputs and Labels ---\n",
    "            # Append the raw model outputs and true labels to lists\n",
    "            # Keep them as tensors on the device for now\n",
    "            raw_output.append(pred)\n",
    "            true_labels.append(y)\n",
    "\n",
    "    # NOT DONE YET, Need to add stuff to generate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model_ft, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46656f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trial data\n",
    "\n",
    "pickle_file_path = 'epoch_data_kingdom.pkl'\n",
    "\n",
    "# --- Load the Pickle File ---\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Loading data from: {pickle_file_path}\")\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(pickle_file_path, 'rb') as file_handle:\n",
    "            # Load the object(s) from the pickle file\n",
    "            user_data_string_from_input = pickle.load(file_handle)\n",
    "\n",
    "        print(\"Data loaded successfully!\")        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at '{pickle_file_path}'. Please check the filename and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_data_string_from_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweetnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
